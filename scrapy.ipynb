{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "view-in-github"
   },
   "source": [
    "<a href=\"https://colab.research.google.com/github/zanasaed/NoteBook/blob/main/selenium.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "EpTwQSN34Tpg"
   },
   "source": [
    "# Scrapy \n",
    "Udemy - Modern Web Scraping with Python using Scrapy Splash Selenium 2020-5 \n",
    "\n",
    "this document it is not a tutorail to learn scarapy it is just a note book of importnet thighs that I think it's useful \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "w68acUElpBFf"
   },
   "source": [
    "## Install \n",
    "\n",
    "\n",
    "1.   First install anaconda  and python on anaconda \n",
    "\n",
    "2.   create an Environment  \n",
    "\n",
    "3.   install scrapy on anaconda [links](https://anaconda.org/conda-forge/scrapy)\n",
    "\n",
    "for coding use VSC \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Xz2B7MlEqaEl"
   },
   "source": [
    "## DOC\n",
    "the Official [documentation](https://docs.scrapy.org/en/latest/)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ozmQsU0Yr9Gf"
   },
   "source": [
    "## Use terminal \n",
    "go the official website for more [details](https://docs.scrapy.org/en/latest/topics/commands.html) \n",
    "\n",
    "open anaconda  terminal and active the Env that contains scrapy \n",
    "\n",
    "type `scrapy`  to see short help \n",
    "\n",
    "to get benchmark type `scrapy bench` \n",
    "\n",
    "\n",
    "**FETCH** :  fetch a URL using the Scrapy  downloader \n",
    "\n",
    "to fetch a website : \n",
    "\n",
    "`scrapy  fetch http://google.com` \n",
    "\n",
    "we get back raw HTML markup of google.com \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "iCPrDjp5BB5e"
   },
   "source": [
    "### startproject\n",
    "[Doc](https://docs.scrapy.org/en/latest/topics/commands.html#startproject)\n",
    "\n",
    "\n",
    "Syntax: scrapy startproject \\<project_name> [project_dir]\n",
    "\n",
    "Creates a new Scrapy project named project_name, under the project_dir directory.\n",
    "\n",
    "If project_dir wasn’t specified, project_dir will be the same as project_name.\n",
    "\n",
    "Example :\n",
    "` scrapy startproject project1 mainproject`\n",
    "\n",
    "\n",
    "To handel multi project in one project look at [this](https://stackoverflow.com/questions/31662797/getting-scrapy-project-settings-when-script-is-outside-of-root-directory)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ebse0FKgBDHf"
   },
   "source": [
    "### genspider\n",
    "[Doc](https://docs.scrapy.org/en/latest/topics/commands.html#genspider)\n",
    "\n",
    "`scrapy genspider [-t template] <name> <domain>` \n",
    "\n",
    "**NOTE**: the URL should not end with slash \"/\" in domain variable  and not start with \"https://\" or \"http://\" \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_Eox_tJKE4d5"
   },
   "source": [
    "# Using Shell"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HxbdFEdOE8tv"
   },
   "source": [
    "[Doc](https://docs.scrapy.org/en/latest/topics/shell.html?highlight=shell)\n",
    "\n",
    "\n",
    "The Scrapy shell is an interactive shell where you can try and debug your scraping code very quickly, without having to run the spider. It’s meant to be used for testing data extraction code, but you can actually use it for testing any kind of code as it is also a regular Python shell.\n",
    "\n",
    "\n",
    "for start:\n",
    "\n",
    "`scrapy shell`\n",
    "\n",
    "\n",
    "**Some importent feature of shell**:\n",
    "1. The shell is used for testing XPath or CSS\n",
    "2. shell also works for local files.\n",
    "3. [Available Shortcuts](https://docs.scrapy.org/en/latest/topics/shell.html?highlight=shell#available-shortcuts)\n",
    "4.[Avalibale Scrapy objects](https://docs.scrapy.org/en/latest/topics/shell.html?highlight=shell#available-scrapy-objects)\n",
    "\n",
    "get website in sell : \n",
    "\n",
    "1. before shell and in shell command : `scrapy shell \"url\"`\n",
    "2. `fetch(\"url\")`\n",
    "3. `r = scrapy.Request(url = 'https//:www.google.com')` and `fetch(r)`\n",
    "\n",
    "\n",
    "for the raw HTML markup get back : \n",
    "\n",
    "\n",
    "`response.body`\n",
    "\n",
    "how scrapy see website : \n",
    "\n",
    "`view(response)`\n",
    "\n",
    "\n",
    "**Note** : scrapy doesn't work with Java script so the part of website it is use Java script it is not visible in scrapy request response\n",
    "\n",
    "for test xpath it is working:\n",
    "`response.xpath('//title/text()').get()`\n",
    "\n",
    "\n",
    "**Note** : when there are multiple element to get back use `getall()` \n",
    "`response.xpath('//text()').getall()`\n",
    "\n",
    "<br>\n",
    "\n",
    "**Example** : \n",
    "```\n",
    "scrapy shell \n",
    "r = scrapy.Request('https://www.google.com\")\n",
    "fetch(r) \n",
    "response.body \n",
    "view(response)\n",
    "title = response.xpath('h1')\n",
    "```\n",
    "\n",
    "\n",
    "**very important part**\n",
    "\n",
    "[Invoking the shell from spiders to inspect responses\n",
    "](https://docs.scrapy.org/en/latest/topics/shell.html?highlight=shell#invoking-the-shell-from-spiders-to-inspect-responses)\n",
    "\n",
    "Sometimes you want to inspect the responses that are being processed in a certain point of your spider, if only to check that response you expect is getting there.\n",
    "\n",
    "<br>\n",
    "\n",
    "This can be achieved by using the scrapy.shell.inspect_response function.\n",
    "\n",
    "<br>\n",
    "\n",
    "Here’s an example of how you would call it from your spider:\n",
    "\n",
    "```\n",
    "import scrapy\n",
    "\n",
    "\n",
    "class MySpider(scrapy.Spider):\n",
    "    name = \"myspider\"\n",
    "    start_urls = [\n",
    "        \"http://example.com\",\n",
    "        \"http://example.org\",\n",
    "        \"http://example.net\",\n",
    "    ]\n",
    "\n",
    "    def parse(self, response):\n",
    "        # We want to inspect one specific response.\n",
    "        if \".org\" in response.url:\n",
    "            from scrapy.shell import inspect_response\n",
    "            inspect_response(response, self)\n",
    "\n",
    "        # Rest of parsing code.\n",
    "\n",
    "```\n",
    "\n",
    "When you run the spider, you will get something similar to this:\n",
    "\n",
    "```\n",
    "2014-01-23 17:48:31-0400 [scrapy.core.engine] DEBUG: Crawled (200) <GET http://example.com> (referer: None)\n",
    "2014-01-23 17:48:31-0400 [scrapy.core.engine] DEBUG: Crawled (200) <GET http://example.org> (referer: None)\n",
    "[s] Available Scrapy objects:\n",
    "[s]   crawler    <scrapy.crawler.Crawler object at 0x1e16b50>\n",
    "...\n",
    "\n",
    ">>> response.url\n",
    "'http://example.org'\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3WEIA3qV--9k"
   },
   "source": [
    "# Spider \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gKH1WF4aC46Q"
   },
   "source": [
    "Doc \n",
    "\n",
    "**some crazy note**\n",
    "\n",
    "we can't add \"http\" or \"https\" to the start of allowed_domains in spider\n",
    " \n",
    " <font color='red'> Wrong use </font> : \n",
    "\n",
    "`allowed_domains = ['https://www.google.com']`\n",
    "\n",
    "`allowed_domains = ['http://www.google.com']`\n",
    "\n",
    "scrapy in default use http methode so for thoese website that use https change the \"start_urls in spider :\n",
    "`start_urls = ['https://google.com']`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gwNYT3aSv30i"
   },
   "source": [
    "# XPath & Css\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "RPzO4kYSCzse"
   },
   "source": [
    "XPath stands for XML PATH LANGUAGE \n",
    "\n",
    "\n",
    "CSS standing for Cascading Style Sheet \n",
    "\n",
    "\n",
    "the cheat sheet for these part it is in google drive files folder \n",
    "\n",
    "\n",
    "[CSS cheat sheet](https://drive.google.com/file/d/18lvabIhNczV5BVr-TE3zliqGYkNhVK6Y/view?usp=sharing)\n",
    "\n",
    "\n",
    "[XPath cheat sheet](https://drive.google.com/file/d/1koAnuFx5NIsergoB0cxsoQzGpXPZeVyt/view?usp=sharing)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "l0u5G6DCyvsq"
   },
   "source": [
    "## XPath\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Z3BnjbqY3bad"
   },
   "source": [
    "\n",
    "###CLASS & ID\n",
    "to select any element by its class attribute value we use the\n",
    "following syntax:\n",
    "\n",
    "`//elementName[@attributeName=’value’]`\n",
    "\n",
    "Example\n",
    "\n",
    "`//div[@class=\"intro\"]/p`\n",
    "\n",
    "`//p[@id=\"outside\"]/p`\n",
    "\n",
    "**element with two class**\n",
    "\n",
    "`//p[@class=\"bold italic\"]`\n",
    "\n",
    "OR \n",
    "\n",
    "Although the element does have two classes we can for example\n",
    "search for a substring within the class attribute value by using the\n",
    "contains function.\n",
    "\n",
    "**contains function**\n",
    "\n",
    "`//p[contains(@class, \"italic\")]`\n",
    "\n",
    "The contains function takes two arguments:\n",
    "\n",
    "1. The first one is where to search, whether on the class\n",
    "attribute value, id or anything else.\n",
    "2. The second argument is the value you’re looking for.\n",
    "3.  The value you search for is also case sensitive, so be\n",
    "careful!\n",
    "\n",
    "\n",
    "\n",
    "Sometimes we want also to select elements based on a foreign\n",
    "attribute which doesn’t belong to HTML markup standard. For\n",
    "example to select the “li” element with the attribute “dataidentifier” equals to 7 in this case we use the following XPath\n",
    "expression:\n",
    "\n",
    "`//li[@data-identifier=\"7\"]`\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hzdgyygo3gvo"
   },
   "source": [
    "\n",
    "\n",
    "###Value lookup\n",
    "\n",
    "select all the \"a\" elements in which the \"href\" attribute value starts with \"https\" and not \"http\", in this case we can\n",
    "use the following XPath expression:\n",
    "\n",
    "`//a[starts-with(@class, ‘https’)]`\n",
    "\n",
    "**Extract  text from element**\n",
    "\n",
    "`//p[@id=\"outside\"]/text()`\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8HHvYBef3iHv"
   },
   "source": [
    "###The position\n",
    "\n",
    "get second \"li\" element \n",
    "\n",
    "`//ul[@id=\"items\"]/li[2]`\n",
    " \n",
    " OR \n",
    "in this model we can add others attribute to specified our search\n",
    " `//ul[@id=\"items\"]/li[position() = 2 and text() = \"Item 2\"]`\n",
    "\n",
    " this one besid the posstion have contains \"Item 2\" in text varibale \n",
    "\n",
    " **In contrast to the “and” logical operator we also have the “or” logical\n",
    "operator.**\n",
    "\n",
    "<br>\n",
    "\n",
    "**NOTE**:\n",
    "\n",
    "In XPath everything we write within [] is known as a\n",
    "predicate.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "SW6CSzjw3jIz"
   },
   "source": [
    "\n",
    "### Xpath axex \n",
    "\n",
    "\n",
    "In XPath an axis is used to search for an element based on its\n",
    "relatioship with another element, we have some axes which we can\n",
    "use to navigate up and down in the HTML markup.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hzTDJnhLoj3L"
   },
   "source": [
    "### copy of word file\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Fhzrn3mWosdW"
   },
   "source": [
    "**Element Node**: tags like `<p></p> <a></a>`\n",
    "\n",
    "**Attribute Node**: @href @id @class \n",
    "\n",
    "**Comment Node**: represent the comment in the HTML document.\n",
    "\n",
    "**Text Node**: represent text content within an Element Node.\n",
    "\n",
    "Example\n",
    "\n",
    "```\n",
    "<ul>\n",
    "<li>Item1</li>\n",
    "<li>Item2</li> \n",
    "<li>Item3</li>\n",
    "<li>Item4</li> \n",
    "</ul>\n",
    "```\n",
    "\n",
    "If want select 3 and 4 item :\n",
    "\n",
    "`//li[position() = 3 or position() = 4]`\n",
    "\n",
    "Or\n",
    "\n",
    "`//li[position() =  last()-1 or position() = last()]`\n",
    "\n",
    "Or\n",
    "\n",
    "`//li[position() >=3]`\n",
    "\n",
    "\n",
    "\n",
    "**How find all element with a specific attribute**\n",
    "\n",
    "`//*[contains(@class,title)]`\n",
    "\n",
    "**How find all node that start with specific text value**\n",
    "\n",
    "`//*[starts-with(text(), \"sometext”)]`\n",
    "\n",
    "**[index]** -> Selects a node based on its position, it’s similar to the position() function but you can’t include any logical operator inside it like (or, and), arithmetic operators ( +, -, *, /)\n",
    "\n",
    "**[position() = int]** -> Selects a node based on its position in addition to that you can use logical operators inside that predicate, arithmetic operators and comparison operators ... Ex [position()>= 3 or position() = 4].\n",
    "\n",
    "\n",
    "**[last()]** -> Selects the last node.\n",
    "\n",
    "**[contains(haystack, needle)]** -> Determines whether the first argument (haystack) contains the second argument (needle).\n",
    "\n",
    "**[starts-with(haystack, needle)]** -> Determines whether the first argument (haystack) starts with the second argument (needle)\n",
    "\n",
    "**[text() = String]** -> Selects a node that its text content equal to a String\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "SdhliojnChy6"
   },
   "source": [
    "# project example 1\n",
    "in this section we talk about some example that use in project "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wwaF-IS3Dkyi"
   },
   "source": [
    "## get the list of element \n",
    "\n",
    "we can select a list of element then use one by onr in for loop \n",
    "\n",
    "```\n",
    "def parse(self, response):\n",
    "    countries = response.xpath(\"//td/a\")\n",
    "    for country in countries:\n",
    "        name = country.xpath(\".//text()\").get()\n",
    "        link = country.xpath(\".//@href\").get()\n",
    "\n",
    "        # absolute_url = f\"https://www.worldometers.info{link}\"\n",
    "        # absolute_url = response.urljoin(link)\n",
    "\n",
    "        yield response.follow(url=link, callback=self.parse_country, meta={'country_name': name})\n",
    "\n",
    "```\n",
    "\n",
    "**Note** : using ` response.follow` insted convert reletive url to absolut url\n",
    "USE mete : this attribuite send the data that we want to the call back methode so in the call back methode we can get that data \n",
    "\n",
    "```\n",
    "def parse(self, response):\n",
    "    yield response.follow(url=link, callback=self.parse_country, meta={'country_name': name})\n",
    "\n",
    "def parse_country(self, response):\n",
    "    name = response.meta['country_name']\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "RyPcvf0MGbpG"
   },
   "source": [
    "\n",
    "## get absolut url \n",
    "the url that we get from href attributie it is possiblt to be not complete url so it is need to add the first part of the url from the website so can be use in Request methode.\n",
    "\n",
    "```\n",
    "link = country.xpath(\".//@href\").get()\n",
    "\n",
    "absolute_url = f\"https://www.worldometers.info{link}\"\n",
    "OR\n",
    "absolute_url = response.urljoin(link)\n",
    "```\n",
    "\n",
    "**using ` response.follow` insted convert reletive url to absolut url**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "t4MdpKoYGdym"
   },
   "source": [
    "\n",
    "## How to follow a link:\n",
    "\n",
    "To extract data from links in a page we can send the Request method to Scrapy so Scrapy extracts that data we want from that link to do this:\n",
    "\n",
    "\n",
    "First in parse Method extract links that must be extract data from it.\n",
    "\n",
    " `link = country.xpath(\".//@href\").get()`\n",
    "\n",
    "Then:\n",
    "\n",
    "` yield response.follow(url=link)`\n",
    "\n",
    "For extract data of XPath element use extract() function end of the code \n",
    "\n",
    "   `name = countries.xpath(\".//text()\").extract()`\n",
    "\n",
    "**Note** : To get the first value of the element can use `extractfirst()`\n",
    "\n",
    "**NOTE**: `allowed_domains` shouldn't include the http prefix eg:\n",
    "`allowed_domains = [\"boliga.dk\"]`\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PbGyma4WFPFN"
   },
   "source": [
    "## Go next page:\n",
    "1. Find next button \n",
    "\n",
    "2. Extract @href attribute of the button and send Request to scrape next page \n",
    "\n",
    "3. When getting the link first check it is not empty because on the last page there is no next page button\n",
    "4. The next page value is not full URL and it is only the last part that change in every page  So, we need to join them to the main URL\n",
    "\n",
    "```\n",
    "\n",
    "Next_page = response.selector.xpath(\"//a[@class='next_page']/@href\").extract_first()  #return a string \n",
    "If next_page is not None:\n",
    "    Next_page_link=response.urljoin(next_page)\n",
    "    yield scrapy.Request(url=Next_page_link , callback = self.parse)\n",
    "\n",
    "```\n",
    "\n",
    "**NOTE**: `allowed_domains` shouldn't include the http prefix eg:\n",
    "`allowed_domains = [\"boliga.dk\"]`\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "x1x5WykTMz3O"
   },
   "source": [
    "# [Storing the scraped data](https://docs.scrapy.org/en/latest/intro/tutorial.html#storing-the-scraped-data)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "VgTh-R20M2OU"
   },
   "source": [
    "One of the most frequently required features when implementing scrapers is being able to store the scraped data properly and, quite often, that means generating an “export file” with the scraped data (commonly called “export feed”) to be consumed by other systems.\n",
    "\n",
    "\n",
    "Scrapy provides this functionality out of the box with the Feed Exports, which allows you to generate feeds with the scraped items, using multiple serialization formats and storage backends.\n",
    "\n",
    "\n",
    "\n",
    "Serialization formats\n",
    "For serializing the scraped data, the feed exports use the Item exporters. These formats are supported out of the box:\n",
    "\n",
    "* JSON\n",
    "\n",
    "* JSON lines\n",
    "\n",
    "* CSV\n",
    "\n",
    "* XML\n",
    "\n",
    "\n",
    "\n",
    "for save extracacted data we can ues scrapy command line code\n",
    "\n",
    "` scrapy crawal spider_name -o file_name.csv `\n",
    "\n",
    "\n",
    "This code saves a CSV file in the route directory of the project(.cfg file path)\n",
    "\n",
    "\n",
    "If you want to change the directory for save file go to the setting file and add:\n",
    "\n",
    "`FEED_URI = \"file:custompath/filename.csv\"`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "UxyMOs0SyG5M"
   },
   "source": [
    "# FAQ\n",
    "\n",
    "## check a allow domain in a website\n",
    "for check a allow domain in a website :\n",
    "\n",
    "go to the robots sub domain in website \n",
    "\n",
    "`www.digikala.com/robots.txt`\n",
    "\n",
    "## Disabale javascripts \n",
    "ta disabale javascripts when looking a website :\n",
    "\n",
    "`Ctrl + shift + I > Ctrl + shift + P > Disabale javascript `\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JJ1U64EOrVru"
   },
   "source": [
    "# setting.py\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vuhk3NuIrdqA"
   },
   "source": [
    "## change the encoding file when saveing them\n",
    "\n",
    "`FEED_EXPORT_ENCODING = \"utf_8\" `\n",
    "\n",
    "\n",
    "## Add user Agents\n",
    "to pervent blocked scrapy by websit that check USER_AGENT :\n",
    "get a facke one from web \n",
    "`USER_AGENT = 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.77 Safari/537.36'`\n",
    "\n",
    "**Note**:\n",
    "this approach use this user_agent for all request.\n",
    "\n",
    "**for multiple request header**:\n",
    "we can do this in setting.py\n",
    "```\n",
    "DEAFALT_REQUEST_HEADER = {\n",
    "    'User_Agent' :'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.77 Safari/537.36'\n",
    "```\n",
    "\n",
    "OR \n",
    "\n",
    "overwrite the start methode in spider.py \n",
    "\n",
    "```\n",
    "def start_requests(self):\n",
    "    yield scrapy.Request(url='www.oteuao.eoe' , callback = self.parse, headers = {\n",
    "         'User_Agent' :'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.77 Safari/537.36'\n",
    "    }\n",
    "\n",
    "\n",
    "```\n",
    "and for all other request in the spider you must add the user agent to request \n",
    "\n",
    "##change the directory for save file\n",
    "If you want to change the directory for save file go to the setting file and add:\n",
    "\n",
    "`FEED_URI = \"file:custompath/filename.csv\"`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Wf-SzmMu5qy7"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [
    "_Eox_tJKE4d5",
    "3WEIA3qV--9k"
   ],
   "name": "scrapy.ipynb",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "main",
   "language": "python",
   "name": "main"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
