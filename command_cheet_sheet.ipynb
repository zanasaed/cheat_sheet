{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"command_cheet_sheet.ipynb","provenance":[],"toc_visible":true,"authorship_tag":"ABX9TyMC/eO5YCmV1Q7Yt0DmomAQ"},"kernelspec":{"name":"python3","display_name":"Python 3"}},"cells":[{"cell_type":"markdown","metadata":{"id":"04EGDMnB6SKn"},"source":["# print\r\n"]},{"cell_type":"code","metadata":{"id":"1-lsfLzn6LMl"},"source":["name = 'Fred'\r\n","\r\n","# Using the old .format() method:\r\n","print('His name is {var}.'.format(var=name))\r\n","# His name is Fred.\r\n","\r\n","# Using f-strings:\r\n","print(f'His name is {name}.')\r\n","# His name is Fred.\r\n","\r\n","print(f'His name is {name!r}')\r\n","# His name is 'Fred'\r\n","\r\n","d = {'a':123,'b':456}\r\n","print(f\"Address: {d['a']} Main Street\")\r\n","# Address: 123 Main Street\r\n","\r\n","library = [('Author', 'Topic', 'Pages'), ('Twain', 'Rafting', 601), ('Feynman', 'Physics', 95), ('Hamilton', 'Mythology', 144)]\r\n","for book in library:\r\n","    print(f'{book[0]:{10}} {book[1]:{10}} {book[2]:.>{7}}') # here .> was added\r\n","# Author     Topic      ..Pages\r\n","# Twain      Rafting    ....601\r\n","# Feynman    Physics    .....95\r\n","# Hamilton   Mythology  ....144\r\n","\r\n","from datetime import datetime\r\n","today = datetime(year=2018, month=1, day=27)\r\n","print(f'{today:%B %d, %Y}')\r\n","#January 27, 2018\r\n","\r\n","print(today)\r\n","#2018-01-27 00:00:00"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"lAdIsgVt7ayJ"},"source":["# Files\r\n"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"ppUkDXcH6Tl-","executionInfo":{"status":"ok","timestamp":1613380070797,"user_tz":-210,"elapsed":802,"user":{"displayName":"zana saedpanah","photoUrl":"","userId":"08711949870913782480"}},"outputId":"9a60479d-bd38-43e5-b379-e81a8ece18e9"},"source":["%%writefile test.txt\r\n","Hello, this is a quick test file.\r\n","This is the second line of the file.\r\n","''' Overwriting test.txt '''\r\n","\r\n","# Open the text.txt file we created earlier\r\n","my_file = open('test.txt')\r\n","\r\n","my_file\r\n","''' <_io.TextIOWrapper name='test.txt' mode='r' encoding='UTF-8'> '''\r\n","\r\n","\r\n","### .read() and .seek() ###\r\n","# We can now read the file\r\n","my_file.read()\r\n","''' Hello, this is a quick test file.\\nThis is the second line of the file.\\n'''\r\n","\r\n","# But what happens if we try to read it again?\r\n","my_file.read()\r\n","''' '' '''\r\n","\r\n","# Seek to the start of file (index 0)\r\n","my_file.seek(0)\r\n","# Now read again\r\n","my_file.read()\r\n","''' Hello, this is a quick test file.\\nThis is the second line of the file.\\n '''\r\n","\r\n","### .readlines() ###\r\n","# Readlines returns a list of the lines in the file\r\n","my_file.seek(0)\r\n","my_file.readlines()\r\n","''' ['Hello, this is a quick test file.\\n',\r\n","  'This is the second line of the file.\\n'] '''\r\n","\r\n","\r\n","### Writing to a File ###\r\n","# Add a second argument to the function, 'w' which stands for write.\r\n","# Passing 'w+' lets us read and write to the file\r\n","my_file = open('test.txt','w+')\r\n","# Write to the file\r\n","my_file.write('This is a new first line')\r\n","'''  24 '''\r\n","# Read the file\r\n","my_file.seek(0)\r\n","my_file.read()\r\n","''' This is a new first line '''\r\n","\r\n","### Appending to a File ###\r\n","my_file = open('test.txt','a+')\r\n","my_file.write('\\nThis line is being appended to test.txt')\r\n","my_file.write('\\nAnd another line here.')\r\n","'''  23 '''\r\n","my_file.seek(0)\r\n","print(my_file.read())\r\n"," '''This is a new first line\r\n","    This line is being appended to test.txt\r\n","    And another line here.\r\n","    '''\r\n","\r\n","\r\n","### Appending with %%writefile ###\r\n","%%writefile -a test.txt\r\n","This is more text being appended to test.txt\r\n","And another line here.\r\n","''' Appending to test.txt '''\r\n","### Aliases and Context Managers ###\r\n","with open('test.txt','r') as txt:\r\n","    first_line = txt.readlines()[0]\r\n","print(first_line)\r\n","''' This is a new first line '''\r\n","\r\n","### Iterating through a File ###\r\n","with open('test.txt','r') as txt:\r\n","    for line in txt:\r\n","        print(line, end='')  # the end='' argument removes extra linebreaks\r\n","        '''\r\n","This is a new first line\r\n","This line is being appended to test.txt\r\n","And another line here.\r\n","This is more text being appended to test.txt\r\n","And another line here.\r\n","'''\r\n"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Overwriting test.txt\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"3XyC9syXK0JO"},"source":["!pip install jupyter_contrib_nbextensions\r\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"C1a5I90MLSej"},"source":[""],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"AXaGvMqL8B3I","executionInfo":{"status":"ok","timestamp":1613380285804,"user_tz":-210,"elapsed":902,"user":{"displayName":"zana saedpanah","photoUrl":"","userId":"08711949870913782480"}},"outputId":"3768412e-823d-4071-c7b9-59b82d5c1499"},"source":[""],"execution_count":null,"outputs":[{"output_type":"stream","text":["This is a new first line\n","This line is being appended to test.txt\n","And another line here.\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"ib-ufXA2716m","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1613669177647,"user_tz":-210,"elapsed":984,"user":{"displayName":"zana saedpanah","photoUrl":"","userId":"08711949870913782480"}},"outputId":"c39310b1-7050-4524-ea40-5df322894b15"},"source":["!jupyter nbextension enable hinterland/hinterland\r\n"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Enabling notebook extension hinterland/hinterland...\n","      - Validating: problems found:\n","        - require? \u001b[31m X\u001b[0m hinterland/hinterland\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"STe0_ldbvQGb"},"source":["# Regx"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":69},"id":"xuumCAe7vS7D","executionInfo":{"status":"ok","timestamp":1613478195527,"user_tz":-210,"elapsed":870,"user":{"displayName":"zana saedpanah","photoUrl":"","userId":"08711949870913782480"}},"outputId":"4ca1edbb-d01b-43cd-db09-8ae634bdfe79"},"source":["import re\r\n","\r\n","\r\n","text = \"The agent's phone number is 408-555-1234. Call soon!\"\r\n","pattern = 'phone'\r\n","re.search(pattern,text)\r\n","'''<_sre.SRE_Match object; span=(12, 17), match='phone'>'''\r\n","\r\n","pattern = \"NOT IN TEXT\"\r\n","re.search(pattern,text)\r\n","''' None'''\r\n","\r\n","pattern = 'phone'\r\n","match = re.search(pattern,text)\r\n","match\r\n","'''<_sre.SRE_Match object; span=(12, 17), match='phone'>'''\r\n","\r\n","match.span()\r\n","'''(12, 17)'''\r\n","\r\n","match.start()\r\n","'''12'''\r\n","match.end()\r\n","'''17'''\r\n","### But what if the pattern occurs more than once? ###\r\n","text = \"my phone is a new phone\"\r\n","match = re.search(\"phone\",text)\r\n","match.span()\r\n","'''(3, 8)'''\r\n","\r\n","\r\n","### Notice it only matches the first instance. If we wanted a list of all matches, we can use .findall() method: ###\r\n","matches = re.findall(\"phone\",text)\r\n","matches\r\n","'''['phone', 'phone']'''\r\n","len(matches)\r\n","'''2'''\r\n","\r\n","### To get actual match objects, use the iterator:###\r\n","for match in re.finditer(\"phone\",text):\r\n","    print(match.span())\r\n","\r\n","'''(3, 8)\r\n","(18, 23)'''\r\n","\r\n","\r\n","### If you wanted the actual text that matched, you can use the .group() method. ###\r\n","match.group()\r\n","'''  'phone' '''"],"execution_count":null,"outputs":[{"output_type":"stream","text":["(3, 8)\n","(18, 23)\n"],"name":"stdout"},{"output_type":"execute_result","data":{"application/vnd.google.colaboratory.intrinsic+json":{"type":"string"},"text/plain":["\"  'phone' \""]},"metadata":{"tags":[]},"execution_count":2}]},{"cell_type":"markdown","metadata":{"id":"ZUTPC4agtTmH"},"source":["## Patterns\n","\n","So far we've learned how to search for a basic string. What about more complex examples? Such as trying to find a telephone number in a large string of text? Or an email address?\n","\n","We could just use search method if we know the exact phone or email, but what if we don't know it? We may know the general format, and we can use that along with regular expressions to search the document for strings that match a particular pattern.\n","\n","This is where the syntax may appear strange at first, but take your time with this; often it's just a matter of looking up the pattern code.\n","\n","Let's begin!"]},{"cell_type":"markdown","metadata":{"id":"ql3Wn58AtTmK"},"source":["### Identifiers for Characters in Patterns\n","\n","Characters such as a digit or a single string have different codes that represent them. You can use these to build up a pattern string. Notice how these make heavy use of the backwards slash \\ . Because of this when defining a pattern string for regular expression we use the format:\n","\n","    r'mypattern'\n","    \n","placing the r in front of the string allows python to understand that the \\ in the pattern string are not meant to be escape slashes.\n","\n","Below you can find a table of all the possible identifiers:"]},{"cell_type":"markdown","metadata":{"id":"fNbvYEjStTmM"},"source":["<table ><tr><th>Character</th><th>Description</th><th>Example Pattern Code</th><th >Exammple Match</th></tr>\n","\n","<tr ><td><span >\\d</span></td><td>A digit</td><td>file_\\d\\d</td><td>file_25</td></tr>\n","\n","<tr ><td><span >\\w</span></td><td>Alphanumeric</td><td>\\w-\\w\\w\\w</td><td>A-b_1</td></tr>\n","\n","\n","\n","<tr ><td><span >\\s</span></td><td>White space</td><td>a\\sb\\sc</td><td>a b c</td></tr>\n","\n","\n","\n","<tr ><td><span >\\D</span></td><td>A non digit</td><td>\\D\\D\\D</td><td>ABC</td></tr>\n","\n","<tr ><td><span >\\W</span></td><td>Non-alphanumeric</td><td>\\W\\W\\W\\W\\W</td><td>*-+=)</td></tr>\n","\n","<tr ><td><span >\\S</span></td><td>Non-whitespace</td><td>\\S\\S\\S\\S</td><td>Yoyo</td></tr></table>"]},{"cell_type":"markdown","metadata":{"id":"a4eadugDtTmO"},"source":["For example:"]},{"cell_type":"code","metadata":{"id":"ylTt0zrptTmP","colab":{"base_uri":"https://localhost:8080/","height":35},"executionInfo":{"status":"ok","timestamp":1613478200775,"user_tz":-210,"elapsed":801,"user":{"displayName":"zana saedpanah","photoUrl":"","userId":"08711949870913782480"}},"outputId":"a9c107be-228f-421f-e8a0-8958c29502d4"},"source":["text = \"My telephone number is 408-555-1234\"\r\n","phone = re.search(r'\\d\\d\\d-\\d\\d\\d-\\d\\d\\d\\d',text)\r\n","phone = re.search(r'\\d\\d\\d-\\d\\d\\d-\\d\\d\\d\\d',text)\r\n","''' '408-555-1234'''"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"application/vnd.google.colaboratory.intrinsic+json":{"type":"string"},"text/plain":["\" '408-555-1234\""]},"metadata":{"tags":[]},"execution_count":3}]},{"cell_type":"markdown","metadata":{"id":"-BwlnepPzcAo"},"source":["Notice the repetition of \\d. That is a bit of an annoyance, especially if we are looking for very long strings of numbers. Let's explore the possible quantifiers.\n","\n","### Quantifiers\n","\n","Now that we know the special character designations, we can use them along with quantifiers to define how many we expect."]},{"cell_type":"markdown","metadata":{"id":"9kav6Ja0tTmS"},"source":["<table ><tr><th>Character</th><th>Description</th><th>Example Pattern Code</th><th >Exammple Match</th></tr>\n","\n","<tr ><td><span >+</span></td><td>Occurs one or more times</td><td>\tVersion \\w-\\w+</td><td>Version A-b1_1</td></tr>\n","\n","<tr ><td><span >{3}</span></td><td>Occurs exactly 3 times</td><td>\\D{3}</td><td>abc</td></tr>\n","\n","\n","\n","<tr ><td><span >{2,4}</span></td><td>Occurs 2 to 4 times</td><td>\\d{2,4}</td><td>123</td></tr>\n","\n","\n","\n","<tr ><td><span >{3,}</span></td><td>Occurs 3 or more</td><td>\\w{3,}</td><td>anycharacters</td></tr>\n","\n","<tr ><td><span >\\*</span></td><td>Occurs zero or more times</td><td>A\\*B\\*C*</td><td>AAACC</td></tr>\n","\n","<tr ><td><span >?</span></td><td>Once or none</td><td>plurals?</td><td>plural</td></tr></table>"]},{"cell_type":"markdown","metadata":{"id":"KOPZX1KktTmU"},"source":["Let's rewrite our pattern using these quantifiers:"]},{"cell_type":"code","metadata":{"id":"fxhdBdSVtTmV","outputId":"ab98f3ee-da3d-498f-ec23-ca50135a6163"},"source":["re.search(r'\\d{3}-\\d{3}-\\d{4}',text)"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["<_sre.SRE_Match object; span=(23, 35), match='408-555-1234'>"]},"metadata":{"tags":[]},"execution_count":25}]},{"cell_type":"markdown","metadata":{"id":"LHQJzR0ptTmX"},"source":["### Groups\n","\n","What if we wanted to do two tasks, find phone numbers, but also be able to quickly extract their area code (the first three digits). We can use groups for any general task that involves grouping together regular expressions (so that we can later break them down). \n","\n","Using the phone number example, we can separate groups of regular expressions using parentheses:"]},{"cell_type":"code","metadata":{"id":"J2yZPtUPtTmZ"},"source":["phone_pattern = re.compile(r'(\\d{3})-(\\d{3})-(\\d{4})')\r\n","\r\n","results = re.search(phone_pattern,text)\r\n","\r\n","# The entire result\r\n","results.group()\r\n","''' '408-555-1234' '''\r\n","\r\n","# Can then also call by group position.\r\n","# remember groups were separated by parentheses ()\r\n","# Something to note is that group ordering starts at 1. Passing in 0 returns everything\r\n","results.group(1)\r\n","''' '408' '''\r\n","results.group(2)\r\n","''' '555' '''\r\n","results.group(3)\r\n","''' '1234' '''\r\n","# We only had three groups of parentheses\r\n","results.group(4)\r\n","'''\r\n","---------------------------------------------------------------------------\r\n","IndexError                                Traceback (most recent call last)\r\n","<ipython-input-32-79a918a9b5dc> in <module>()\r\n","      1 # We only had three groups of parentheses\r\n","----> 2 results.group(4)\r\n","\r\n","IndexError: no such group\r\n","'''\r\n"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"JhiMZpA9tTnA"},"source":["### Additional Regex Syntax\n","\n","#### Or operator |\n","\n","Use the pipe operator to have an **or** statment. For example"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":35},"id":"-4HJK0yitTnA","executionInfo":{"status":"ok","timestamp":1613479217734,"user_tz":-210,"elapsed":761,"user":{"displayName":"zana saedpanah","photoUrl":"","userId":"08711949870913782480"}},"outputId":"3393f62e-44be-48ec-952e-0a6c6e604624"},"source":["re.search(r\"man|woman\",\"This man was here.\")\r\n","''' <_sre.SRE_Match object; span=(5, 8), match='man'> '''\r\n","\r\n","re.search(r\"man|woman\",\"This woman was here.\")\r\n","''' <_sre.SRE_Match object; span=(5, 10), match='woman'> '''"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"application/vnd.google.colaboratory.intrinsic+json":{"type":"string"},"text/plain":["\" <_sre.SRE_Match object; span=(5, 10), match='woman'> \""]},"metadata":{"tags":[]},"execution_count":4}]},{"cell_type":"markdown","metadata":{"id":"8a3lFM1atTnE"},"source":["#### The Wildcard Character\n","\n","Use a \"wildcard\" as a placement that will match any character placed there. You can use a simple period **.** for this. For example:"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":35},"id":"HyXPdnCytTnE","executionInfo":{"status":"ok","timestamp":1613479218276,"user_tz":-210,"elapsed":1279,"user":{"displayName":"zana saedpanah","photoUrl":"","userId":"08711949870913782480"}},"outputId":"f18fc865-1c7b-4e48-9a0c-f68c3738dc5f"},"source":["re.findall(r\".at\",\"The cat in the hat sat here.\")\r\n","''' ['cat', 'hat', 'sat'] '''\r\n","\r\n","re.findall(r\".at\",\"The bat went splat\")\r\n","''' ['bat', 'lat'] '''\r\n","\r\n"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"application/vnd.google.colaboratory.intrinsic+json":{"type":"string"},"text/plain":["\" ['bat', 'lat'] \""]},"metadata":{"tags":[]},"execution_count":5}]},{"cell_type":"markdown","metadata":{"id":"pl0K3-PYtTnK"},"source":["Notice how we only matched the first 3 letters, that is because we need a **.** for each wildcard letter. Or use the quantifiers described above to set its own rules."]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"9saqrakPtTnL","executionInfo":{"status":"ok","timestamp":1613479218283,"user_tz":-210,"elapsed":1276,"user":{"displayName":"zana saedpanah","photoUrl":"","userId":"08711949870913782480"}},"outputId":"39de78fe-2395-4364-f11a-62ae484091d1"},"source":["re.findall(r\"...at\",\"The bat went splat\")"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["['e bat', 'splat']"]},"metadata":{"tags":[]},"execution_count":6}]},{"cell_type":"markdown","metadata":{"id":"9zK0s3HAtTnM"},"source":["However this still leads the problem to grabbing more beforehand. Really we only want words that end with \"at\"."]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"I7KDINO1tTnN","executionInfo":{"status":"ok","timestamp":1613479218284,"user_tz":-210,"elapsed":1264,"user":{"displayName":"zana saedpanah","photoUrl":"","userId":"08711949870913782480"}},"outputId":"72c9d991-5795-4445-a4ca-f403889a5c0a"},"source":["# One or more non-whitespace that ends with 'at'\n","re.findall(r'\\S+at',\"The bat went splat\")"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["['bat', 'splat']"]},"metadata":{"tags":[]},"execution_count":7}]},{"cell_type":"markdown","metadata":{"id":"CttsbBr-tTnO"},"source":["#### Starts With and Ends With\n","\n","We can use the **^** to signal starts with, and the **$** to signal ends with:"]},{"cell_type":"code","metadata":{"id":"UfHq9cfetTnP"},"source":["# Ends with a number\n","re.findall(r'\\d$','This ends with a number 2')\n","''' ['2'] '''\n","# Starts with a number\n","re.findall(r'^\\d','1 is the loneliest number.')\n","''' ['1'] '''"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"rf5BknDotTnS"},"source":["Note that this is for the entire string, not individual words!"]},{"cell_type":"markdown","metadata":{"id":"U_5yFX8OtTnS"},"source":["#### Exclusion\n","\n","To exclude characters, we can use the **^** symbol in conjunction with a set of brackets **[]**. Anything inside the brackets is excluded. For example:"]},{"cell_type":"code","metadata":{"id":"uJqWF-JXtTnT"},"source":["phrase = \"there are 3 numbers 34 inside 5 this sentence.\""],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"xkBG31CMtTnT","executionInfo":{"status":"ok","timestamp":1613478603191,"user_tz":-210,"elapsed":1759,"user":{"displayName":"zana saedpanah","photoUrl":"","userId":"08711949870913782480"}},"outputId":"4e3295c9-2799-4446-fa41-104acac30487"},"source":["re.findall(r'[^\\d]',phrase)"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["['t',\n"," 'h',\n"," 'e',\n"," 'r',\n"," 'e',\n"," ' ',\n"," 'a',\n"," 'r',\n"," 'e',\n"," ' ',\n"," ' ',\n"," 'n',\n"," 'u',\n"," 'm',\n"," 'b',\n"," 'e',\n"," 'r',\n"," 's',\n"," ' ',\n"," ' ',\n"," 'i',\n"," 'n',\n"," 's',\n"," 'i',\n"," 'd',\n"," 'e',\n"," ' ',\n"," ' ',\n"," 't',\n"," 'h',\n"," 'i',\n"," 's',\n"," ' ',\n"," 's',\n"," 'e',\n"," 'n',\n"," 't',\n"," 'e',\n"," 'n',\n"," 'c',\n"," 'e',\n"," '.']"]},"metadata":{"tags":[]},"execution_count":21}]},{"cell_type":"markdown","metadata":{"id":"RtUTh9uYtTnU"},"source":["To get the words back together, use a + sign "]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"aLIpNBvktTnV","executionInfo":{"status":"ok","timestamp":1613478603193,"user_tz":-210,"elapsed":1716,"user":{"displayName":"zana saedpanah","photoUrl":"","userId":"08711949870913782480"}},"outputId":"97cbc36f-42d1-421c-d908-b62c64fef2e0"},"source":["re.findall(r'[^\\d]+',phrase)"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["['there are ', ' numbers ', ' inside ', ' this sentence.']"]},"metadata":{"tags":[]},"execution_count":22}]},{"cell_type":"markdown","metadata":{"id":"va4fYQWItTnW"},"source":["We can use this to remove punctuation from a sentence."]},{"cell_type":"code","metadata":{"id":"sr9hnSuBtTnX"},"source":["test_phrase = 'This is a string! But it has punctuation. How can we remove it?'"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Z6yglSI4tTnX","executionInfo":{"status":"ok","timestamp":1613478603196,"user_tz":-210,"elapsed":1703,"user":{"displayName":"zana saedpanah","photoUrl":"","userId":"08711949870913782480"}},"outputId":"06e6a123-85d3-42dc-c455-000fefd2bc8f"},"source":["re.findall('[^!.? ]+',test_phrase)"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["['This',\n"," 'is',\n"," 'a',\n"," 'string',\n"," 'But',\n"," 'it',\n"," 'has',\n"," 'punctuation',\n"," 'How',\n"," 'can',\n"," 'we',\n"," 'remove',\n"," 'it']"]},"metadata":{"tags":[]},"execution_count":24}]},{"cell_type":"code","metadata":{"id":"k1nErIi4tTnY"},"source":["clean = ' '.join(re.findall('[^!.? ]+',test_phrase))"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"ef5z-NjXtTnY","executionInfo":{"status":"ok","timestamp":1613478603233,"user_tz":-210,"elapsed":1703,"user":{"displayName":"zana saedpanah","photoUrl":"","userId":"08711949870913782480"}},"outputId":"8e8027b2-0514-47ae-8cc2-2cfecadbe9ec"},"source":["clean"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"application/vnd.google.colaboratory.intrinsic+json":{"type":"string"},"text/plain":["'This is a string But it has punctuation How can we remove it'"]},"metadata":{"tags":[]},"execution_count":26}]},{"cell_type":"markdown","metadata":{"id":"T5_IeUp-tTnZ"},"source":["### Brackets for Grouping\n","\n","As we showed above we can use brackets to group together options, for example if we wanted to find hyphenated words:"]},{"cell_type":"code","metadata":{"id":"_CpCLs_btTnZ"},"source":["text = 'Only find the hypen-words in this sentence. But you do not know how long-ish they are'"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"xEumFeDDtTna","outputId":"0d3f1fd6-1d1d-4d29-a6bb-ab50499bbe4e"},"source":["re.findall(r'[\\w]+-[\\w]+',text)"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["['hypen-words', 'long-ish']"]},"metadata":{"tags":[]},"execution_count":49}]},{"cell_type":"markdown","metadata":{"id":"rR1llxsNtTnb"},"source":["### Parentheses for Multiple Options\n","\n","If we have multiple options for matching, we can use parentheses to list out these options. For Example:"]},{"cell_type":"code","metadata":{"id":"XLBblq5JtTnb"},"source":["# Find words that start with cat and end with one of these options: 'fish','nap', or 'claw'\n","text = 'Hello, would you like some catfish?'\n","texttwo = \"Hello, would you like to take a catnap?\"\n","textthree = \"Hello, have you seen this caterpillar?\""],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"p9OwuJNWtTnc","outputId":"f5dffefa-020a-494f-b5d7-1983b2a88721"},"source":["re.search(r'cat(fish|nap|claw)',text)"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["<_sre.SRE_Match object; span=(27, 34), match='catfish'>"]},"metadata":{"tags":[]},"execution_count":51}]},{"cell_type":"code","metadata":{"id":"3pH1Ld5CtTnd","outputId":"dc206c2c-1b48-466d-ed60-3ce589b140b8"},"source":["re.search(r'cat(fish|nap|claw)',texttwo)"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["<_sre.SRE_Match object; span=(32, 38), match='catnap'>"]},"metadata":{"tags":[]},"execution_count":52}]},{"cell_type":"code","metadata":{"id":"n0vEdGLItTne"},"source":["# None returned\n","re.search(r'cat(fish|nap|claw)',textthree)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"4gAplpRMtTng"},"source":["### Conclusion\n","\n","Excellent work! For full information on all possible patterns, check out: https://docs.python.org/3/howto/regex.html"]},{"cell_type":"markdown","metadata":{"id":"bqv2D0TgEQ1t"},"source":["# Working with spaCy in Python"]},{"cell_type":"code","metadata":{"id":"qUEM-8ywEPnT"},"source":["# Import spaCy and load the language library\r\n","import spacy\r\n","nlp = spacy.load('en_core_web_sm')\r\n","\r\n","# Create a Doc object\r\n","doc = nlp(u'Tesla is looking at buying U.S. startup for $6 million')\r\n","\r\n","# Print each token separately\r\n","for token in doc:\r\n","    print(token.text, token.pos_, token.dep_)\r\n","'''\r\n","Tesla PROPN nsubj\r\n","is VERB aux\r\n","looking VERB ROOT\r\n","at ADP prep\r\n","buying VERB pcomp\r\n","U.S. PROPN compound\r\n","startup NOUN dobj\r\n","for ADP prep\r\n","$ SYM quantmod\r\n","6 NUM compound\r\n","million NUM pobj'''"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"t8mCgXRf30Yx"},"source":["This doesn't look very user-friendly, but right away we see some interesting things happen:\n","1. Tesla is recognized to be a Proper Noun, not just a word at the start of a sentence\n","2. U.S. is kept together as one entity (we call this a 'token')\n","\n","As we dive deeper into spaCy we'll see what each of these abbreviations mean and how they're derived. We'll also see how spaCy can interpret the last three tokens combined `$6 million` as referring to ***money***."]},{"cell_type":"markdown","metadata":{"id":"itj-egVj30Yy"},"source":["___\n","## spaCy Objects\n","\n","After importing the spacy module in the cell above we loaded a **model** and named it `nlp`.<br>Next we created a **Doc** object by applying the model to our text, and named it `doc`.<br>spaCy also builds a companion **Vocab** object that we'll cover in later sections.<br>The **Doc** object that holds the processed text is our focus here."]},{"cell_type":"markdown","metadata":{"id":"EzqKCsOG30Yy"},"source":["___\n","## Pipeline\n","When we run `nlp`, our text enters a *processing pipeline* that first breaks down the text and then performs a series of operations to tag, parse and describe the data.   Image source: https://spacy.io/usage/spacy-101#pipelines"]},{"cell_type":"markdown","metadata":{"id":"cLn4sWSj30Yy"},"source":["<a href='http://www.pieriandata.com'><img src=\"../pipeline1.png\" width=\"600\">"]},{"cell_type":"markdown","metadata":{"id":"XwnNUBJ630Yz"},"source":["We can check to see what components currently live in the pipeline. In later sections we'll learn how to disable components and add new ones as needed."]},{"cell_type":"code","metadata":{"id":"UxLHfRIP30Y0","outputId":"fd05fbfe-3a27-4319-b68c-c71bd40086c5"},"source":["nlp.pipeline"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["[('tagger', <spacy.pipeline.Tagger at 0x237cb1e8f98>),\n"," ('parser', <spacy.pipeline.DependencyParser at 0x237cb2852b0>),\n"," ('ner', <spacy.pipeline.EntityRecognizer at 0x237cb285360>)]"]},"metadata":{"tags":[]},"execution_count":2}]},{"cell_type":"code","metadata":{"id":"e6Igvxa730Y1","outputId":"bb63d771-a2a5-4b81-da69-9bbbb7de03d2"},"source":["nlp.pipe_names"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["['tagger', 'parser', 'ner']"]},"metadata":{"tags":[]},"execution_count":3}]},{"cell_type":"markdown","metadata":{"id":"IU5ZFRSo30Y1"},"source":["___\n","### Tokenization\n","The first step in processing text is to split up all the component parts (words & punctuation) into \"tokens\". These tokens are annotated inside the Doc object to contain descriptive information. We'll go into much more detail on tokenization in an upcoming lecture. For now, let's look at another example:"]},{"cell_type":"code","metadata":{"id":"pU9zuyT530Y2","outputId":"f1fd017a-b2fc-4528-c3f3-15333fe1a5e8"},"source":["doc2 = nlp(u\"Tesla isn't   looking into startups anymore.\")\n","\n","for token in doc2:\n","    print(token.text, token.pos_, token.dep_)"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Tesla PROPN nsubj\n","is VERB aux\n","n't ADV neg\n","   SPACE \n","looking VERB ROOT\n","into ADP prep\n","startups NOUN pobj\n","anymore ADV advmod\n",". PUNCT punct\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"5ylqzm4L30Y3"},"source":["Notice how `isn't` has been split into two tokens. spaCy recognizes both the root verb `is` and the negation attached to it. Notice also that both the extended whitespace and the period at the end of the sentence are assigned their own tokens.\n","\n","It's important to note that even though `doc2` contains processed information about each token, it also retains the original text:"]},{"cell_type":"code","metadata":{"id":"-stbuETQ30Y4","outputId":"d75da0bf-5c26-4181-a01e-79a0b444c3dc"},"source":["doc2"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["Tesla isn't   looking into startups anymore."]},"metadata":{"tags":[]},"execution_count":5}]},{"cell_type":"code","metadata":{"id":"zy4FROoB30Y5","outputId":"d8c0eaf7-c823-4edb-9158-111ffae8815f"},"source":["doc2[0]"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["Tesla"]},"metadata":{"tags":[]},"execution_count":6}]},{"cell_type":"code","metadata":{"id":"UrmofaTk30Y5","outputId":"af2f3010-965a-402a-b95f-188c5b39caac"},"source":["type(doc2)"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["spacy.tokens.doc.Doc"]},"metadata":{"tags":[]},"execution_count":7}]},{"cell_type":"markdown","metadata":{"id":"9vGeKX8W30Y6"},"source":["___\n","### Part-of-Speech Tagging (POS)\n","The next step after splitting the text up into tokens is to assign parts of speech. In the above example, `Tesla` was recognized to be a ***proper noun***. Here some statistical modeling is required. For example, words that follow \"the\" are typically nouns.\n","\n","For a full list of POS Tags visit https://spacy.io/api/annotation#pos-tagging"]},{"cell_type":"code","metadata":{"id":"KhUw-LSi30Y6","outputId":"1b3e0e66-3633-456c-cbeb-39f2ae8cd63f"},"source":["doc2[0].pos_"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["'PROPN'"]},"metadata":{"tags":[]},"execution_count":8}]},{"cell_type":"markdown","metadata":{"id":"4miEiSf530Y6"},"source":["___\n","### Dependencies\n","We also looked at the syntactic dependencies assigned to each token. `Tesla` is identified as an `nsubj` or the ***nominal subject*** of the sentence.\n","\n","For a full list of Syntactic Dependencies visit https://spacy.io/api/annotation#dependency-parsing\n","<br>A good explanation of typed dependencies can be found [here](https://nlp.stanford.edu/software/dependencies_manual.pdf)"]},{"cell_type":"code","metadata":{"id":"REPemKMY30Y7","outputId":"71efb929-8fba-424f-bfc4-27fc4bea0d1e"},"source":["doc2[0].dep_"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["'nsubj'"]},"metadata":{"tags":[]},"execution_count":9}]},{"cell_type":"markdown","metadata":{"id":"whB-RB_H30Y7"},"source":["To see the full name of a tag use `spacy.explain(tag)`"]},{"cell_type":"code","metadata":{"id":"cLkNS4pB30Y8","outputId":"fcee7612-6da1-474a-c24b-f6f747957748"},"source":["spacy.explain('PROPN')"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["'proper noun'"]},"metadata":{"tags":[]},"execution_count":10}]},{"cell_type":"code","metadata":{"id":"c6WRFIOI30Y9","outputId":"57f868c1-6ee2-489c-b7c2-0dd46b42455a"},"source":["spacy.explain('nsubj')"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["'nominal subject'"]},"metadata":{"tags":[]},"execution_count":11}]},{"cell_type":"markdown","metadata":{"id":"svAawAvW30Y9"},"source":["___\n","### Additional Token Attributes\n","We'll see these again in upcoming lectures. For now we just want to illustrate some of the other information that spaCy assigns to tokens:"]},{"cell_type":"markdown","metadata":{"id":"RrvizOP-30Y-"},"source":["|Tag|Description|doc2[0].tag|\n","|:------|:------:|:------|\n","|`.text`|The original word text<!-- .element: style=\"text-align:left;\" -->|`Tesla`|\n","|`.lemma_`|The base form of the word|`tesla`|\n","|`.pos_`|The simple part-of-speech tag|`PROPN`/`proper noun`|\n","|`.tag_`|The detailed part-of-speech tag|`NNP`/`noun, proper singular`|\n","|`.shape_`|The word shape – capitalization, punctuation, digits|`Xxxxx`|\n","|`.is_alpha`|Is the token an alpha character?|`True`|\n","|`.is_stop`|Is the token part of a stop list, i.e. the most common words of the language?|`False`|"]},{"cell_type":"code","metadata":{"id":"dWk-_x2I30Y-","outputId":"15708bc1-8daa-4a6c-db9b-e212a9a1d793"},"source":["# Lemmas (the base form of the word):\n","print(doc2[4].text)\n","print(doc2[4].lemma_)"],"execution_count":null,"outputs":[{"output_type":"stream","text":["looking\n","look\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"JZrdAC6k30Y_","outputId":"c1363337-ca15-4ba7-f6b0-5812cb4d53b3"},"source":["# Simple Parts-of-Speech & Detailed Tags:\n","print(doc2[4].pos_)\n","print(doc2[4].tag_ + ' / ' + spacy.explain(doc2[4].tag_))"],"execution_count":null,"outputs":[{"output_type":"stream","text":["VERB\n","VBG / verb, gerund or present participle\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"OkLjuO9R30Y_","outputId":"6ff20b3d-a765-4ed2-8dbb-15bfa1f2f4b9"},"source":["# Word Shapes:\n","print(doc2[0].text+': '+doc2[0].shape_)\n","print(doc[5].text+' : '+doc[5].shape_)"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Tesla: Xxxxx\n","U.S. : X.X.\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"y6gm-mXU30Y_","outputId":"841e8874-9aeb-4c73-b574-bbc91c449923"},"source":["# Boolean Values:\n","print(doc2[0].is_alpha)\n","print(doc2[0].is_stop)"],"execution_count":null,"outputs":[{"output_type":"stream","text":["True\n","False\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"qiK4gNwP30ZA"},"source":["___\n","### Spans\n","Large Doc objects can be hard to work with at times. A **span** is a slice of Doc object in the form `Doc[start:stop]`."]},{"cell_type":"code","metadata":{"id":"3J7on1w730ZA"},"source":["doc3 = nlp(u'Although commmonly attributed to John Lennon from his song \"Beautiful Boy\", \\\n","the phrase \"Life is what happens to us while we are making other plans\" was written by \\\n","cartoonist Allen Saunders and published in Reader\\'s Digest in 1957, when Lennon was 17.')"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"88bY0GQU30ZA","outputId":"28acb9e3-c106-44c6-c140-f23e1c979b1f"},"source":["life_quote = doc3[16:30]\n","print(life_quote)"],"execution_count":null,"outputs":[{"output_type":"stream","text":["\"Life is what happens to us while we are making other plans\"\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"Ocjgu_i730ZB","outputId":"3d231980-0557-4c49-e135-76ba2661df7b"},"source":["type(life_quote)"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["spacy.tokens.span.Span"]},"metadata":{"tags":[]},"execution_count":18}]},{"cell_type":"markdown","metadata":{"id":"lo15mvpV30ZB"},"source":["In upcoming lectures we'll see how to create Span objects using `Span()`. This will allow us to assign additional information to the Span."]},{"cell_type":"markdown","metadata":{"id":"hurv1Vha30ZB"},"source":["___\n","### Sentences\n","Certain tokens inside a Doc object may also receive a \"start of sentence\" tag. While this doesn't immediately build a list of sentences, these tags enable the generation of sentence segments through `Doc.sents`. Later we'll write our own segmentation rules."]},{"cell_type":"code","metadata":{"id":"2Uyz1fWx30ZC"},"source":["doc4 = nlp(u'This is the first sentence. This is another sentence. This is the last sentence.')"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"FoA4hmwu30ZC","outputId":"aab50385-bd62-4629-b301-cbaa69d278a6"},"source":["for sent in doc4.sents:\n","    print(sent)"],"execution_count":null,"outputs":[{"output_type":"stream","text":["This is the first sentence.\n","This is another sentence.\n","This is the last sentence.\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"rbuc4wBX30ZC","outputId":"664f27e0-4bdc-46c3-b872-24a09b65ccdc"},"source":["doc4[6].is_sent_start"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["True"]},"metadata":{"tags":[]},"execution_count":21}]},{"cell_type":"code","metadata":{"id":"6L50SjUWEwC5"},"source":[""],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"g20iaNRnHq6M"},"source":[""]},{"cell_type":"markdown","metadata":{"id":"4MTPSl1NHnj9"},"source":["## Tokenization\n","The first step in creating a `Doc` object is to break down the incoming text into component pieces or \"tokens\"."]},{"cell_type":"code","metadata":{"id":"vm35Kr2mHnkA"},"source":["# Import spaCy and load the language library\n","import spacy\n","nlp = spacy.load('en_core_web_sm')"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"54K8IEmdHnkG","outputId":"062ec2a2-953a-46b9-fe23-2b614067c8cc"},"source":["# Create a string that includes opening and closing quotation marks\n","mystring = '\"We\\'re moving to L.A.!\"'\n","print(mystring)"],"execution_count":null,"outputs":[{"output_type":"stream","text":["\"We're moving to L.A.!\"\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"KR0fb4myHnkJ","outputId":"d1d315e3-d1d9-4688-8b94-81407a4e6f4d"},"source":["# Create a Doc object and explore tokens\n","doc = nlp(mystring)\n","\n","for token in doc:\n","    print(token.text, end=' | ')"],"execution_count":null,"outputs":[{"output_type":"stream","text":["\" | We | 're | moving | to | L.A. | ! | \" | "],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"Gd4djQxBHnkt"},"source":["<img src=\"../tokenization.png\" width=\"600\">"]},{"cell_type":"markdown","metadata":{"id":"40jlD6NaHnku"},"source":["-  **Prefix**:\tCharacter(s) at the beginning &#9656; `$ ( “ ¿`\n","-  **Suffix**:\tCharacter(s) at the end &#9656; `km ) , . ! ”`\n","-  **Infix**:\tCharacter(s) in between &#9656; `- -- / ...`\n","-  **Exception**: Special-case rule to split a string into several tokens or prevent a token from being split when punctuation rules are applied &#9656; `St. U.S.`"]},{"cell_type":"markdown","metadata":{"id":"f5cQDF2kHnkv"},"source":["Notice that tokens are pieces of the original text. That is, we don't see any conversion to word stems or lemmas (base forms of words) and we haven't seen anything about organizations/places/money etc. Tokens are the basic building blocks of a Doc object - everything that helps us understand the meaning of the text is derived from tokens and their relationship to one another."]},{"cell_type":"markdown","metadata":{"id":"raVSam-7Hnkw"},"source":["### Prefixes, Suffixes and Infixes\n","spaCy will isolate punctuation that does *not* form an integral part of a word. Quotation marks, commas, and punctuation at the end of a sentence will be assigned their own token. However, punctuation that exists as part of an email address, website or numerical value will be kept as part of the token."]},{"cell_type":"code","metadata":{"id":"fg1fu02tHnky","outputId":"aaf4ba3d-503a-4747-a9b6-86e022bcafdd"},"source":["doc2 = nlp(u\"We're here to help! Send snail-mail, email support@oursite.com or visit us at http://www.oursite.com!\")\n","\n","for t in doc2:\n","    print(t)"],"execution_count":null,"outputs":[{"output_type":"stream","text":["We\n","'re\n","here\n","to\n","help\n","!\n","Send\n","snail\n","-\n","mail\n",",\n","email\n","support@oursite.com\n","or\n","visit\n","us\n","at\n","http://www.oursite.com\n","!\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"TTWrNF58Hnk4"},"source":["<font color=green>Note that the exclamation points, comma, and the hyphen in 'snail-mail' are assigned their own tokens, yet both the email address and website are preserved.</font>"]},{"cell_type":"code","metadata":{"id":"ujhDgkAzHnk5","outputId":"a88919e0-008b-4661-e368-77b94b000103"},"source":["doc3 = nlp(u'A 5km NYC cab ride costs $10.30')\n","\n","for t in doc3:\n","    print(t)"],"execution_count":null,"outputs":[{"output_type":"stream","text":["A\n","5\n","km\n","NYC\n","cab\n","ride\n","costs\n","$\n","10.30\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"KIFbIup4Hnk6"},"source":["<font color=green>Here the distance unit and dollar sign are assigned their own tokens, yet the dollar amount is preserved.</font>"]},{"cell_type":"markdown","metadata":{"id":"rEQMpOElHnk6"},"source":["### Exceptions\n","Punctuation that exists as part of a known abbreviation will be kept as part of the token."]},{"cell_type":"code","metadata":{"id":"WG154-_vHnk7","outputId":"d4ba0071-8d5d-48b1-fb55-028ecc6bfc1a"},"source":["doc4 = nlp(u\"Let's visit St. Louis in the U.S. next year.\")\n","\n","for t in doc4:\n","    print(t)"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Let\n","'s\n","visit\n","St.\n","Louis\n","in\n","the\n","U.S.\n","next\n","year\n",".\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"7_eWPHpGHnk8"},"source":["<font color=green>Here the abbreviations for \"Saint\" and \"United States\" are both preserved.</font>"]},{"cell_type":"markdown","metadata":{"id":"5P8FBM_mHnk8"},"source":["### Counting Tokens\n","`Doc` objects have a set number of tokens:"]},{"cell_type":"code","metadata":{"id":"cChbO3kcHnk9","outputId":"e96f3f97-d4e6-40e3-e3e1-840747be6746"},"source":["len(doc)"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["8"]},"metadata":{"tags":[]},"execution_count":7}]},{"cell_type":"markdown","metadata":{"id":"v6ddKwfDHnlT"},"source":["### Counting Vocab Entries\n","`Vocab` objects contain a full library of items!"]},{"cell_type":"code","metadata":{"id":"wWp0daz5HnlW","outputId":"33768c4f-2608-4e78-96a1-97a3f3aa9df5"},"source":["len(doc.vocab)"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["57852"]},"metadata":{"tags":[]},"execution_count":8}]},{"cell_type":"markdown","metadata":{"id":"Rik09P8DHnlY"},"source":["<font color=green>NOTE: This number changes based on the language library loaded at the start, and any new lexemes introduced to the `vocab` when the `Doc` was created.</font>"]},{"cell_type":"markdown","metadata":{"id":"zfWyGuVQHnlZ"},"source":["### Tokens can be retrieved by index position and slice\n","`Doc` objects can be thought of as lists of `token` objects. As such, individual tokens can be retrieved by index position, and spans of tokens can be retrieved through slicing:"]},{"cell_type":"code","metadata":{"id":"ydh1f_SoHnlc","outputId":"f085ac1e-a210-466d-fc81-04d227697541"},"source":["doc5 = nlp(u'It is better to give than to receive.')\n","\n","# Retrieve the third token:\n","doc5[2]"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["better"]},"metadata":{"tags":[]},"execution_count":9}]},{"cell_type":"code","metadata":{"id":"KLrIkek7Hnld","outputId":"97b38d06-b4f9-49f9-8033-0fc24a01b6a0"},"source":["# Retrieve three tokens from the middle:\n","doc5[2:5]"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["better to give"]},"metadata":{"tags":[]},"execution_count":10}]},{"cell_type":"code","metadata":{"id":"uM0m5sdFHnld","outputId":"33eb1003-90a2-477b-90c5-21257292dee9"},"source":["# Retrieve the last four tokens:\n","doc5[-4:]"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["than to receive."]},"metadata":{"tags":[]},"execution_count":11}]},{"cell_type":"markdown","metadata":{"id":"Pcg7WJV9Hnle"},"source":["### Tokens cannot be reassigned\n","Although `Doc` objects can be considered lists of tokens, they do *not* support item reassignment:"]},{"cell_type":"code","metadata":{"id":"CccHJ97lHnlf"},"source":["doc6 = nlp(u'My dinner was horrible.')\n","doc7 = nlp(u'Your dinner was delicious.')"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"4OIjYFhpHnlf","outputId":"916afab2-d19b-43d1-8dd8-7d98e1f93aa7"},"source":["# Try to change \"My dinner was horrible\" to \"My dinner was delicious\"\n","doc6[3] = doc7[3]"],"execution_count":null,"outputs":[{"output_type":"error","ename":"TypeError","evalue":"'spacy.tokens.doc.Doc' object does not support item assignment","traceback":["\u001b[1;31m---------------------------------------------------------------------------\u001b[0m","\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)","\u001b[1;32m<ipython-input-13-d4fb8c39c40b>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;31m# Try to change \"My dinner was horrible\" to \"My dinner was delicious\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0mdoc6\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m3\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdoc7\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m3\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m","\u001b[1;31mTypeError\u001b[0m: 'spacy.tokens.doc.Doc' object does not support item assignment"]}]},{"cell_type":"markdown","metadata":{"collapsed":true,"id":"3ZGStwNQHnlg"},"source":["___\n","## Named Entities\n","Going a step beyond tokens, *named entities* add another layer of context. The language model recognizes that certain words are organizational names while others are locations, and still other combinations relate to money, dates, etc. Named entities are accessible through the `ents` property of a `Doc` object."]},{"cell_type":"code","metadata":{"id":"G4Oef6j0Hnlh","outputId":"9e7ca8d3-6ecd-4feb-8671-0bf2497b4583"},"source":["doc8 = nlp(u'Apple to build a Hong Kong factory for $6 million')\n","\n","for token in doc8:\n","    print(token.text, end=' | ')\n","\n","print('\\n----')\n","\n","for ent in doc8.ents:\n","    print(ent.text+' - '+ent.label_+' - '+str(spacy.explain(ent.label_)))"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Apple | to | build | a | Hong | Kong | factory | for | $ | 6 | million | \n","----\n","Apple - ORG - Companies, agencies, institutions, etc.\n","Hong Kong - GPE - Countries, cities, states\n","$6 million - MONEY - Monetary values, including unit\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"LXP1GuxVHnlh"},"source":["<font color=green>Note how two tokens combine to form the entity `Hong Kong`, and three tokens combine to form the monetary entity:  `$6 million`</font>"]},{"cell_type":"code","metadata":{"id":"tR8jqK3jHnli","outputId":"5a090f78-38a9-47bb-9310-eff0c09bb79f"},"source":["len(doc8.ents)"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["3"]},"metadata":{"tags":[]},"execution_count":15}]},{"cell_type":"markdown","metadata":{"id":"LQcbfqI6Hnli"},"source":["Named Entity Recognition (NER) is an important machine learning tool applied to Natural Language Processing.<br>We'll do a lot more with it in an upcoming section. For more info on **named entities** visit https://spacy.io/usage/linguistic-features#named-entities"]},{"cell_type":"markdown","metadata":{"id":"sekLmBcCHnlj"},"source":["---\n","## Noun Chunks\n","Similar to `Doc.ents`, `Doc.noun_chunks` are another object property. *Noun chunks* are \"base noun phrases\" – flat phrases that have a noun as their head. You can think of noun chunks as a noun plus the words describing the noun – for example, in [Sheb Wooley's 1958 song](https://en.wikipedia.org/wiki/The_Purple_People_Eater), a *\"one-eyed, one-horned, flying, purple people-eater\"* would be one long noun chunk."]},{"cell_type":"code","metadata":{"id":"7HB1NLPTHnlk","outputId":"e31ca808-15c9-43d7-a5f2-bccecfac1ebc"},"source":["doc9 = nlp(u\"Autonomous cars shift insurance liability toward manufacturers.\")\n","\n","for chunk in doc9.noun_chunks:\n","    print(chunk.text)"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Autonomous cars\n","insurance liability\n","manufacturers\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"loaTPxw0Hnll","outputId":"b0b8b097-aa20-4114-9b0f-8743e714f94d"},"source":["doc10 = nlp(u\"Red cars do not carry higher insurance rates.\")\n","\n","for chunk in doc10.noun_chunks:\n","    print(chunk.text)"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Red cars\n","higher insurance rates\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"L1zBuIvEHnlm","outputId":"a7316936-7c9a-4637-a01b-34fe6d95c1be"},"source":["doc11 = nlp(u\"He was a one-eyed, one-horned, flying, purple people-eater.\")\n","\n","for chunk in doc11.noun_chunks:\n","    print(chunk.text)"],"execution_count":null,"outputs":[{"output_type":"stream","text":["He\n","a one-eyed, one-horned, flying, purple people-eater\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"nAMVL7LmHnlq"},"source":["We'll look at additional noun_chunks components besides `.text` in an upcoming section.<br>For more info on **noun_chunks** visit https://spacy.io/usage/linguistic-features#noun-chunks"]},{"cell_type":"markdown","metadata":{"id":"Z1vfPjMAHnlr"},"source":["___\n","## Built-in Visualizers\n","\n","spaCy includes a built-in visualization tool called **displaCy**. displaCy is able to detect whether you're working in a Jupyter notebook, and will return markup that can be rendered in a cell right away. When you export your notebook, the visualizations will be included as HTML.\n","\n","For more info visit https://spacy.io/usage/visualizers"]},{"cell_type":"markdown","metadata":{"id":"uzgSvOaXHnlt"},"source":["### Visualizing the dependency parse\n","Run the cell below to import displacy and display the dependency graphic"]},{"cell_type":"code","metadata":{"id":"ss-EQ-xNHnlz","outputId":"94d49f57-28d9-4a3d-9ed9-b612e2045334"},"source":["from spacy import displacy\n","\n","doc = nlp(u'Apple is going to build a U.K. factory for $6 million.')\n","displacy.render(doc, style='dep', jupyter=True, options={'distance': 110})"],"execution_count":null,"outputs":[{"output_type":"display_data","data":{"text/html":["<svg xmlns=\"http://www.w3.org/2000/svg\" xmlns:xlink=\"http://www.w3.org/1999/xlink\" id=\"0\" class=\"displacy\" width=\"1370\" height=\"357.0\" style=\"max-width: none; height: 357.0px; color: #000000; background: #ffffff; font-family: Arial\">\n","<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"267.0\">\n","    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"50\">Apple</tspan>\n","    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"50\">PROPN</tspan>\n","</text>\n","\n","<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"267.0\">\n","    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"160\">is</tspan>\n","    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"160\">VERB</tspan>\n","</text>\n","\n","<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"267.0\">\n","    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"270\">going</tspan>\n","    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"270\">VERB</tspan>\n","</text>\n","\n","<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"267.0\">\n","    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"380\">to</tspan>\n","    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"380\">PART</tspan>\n","</text>\n","\n","<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"267.0\">\n","    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"490\">build</tspan>\n","    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"490\">VERB</tspan>\n","</text>\n","\n","<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"267.0\">\n","    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"600\">a</tspan>\n","    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"600\">DET</tspan>\n","</text>\n","\n","<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"267.0\">\n","    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"710\">U.K.</tspan>\n","    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"710\">PROPN</tspan>\n","</text>\n","\n","<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"267.0\">\n","    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"820\">factory</tspan>\n","    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"820\">NOUN</tspan>\n","</text>\n","\n","<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"267.0\">\n","    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"930\">for</tspan>\n","    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"930\">ADP</tspan>\n","</text>\n","\n","<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"267.0\">\n","    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"1040\">$</tspan>\n","    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"1040\">SYM</tspan>\n","</text>\n","\n","<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"267.0\">\n","    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"1150\">6</tspan>\n","    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"1150\">NUM</tspan>\n","</text>\n","\n","<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"267.0\">\n","    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"1260\">million.</tspan>\n","    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"1260\">NUM</tspan>\n","</text>\n","\n","<g class=\"displacy-arrow\">\n","    <path class=\"displacy-arc\" id=\"arrow-0-0\" stroke-width=\"2px\" d=\"M70,222.0 C70,112.0 260.0,112.0 260.0,222.0\" fill=\"none\" stroke=\"currentColor\"/>\n","    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n","        <textPath xlink:href=\"#arrow-0-0\" class=\"displacy-label\" startOffset=\"50%\" fill=\"currentColor\" text-anchor=\"middle\">nsubj</textPath>\n","    </text>\n","    <path class=\"displacy-arrowhead\" d=\"M70,224.0 L62,212.0 78,212.0\" fill=\"currentColor\"/>\n","</g>\n","\n","<g class=\"displacy-arrow\">\n","    <path class=\"displacy-arc\" id=\"arrow-0-1\" stroke-width=\"2px\" d=\"M180,222.0 C180,167.0 255.0,167.0 255.0,222.0\" fill=\"none\" stroke=\"currentColor\"/>\n","    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n","        <textPath xlink:href=\"#arrow-0-1\" class=\"displacy-label\" startOffset=\"50%\" fill=\"currentColor\" text-anchor=\"middle\">aux</textPath>\n","    </text>\n","    <path class=\"displacy-arrowhead\" d=\"M180,224.0 L172,212.0 188,212.0\" fill=\"currentColor\"/>\n","</g>\n","\n","<g class=\"displacy-arrow\">\n","    <path class=\"displacy-arc\" id=\"arrow-0-2\" stroke-width=\"2px\" d=\"M400,222.0 C400,167.0 475.0,167.0 475.0,222.0\" fill=\"none\" stroke=\"currentColor\"/>\n","    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n","        <textPath xlink:href=\"#arrow-0-2\" class=\"displacy-label\" startOffset=\"50%\" fill=\"currentColor\" text-anchor=\"middle\">aux</textPath>\n","    </text>\n","    <path class=\"displacy-arrowhead\" d=\"M400,224.0 L392,212.0 408,212.0\" fill=\"currentColor\"/>\n","</g>\n","\n","<g class=\"displacy-arrow\">\n","    <path class=\"displacy-arc\" id=\"arrow-0-3\" stroke-width=\"2px\" d=\"M290,222.0 C290,112.0 480.0,112.0 480.0,222.0\" fill=\"none\" stroke=\"currentColor\"/>\n","    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n","        <textPath xlink:href=\"#arrow-0-3\" class=\"displacy-label\" startOffset=\"50%\" fill=\"currentColor\" text-anchor=\"middle\">xcomp</textPath>\n","    </text>\n","    <path class=\"displacy-arrowhead\" d=\"M480.0,224.0 L488.0,212.0 472.0,212.0\" fill=\"currentColor\"/>\n","</g>\n","\n","<g class=\"displacy-arrow\">\n","    <path class=\"displacy-arc\" id=\"arrow-0-4\" stroke-width=\"2px\" d=\"M620,222.0 C620,112.0 810.0,112.0 810.0,222.0\" fill=\"none\" stroke=\"currentColor\"/>\n","    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n","        <textPath xlink:href=\"#arrow-0-4\" class=\"displacy-label\" startOffset=\"50%\" fill=\"currentColor\" text-anchor=\"middle\">det</textPath>\n","    </text>\n","    <path class=\"displacy-arrowhead\" d=\"M620,224.0 L612,212.0 628,212.0\" fill=\"currentColor\"/>\n","</g>\n","\n","<g class=\"displacy-arrow\">\n","    <path class=\"displacy-arc\" id=\"arrow-0-5\" stroke-width=\"2px\" d=\"M730,222.0 C730,167.0 805.0,167.0 805.0,222.0\" fill=\"none\" stroke=\"currentColor\"/>\n","    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n","        <textPath xlink:href=\"#arrow-0-5\" class=\"displacy-label\" startOffset=\"50%\" fill=\"currentColor\" text-anchor=\"middle\">compound</textPath>\n","    </text>\n","    <path class=\"displacy-arrowhead\" d=\"M730,224.0 L722,212.0 738,212.0\" fill=\"currentColor\"/>\n","</g>\n","\n","<g class=\"displacy-arrow\">\n","    <path class=\"displacy-arc\" id=\"arrow-0-6\" stroke-width=\"2px\" d=\"M510,222.0 C510,57.0 815.0,57.0 815.0,222.0\" fill=\"none\" stroke=\"currentColor\"/>\n","    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n","        <textPath xlink:href=\"#arrow-0-6\" class=\"displacy-label\" startOffset=\"50%\" fill=\"currentColor\" text-anchor=\"middle\">dobj</textPath>\n","    </text>\n","    <path class=\"displacy-arrowhead\" d=\"M815.0,224.0 L823.0,212.0 807.0,212.0\" fill=\"currentColor\"/>\n","</g>\n","\n","<g class=\"displacy-arrow\">\n","    <path class=\"displacy-arc\" id=\"arrow-0-7\" stroke-width=\"2px\" d=\"M510,222.0 C510,2.0 930.0,2.0 930.0,222.0\" fill=\"none\" stroke=\"currentColor\"/>\n","    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n","        <textPath xlink:href=\"#arrow-0-7\" class=\"displacy-label\" startOffset=\"50%\" fill=\"currentColor\" text-anchor=\"middle\">prep</textPath>\n","    </text>\n","    <path class=\"displacy-arrowhead\" d=\"M930.0,224.0 L938.0,212.0 922.0,212.0\" fill=\"currentColor\"/>\n","</g>\n","\n","<g class=\"displacy-arrow\">\n","    <path class=\"displacy-arc\" id=\"arrow-0-8\" stroke-width=\"2px\" d=\"M1060,222.0 C1060,112.0 1250.0,112.0 1250.0,222.0\" fill=\"none\" stroke=\"currentColor\"/>\n","    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n","        <textPath xlink:href=\"#arrow-0-8\" class=\"displacy-label\" startOffset=\"50%\" fill=\"currentColor\" text-anchor=\"middle\">quantmod</textPath>\n","    </text>\n","    <path class=\"displacy-arrowhead\" d=\"M1060,224.0 L1052,212.0 1068,212.0\" fill=\"currentColor\"/>\n","</g>\n","\n","<g class=\"displacy-arrow\">\n","    <path class=\"displacy-arc\" id=\"arrow-0-9\" stroke-width=\"2px\" d=\"M1170,222.0 C1170,167.0 1245.0,167.0 1245.0,222.0\" fill=\"none\" stroke=\"currentColor\"/>\n","    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n","        <textPath xlink:href=\"#arrow-0-9\" class=\"displacy-label\" startOffset=\"50%\" fill=\"currentColor\" text-anchor=\"middle\">compound</textPath>\n","    </text>\n","    <path class=\"displacy-arrowhead\" d=\"M1170,224.0 L1162,212.0 1178,212.0\" fill=\"currentColor\"/>\n","</g>\n","\n","<g class=\"displacy-arrow\">\n","    <path class=\"displacy-arc\" id=\"arrow-0-10\" stroke-width=\"2px\" d=\"M950,222.0 C950,57.0 1255.0,57.0 1255.0,222.0\" fill=\"none\" stroke=\"currentColor\"/>\n","    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n","        <textPath xlink:href=\"#arrow-0-10\" class=\"displacy-label\" startOffset=\"50%\" fill=\"currentColor\" text-anchor=\"middle\">pobj</textPath>\n","    </text>\n","    <path class=\"displacy-arrowhead\" d=\"M1255.0,224.0 L1263.0,212.0 1247.0,212.0\" fill=\"currentColor\"/>\n","</g>\n","</svg>"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{"tags":[]}}]},{"cell_type":"markdown","metadata":{"id":"OmVjdWyfHnl0"},"source":["The optional `'distance'` argument sets the distance between tokens. If the distance is made too small, text that appears beneath short arrows may become too compressed to read."]},{"cell_type":"markdown","metadata":{"id":"Rnleb95iHnl1"},"source":["### Visualizing the entity recognizer"]},{"cell_type":"code","metadata":{"id":"EL5JQDGWHnl2","outputId":"1b5dca1a-4201-45f9-fd9c-0ff01bebead9"},"source":["doc = nlp(u'Over the last quarter Apple sold nearly 20 thousand iPods for a profit of $6 million.')\n","displacy.render(doc, style='ent', jupyter=True)"],"execution_count":null,"outputs":[{"output_type":"display_data","data":{"text/html":["<div class=\"entities\" style=\"line-height: 2.5\">Over \n","<mark class=\"entity\" style=\"background: #bfe1d9; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em; box-decoration-break: clone; -webkit-box-decoration-break: clone\">\n","    the last quarter\n","    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; text-transform: uppercase; vertical-align: middle; margin-left: 0.5rem\">DATE</span>\n","</mark>\n"," \n","<mark class=\"entity\" style=\"background: #7aecec; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em; box-decoration-break: clone; -webkit-box-decoration-break: clone\">\n","    Apple\n","    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; text-transform: uppercase; vertical-align: middle; margin-left: 0.5rem\">ORG</span>\n","</mark>\n"," sold \n","<mark class=\"entity\" style=\"background: #e4e7d2; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em; box-decoration-break: clone; -webkit-box-decoration-break: clone\">\n","    nearly 20 thousand\n","    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; text-transform: uppercase; vertical-align: middle; margin-left: 0.5rem\">CARDINAL</span>\n","</mark>\n"," \n","<mark class=\"entity\" style=\"background: #bfeeb7; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em; box-decoration-break: clone; -webkit-box-decoration-break: clone\">\n","    iPods\n","    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; text-transform: uppercase; vertical-align: middle; margin-left: 0.5rem\">PRODUCT</span>\n","</mark>\n"," for a profit of \n","<mark class=\"entity\" style=\"background: #e4e7d2; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em; box-decoration-break: clone; -webkit-box-decoration-break: clone\">\n","    $6 million\n","    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; text-transform: uppercase; vertical-align: middle; margin-left: 0.5rem\">MONEY</span>\n","</mark>\n",".</div>"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{"tags":[]}}]},{"cell_type":"markdown","metadata":{"id":"_6IH-SUCHnl3"},"source":["___\n","### Creating Visualizations Outside of Jupyter\n","If you're using another Python IDE or writing a script, you can choose to have spaCy serve up html separately:"]},{"cell_type":"code","metadata":{"id":"2Xj70R38Hnl4","outputId":"a1414e44-ec34-4c3e-f991-07fb0485724e"},"source":["doc = nlp(u'This is a sentence.')\n","displacy.serve(doc, style='dep')"],"execution_count":null,"outputs":[{"output_type":"stream","text":["\n","    Serving on port 5000...\n","    Using the 'dep' visualizer\n","\n","\n","    Shutting down server on port 5000.\n","\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"DTxPJj1gHnl8"},"source":["<font color=blue>**After running the cell above, click the link below to view the dependency parse**:</font>\n","\n","http://127.0.0.1:5000\n","<br><br>\n","<font color=red>**To shut down the server and return to jupyter**, interrupt the kernel either through the **Kernel** menu above, by hitting the black square on the toolbar, or by typing the keyboard shortcut `Esc`, `I`, `I`</font>"]},{"cell_type":"markdown","metadata":{"id":"ZL8usazWHnl9"},"source":["Great! Now you should have an understanding of how tokenization divides text up into individual elements, how named entities provide context, and how certain tools help to visualize grammar rules and entity labels.\n","### Next up: Stemming"]},{"cell_type":"markdown","metadata":{"id":"9Oe6IBRlIM7z"},"source":["## Stemming\n","Often when searching text for a certain keyword, it helps if the search returns variations of the word. For instance, searching for \"boat\" might also return \"boats\" and \"boating\". Here, \"boat\" would be the **stem** for [boat, boater, boating, boats].\n","\n","Stemming is a somewhat crude method for cataloging related words; it essentially chops off letters from the end until the stem is reached. This works fairly well in most cases, but unfortunately English has many exceptions where a more sophisticated process is required. In fact, spaCy doesn't include a stemmer, opting instead to rely entirely on lemmatization. For those interested, there's some background on this decision [here](https://github.com/explosion/spaCy/issues/327). We discuss the virtues of *lemmatization* in the next section.\n","\n","Instead, we'll use another popular NLP tool called **nltk**, which stands for *Natural Language Toolkit*. For more information on nltk visit https://www.nltk.org/"]},{"cell_type":"markdown","metadata":{"id":"U-0tzGENIM71"},"source":["### Porter Stemmer\n","\n","One of the most common - and effective - stemming tools is [*Porter's Algorithm*](https://tartarus.org/martin/PorterStemmer/) developed by Martin Porter in [1980](https://tartarus.org/martin/PorterStemmer/def.txt). The algorithm employs five phases of word reduction, each with its own set of mapping rules. In the first phase, simple suffix mapping rules are defined, such as:"]},{"cell_type":"markdown","metadata":{"id":"CKBmr3orIM72"},"source":["![stemming1.png](../stemming1.png)"]},{"cell_type":"markdown","metadata":{"id":"1TXQjn-yIM73"},"source":["From a given set of stemming rules only one rule is applied, based on the longest suffix S1. Thus, `caresses` reduces to `caress` but not `cares`.\n","\n","More sophisticated phases consider the length/complexity of the word before applying a rule. For example:"]},{"cell_type":"markdown","metadata":{"id":"wsz8rdlSIM74"},"source":["![stemming1.png](../stemming2.png)"]},{"cell_type":"markdown","metadata":{"id":"fe76Md2fIM74"},"source":["Here `m>0` describes the \"measure\" of the stem, such that the rule is applied to all but the most basic stems."]},{"cell_type":"code","metadata":{"id":"GZZZKWuEIM75"},"source":["# Import the toolkit and the full Porter Stemmer library\n","import nltk\n","\n","from nltk.stem.porter import *"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"5sL3PoBBIM77"},"source":["p_stemmer = PorterStemmer()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"Fn_ldacZIM77"},"source":["words = ['run','runner','running','ran','runs','easily','fairly']"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"RF9wHaJGIM78","outputId":"f2e3a81a-d00f-4887-ba3e-78fb5519c0ea"},"source":["for word in words:\n","    print(word+' --> '+p_stemmer.stem(word))"],"execution_count":null,"outputs":[{"output_type":"stream","text":["run --> run\n","runner --> runner\n","running --> run\n","ran --> ran\n","runs --> run\n","easily --> easili\n","fairly --> fairli\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"VGmo8iysIM7-"},"source":["<font color=green>Note how the stemmer recognizes \"runner\" as a noun, not a verb form or participle. Also, the adverbs \"easily\" and \"fairly\" are stemmed to the unusual root \"easili\" and \"fairli\"</font>\n","___"]},{"cell_type":"markdown","metadata":{"id":"eQII4zLAIM7_"},"source":["### Snowball Stemmer\n","This is somewhat of a misnomer, as Snowball is the name of a stemming language developed by Martin Porter. The algorithm used here is more acurately called the \"English Stemmer\" or \"Porter2 Stemmer\". It offers a slight improvement over the original Porter stemmer, both in logic and speed. Since **nltk** uses the name SnowballStemmer, we'll use it here."]},{"cell_type":"code","metadata":{"id":"q-mBMmu9IM8A"},"source":["from nltk.stem.snowball import SnowballStemmer\n","\n","# The Snowball Stemmer requires that you pass a language parameter\n","s_stemmer = SnowballStemmer(language='english')"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"VEPOfhLBIM8D"},"source":["words = ['run','runner','running','ran','runs','easily','fairly']\n","# words = ['generous','generation','generously','generate']"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"eYJMxIE4IM8D","outputId":"3c108371-2cd4-4b58-8be6-3d35bcfa6f48"},"source":["for word in words:\n","    print(word+' --> '+s_stemmer.stem(word))"],"execution_count":null,"outputs":[{"output_type":"stream","text":["run --> run\n","runner --> runner\n","running --> run\n","ran --> ran\n","runs --> run\n","easily --> easili\n","fairly --> fair\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"collapsed":true,"id":"IYTTo3ClIM8E"},"source":["<font color=green>In this case the stemmer performed the same as the Porter Stemmer, with the exception that it handled the stem of \"fairly\" more appropriately with \"fair\"</font>\n","___"]},{"cell_type":"markdown","metadata":{"id":"ZXS1qo3lIM8E"},"source":["### Try it yourself!\n","##### Pass in some of your own words and test each stemmer on them. Remember to pass them as strings!"]},{"cell_type":"code","metadata":{"id":"lWNng6J9IM8E"},"source":["words = ['consolingly']"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"Q7ZsiC3lIM8F","outputId":"3a757654-0759-4e1f-c4cd-02f12039cd4e"},"source":["print('Porter Stemmer:')\n","for word in words:\n","    print(word+' --> '+p_stemmer.stem(word))"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Porter Stemmer:\n","consolingly --> consolingli\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"BoBTbbP0IM8F","outputId":"96824580-1af2-4713-ee72-f09f48ab3650"},"source":["print('Porter2 Stemmer:')\n","for word in words:\n","    print(word+' --> '+s_stemmer.stem(word))"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Porter2 Stemmer:\n","consolingly --> consol\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"FCxUT9CHIM8F"},"source":["___\n","Stemming has its drawbacks. If given the token `saw`, stemming might always return `saw`, whereas lemmatization would likely return either `see` or `saw` depending on whether the use of the token was as a verb or a noun. As an example, consider the following:"]},{"cell_type":"code","metadata":{"id":"d7dyQwvLIM8G","outputId":"27cb910f-92b2-4616-d530-a94f0eebdb6c"},"source":["phrase = 'I am meeting him tomorrow at the meeting'\n","for word in phrase.split():\n","    print(word+' --> '+p_stemmer.stem(word))"],"execution_count":null,"outputs":[{"output_type":"stream","text":["I --> I\n","am --> am\n","meeting --> meet\n","him --> him\n","tomorrow --> tomorrow\n","at --> at\n","the --> the\n","meeting --> meet\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"Ia6qRMLMIM8G"},"source":["Here the word \"meeting\" appears twice - once as a verb, and once as a noun, and yet the stemmer treats both equally."]},{"cell_type":"markdown","metadata":{"id":"2rthUV-tJYe8"},"source":["## Lemmatization\n","In contrast to stemming, lemmatization looks beyond word reduction, and considers a language's full vocabulary to apply a *morphological analysis* to words. The lemma of 'was' is 'be' and the lemma of 'mice' is 'mouse'. Further, the lemma of 'meeting' might be 'meet' or 'meeting' depending on its use in a sentence."]},{"cell_type":"code","metadata":{"id":"ANylt3F6JYe_"},"source":["# Perform standard imports:\n","import spacy\n","nlp = spacy.load('en_core_web_sm')"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"_qE5qrFnJYfB","outputId":"04dad5e3-608b-4520-e2a8-e1658865bba3"},"source":["doc1 = nlp(u\"I am a runner running in a race because I love to run since I ran today\")\n","\n","for token in doc1:\n","    print(token.text, '\\t', token.pos_, '\\t', token.lemma, '\\t', token.lemma_)"],"execution_count":null,"outputs":[{"output_type":"stream","text":["I \t PRON \t 561228191312463089 \t -PRON-\n","am \t VERB \t 10382539506755952630 \t be\n","a \t DET \t 11901859001352538922 \t a\n","runner \t NOUN \t 12640964157389618806 \t runner\n","running \t VERB \t 12767647472892411841 \t run\n","in \t ADP \t 3002984154512732771 \t in\n","a \t DET \t 11901859001352538922 \t a\n","race \t NOUN \t 8048469955494714898 \t race\n","because \t ADP \t 16950148841647037698 \t because\n","I \t PRON \t 561228191312463089 \t -PRON-\n","love \t VERB \t 3702023516439754181 \t love\n","to \t PART \t 3791531372978436496 \t to\n","run \t VERB \t 12767647472892411841 \t run\n","since \t ADP \t 10066841407251338481 \t since\n","I \t PRON \t 561228191312463089 \t -PRON-\n","ran \t VERB \t 12767647472892411841 \t run\n","today \t NOUN \t 11042482332948150395 \t today\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"gKHIhoBVJYfE"},"source":["<font color=green>In the above sentence, `running`, `run` and `ran` all point to the same lemma `run` (...11841) to avoid duplication.</font>"]},{"cell_type":"markdown","metadata":{"id":"Yhb6KlKuJYfF"},"source":["#### Function to display lemmas\n","Since the display above is staggared and hard to read, let's write a function that displays the information we want more neatly."]},{"cell_type":"code","metadata":{"id":"WKXi8aBKJYfG"},"source":["def show_lemmas(text):\n","    for token in text:\n","        print(f'{token.text:{12}} {token.pos_:{6}} {token.lemma:<{22}} {token.lemma_}')"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"d3y3smKwJYfG"},"source":["Here we're using an **f-string** to format the printed text by setting minimum field widths and adding a left-align to the lemma hash value."]},{"cell_type":"code","metadata":{"id":"i8n0lYxhJYfH","outputId":"dc2fe7ba-5ed8-4f9f-8168-5eb74148a61b"},"source":["doc2 = nlp(u\"I saw eighteen mice today!\")\n","\n","show_lemmas(doc2)"],"execution_count":null,"outputs":[{"output_type":"stream","text":["I            PRON   561228191312463089     -PRON-\n","saw          VERB   11925638236994514241   see\n","eighteen     NUM    9609336664675087640    eighteen\n","mice         NOUN   1384165645700560590    mouse\n","today        NOUN   11042482332948150395   today\n","!            PUNCT  17494803046312582752   !\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"yuCt1lFbJYfI"},"source":["<font color=green>Notice that the lemma of `saw` is `see`, `mice` is the plural form of `mouse`, and yet `eighteen` is its own number, *not* an expanded form of `eight`.</font>"]},{"cell_type":"code","metadata":{"id":"kxb4NomDJYfJ","outputId":"ce2a313a-2877-4084-9547-e57fc36be798"},"source":["doc3 = nlp(u\"I am meeting him tomorrow at the meeting.\")\n","\n","show_lemmas(doc3)"],"execution_count":null,"outputs":[{"output_type":"stream","text":["I            PRON   561228191312463089     -PRON-\n","am           VERB   10382539506755952630   be\n","meeting      VERB   6880656908171229526    meet\n","him          PRON   561228191312463089     -PRON-\n","tomorrow     NOUN   3573583789758258062    tomorrow\n","at           ADP    11667289587015813222   at\n","the          DET    7425985699627899538    the\n","meeting      NOUN   14798207169164081740   meeting\n",".            PUNCT  12646065887601541794   .\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"Cl-Ok6m9JYfJ"},"source":["<font color=green>Here the lemma of `meeting` is determined by its Part of Speech tag.</font>"]},{"cell_type":"code","metadata":{"id":"e4p1lJwuJYfK","outputId":"fd844429-e59c-4160-f096-f857032a57a9"},"source":["doc4 = nlp(u\"That's an enormous automobile\")\n","\n","show_lemmas(doc4)"],"execution_count":null,"outputs":[{"output_type":"stream","text":["That         DET    4380130941430378203    that\n","'s           VERB   10382539506755952630   be\n","an           DET    15099054000809333061   an\n","enormous     ADJ    17917224542039855524   enormous\n","automobile   NOUN   7211811266693931283    automobile\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"E0-UCmKCJYfL"},"source":["<font color=green>Note that lemmatization does *not* reduce words to their most basic synonym - that is, `enormous` doesn't become `big` and `automobile` doesn't become `car`.</font>"]},{"cell_type":"markdown","metadata":{"id":"R8lK47veJYfL"},"source":["We should point out that although lemmatization looks at surrounding text to determine a given word's part of speech, it does not categorize phrases. In an upcoming lecture we'll investigate *word vectors and similarity*.\n","\n"]},{"cell_type":"markdown","metadata":{"id":"_3s7KOldLP9k"},"source":["## Stop Words\n","Words like \"a\" and \"the\" appear so frequently that they don't require tagging as thoroughly as nouns, verbs and modifiers. We call these *stop words*, and they can be filtered from the text to be processed. spaCy holds a built-in list of some 305 English stop words."]},{"cell_type":"code","metadata":{"id":"Y_idcED1LP9m"},"source":["# Perform standard imports:\n","import spacy\n","nlp = spacy.load('en_core_web_sm')"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"mFUnT780LP9n","outputId":"15ef3f1c-82e9-425c-b0ca-9899cb862fe6"},"source":["# Print the set of spaCy's default stop words (remember that sets are unordered):\n","print(nlp.Defaults.stop_words)"],"execution_count":null,"outputs":[{"output_type":"stream","text":["{'hers', 'show', 'though', 'various', 'sixty', 'say', 'quite', 'ten', 'anything', 'although', 'hereby', 'in', 'ours', 'herself', 'among', 'unless', 'and', 'whole', 'anywhere', 'latter', 'therein', 'whereafter', 'that', 'one', 'whose', 'either', 'within', 'eight', 'three', 'latterly', 'anyone', 'a', 'less', 'former', 'been', 'same', 'anyway', 'else', 'cannot', 'five', 'i', 'until', 'last', 'thus', 'give', 'move', 'thereafter', 'via', 'than', 'empty', 'off', 'neither', 'too', 'please', 'over', 'just', 'otherwise', 'has', 'her', 'put', 'its', 'whether', 'herein', 'myself', 'me', 'nevertheless', 'whatever', 'someone', 'towards', 'whereby', 'onto', 'sometimes', 'thence', 'them', 'done', 'at', 'back', 'nor', 'another', 'behind', 'together', 'take', 'amongst', 'being', 'seemed', 'seeming', 'fifteen', 'do', 'further', 'something', 'again', 'this', 'were', 'wherein', 'how', 'up', 'must', 'get', 'whereas', 'much', 'upon', 'yet', 'both', 'many', 'very', 'may', 'after', 'regarding', 'full', 'through', 'below', 'his', 'well', 'everything', 'so', 'our', 'should', 'seem', 'while', 'for', 'might', 'mine', 'when', 'with', 'you', 'few', 'never', 'because', 'own', 'also', 'due', 'hence', 'it', 'more', 'their', 'such', 'becomes', 'first', 'hereupon', 'since', 'third', 'twenty', 'who', 'she', 'nobody', 'name', 'really', 'enough', 'least', 'two', 'whoever', 'which', 'yours', 'moreover', 'seems', 'before', 'therefore', 'then', 'used', 'even', 'nowhere', 'without', 'other', 'around', 'made', 'hundred', 'no', 'twelve', 'several', 'your', 'meanwhile', 'per', 'except', 'yourselves', 'why', 'some', 'not', 'yourself', 'sometime', 'somehow', 'become', 'beyond', 'almost', 'will', 'somewhere', 'the', 'everyone', 'about', 'everywhere', 'anyhow', 'side', 'next', 'fifty', 'they', 'most', 'perhaps', 'across', 'themselves', 'besides', 'against', 'can', 'him', 'there', 'noone', 'under', 'formerly', 'already', 'all', 'if', 'my', 'or', 'serious', 'four', 'thereupon', 'whence', 'here', 'whither', 'beside', 'wherever', 'to', 'himself', 'between', 'ourselves', 'none', 'on', 'became', 'an', 'have', 'part', 'did', 'had', 'each', 'six', 'those', 'from', 'whenever', 'any', 'am', 'would', 'make', 'could', 'does', 'go', 'call', 'indeed', 'these', 'often', 'above', 'during', 'by', 'nine', 'thereby', 'others', 'afterwards', 'throughout', 'whom', 'amount', 'as', 'hereafter', 'top', 'mostly', 'us', 'whereupon', 'once', 'only', 'still', 'namely', 'forty', 'ca', 'along', 'be', 'itself', 'where', 'see', 'into', 'toward', 'but', 'is', 'keep', 'bottom', 'ever', 'becoming', 'every', 'always', 'front', 'nothing', 'we', 'of', 'out', 'eleven', 'alone', 'he', 'however', 'rather', 'down', 'thru', 'now', 'using', 'are', 'doing', 'what', 'beforehand', 're', 'was', 'elsewhere'}\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"krYa-HrlLP9p","outputId":"61e4d6d6-5d94-4b9b-e1be-9be8cc6ad06b"},"source":["len(nlp.Defaults.stop_words)"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["305"]},"metadata":{"tags":[]},"execution_count":3}]},{"cell_type":"markdown","metadata":{"id":"TrADh_zALP9r"},"source":["### To see if a word is a stop word"]},{"cell_type":"code","metadata":{"id":"0ZAKkTdfLP9t","outputId":"b42f90ee-2cbb-47ca-d9f6-bfcf3c483552"},"source":["nlp.vocab['myself'].is_stop"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["True"]},"metadata":{"tags":[]},"execution_count":4}]},{"cell_type":"code","metadata":{"id":"TZnFkN8wLP9w","outputId":"51565cbb-7f13-462b-f1f9-371894240bfa"},"source":["nlp.vocab['mystery'].is_stop"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["False"]},"metadata":{"tags":[]},"execution_count":5}]},{"cell_type":"markdown","metadata":{"id":"JbD09UkkLP9y"},"source":["### To add a stop word\n","There may be times when you wish to add a stop word to the default set. Perhaps you decide that `'btw'` (common shorthand for \"by the way\") should be considered a stop word."]},{"cell_type":"code","metadata":{"id":"9Uo11xLwLP9z"},"source":["# Add the word to the set of stop words. Use lowercase!\n","nlp.Defaults.stop_words.add('btw')\n","\n","# Set the stop_word tag on the lexeme\n","nlp.vocab['btw'].is_stop = True"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"7uhRVGVYLP90","outputId":"7aada40f-e4bd-4323-f3bd-4892b3b49325"},"source":["len(nlp.Defaults.stop_words)"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["306"]},"metadata":{"tags":[]},"execution_count":7}]},{"cell_type":"code","metadata":{"id":"csWO6Dc5LP91","outputId":"7d090810-40b1-448b-ba0e-fdf179160559"},"source":["nlp.vocab['btw'].is_stop"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["True"]},"metadata":{"tags":[]},"execution_count":8}]},{"cell_type":"markdown","metadata":{"id":"d_nj4hMDLP91"},"source":["<font color=green>When adding stop words, always use lowercase. Lexemes are converted to lowercase before being added to **vocab**.</font>"]},{"cell_type":"markdown","metadata":{"id":"QyN0NUVFLP92"},"source":["### To remove a stop word\n","Alternatively, you may decide that `'beyond'` should not be considered a stop word."]},{"cell_type":"code","metadata":{"id":"5gYzMoN4LP92"},"source":["# Remove the word from the set of stop words\n","nlp.Defaults.stop_words.remove('beyond')\n","\n","# Remove the stop_word tag from the lexeme\n","nlp.vocab['beyond'].is_stop = False"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"LRfGvK8xLP93","outputId":"44744c11-8532-4594-a1bd-e3c22c012f36"},"source":["len(nlp.Defaults.stop_words)"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["305"]},"metadata":{"tags":[]},"execution_count":10}]},{"cell_type":"code","metadata":{"id":"PBAVgkYQLP93","outputId":"edc8fe69-efd7-4552-d0f7-3230530b656e"},"source":["nlp.vocab['beyond'].is_stop"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["False"]},"metadata":{"tags":[]},"execution_count":11}]},{"cell_type":"markdown","metadata":{"id":"wL_YiJZJLP94"},"source":["Great! Now you should be able to access spaCy's default set of stop words, and add or remove stop words as needed.\n"]},{"cell_type":"markdown","metadata":{"id":"7v3iFhYmMK7X"},"source":["## Vocabulary and Matching\n","So far we've seen how a body of text is divided into tokens, and how individual tokens are parsed and tagged with parts of speech, dependencies and lemmas.\n","\n","In this section we will identify and label specific phrases that match patterns we can define ourselves. "]},{"cell_type":"markdown","metadata":{"id":"kDNUS95cMK7Y"},"source":["### Rule-based Matching\n","spaCy offers a rule-matching tool called `Matcher` that allows you to build a library of token patterns, then match those patterns against a Doc object to return a list of found matches. You can match on any part of the token including text and annotations, and you can add multiple patterns to the same matcher."]},{"cell_type":"code","metadata":{"id":"lt86ky79MK7Z"},"source":["# Perform standard imports\n","import spacy\n","nlp = spacy.load('en_core_web_sm')"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"I46ZiPAGMK7a"},"source":["# Import the Matcher library\n","from spacy.matcher import Matcher\n","matcher = Matcher(nlp.vocab)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"yUdMbfYSMK7b"},"source":["<font color=green>Here `matcher` is an object that pairs to the current `Vocab` object. We can add and remove specific named matchers to `matcher` as needed.</font>"]},{"cell_type":"markdown","metadata":{"id":"8uyO7VCqMK7b"},"source":["#### Creating patterns\n","In literature, the phrase 'solar power' might appear as one word or two, with or without a hyphen. In this section we'll develop a matcher named 'SolarPower' that finds all three:"]},{"cell_type":"code","metadata":{"id":"OY8G0iVBMK7c"},"source":["pattern1 = [{'LOWER': 'solarpower'}]\n","pattern2 = [{'LOWER': 'solar'}, {'LOWER': 'power'}]\n","pattern3 = [{'LOWER': 'solar'}, {'IS_PUNCT': True}, {'LOWER': 'power'}]\n","\n","matcher.add('SolarPower', None, pattern1, pattern2, pattern3)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"KIRhHNwZMK7c"},"source":["Let's break this down:\n","* `pattern1` looks for a single token whose lowercase text reads 'solarpower'\n","* `pattern2` looks for two adjacent tokens that read 'solar' and 'power' in that order\n","* `pattern3` looks for three adjacent tokens, with a middle token that can be any punctuation.<font color=green>*</font>\n","\n","<font color=green>\\* Remember that single spaces are not tokenized, so they don't count as punctuation.</font>\n","<br>Once we define our patterns, we pass them into `matcher` with the name 'SolarPower', and set *callbacks* to `None` (more on callbacks later)."]},{"cell_type":"markdown","metadata":{"id":"r_jHct-RMK7d"},"source":["#### Applying the matcher to a Doc object"]},{"cell_type":"code","metadata":{"id":"GJoWABB1MK7e"},"source":["doc = nlp(u'The Solar Power industry continues to grow as demand \\\n","for solarpower increases. Solar-power cars are gaining popularity.')"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"He_RfZyCMK7e","outputId":"886c6eb6-f5f0-48b5-c455-69ba97283e44"},"source":["found_matches = matcher(doc)\n","print(found_matches)"],"execution_count":null,"outputs":[{"output_type":"stream","text":["[(8656102463236116519, 1, 3), (8656102463236116519, 10, 11), (8656102463236116519, 13, 16)]\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"_5M3n0boMK7g"},"source":["`matcher` returns a list of tuples. Each tuple contains an ID for the match, with start & end tokens that map to the span `doc[start:end]`"]},{"cell_type":"code","metadata":{"id":"rulaKF8_MK7g","outputId":"f0395405-f7cd-4598-e94a-2fa5e65a23e2"},"source":["for match_id, start, end in found_matches:\n","    string_id = nlp.vocab.strings[match_id]  # get string representation\n","    span = doc[start:end]                    # get the matched span\n","    print(match_id, string_id, start, end, span.text)"],"execution_count":null,"outputs":[{"output_type":"stream","text":["8656102463236116519 SolarPower 1 3 Solar Power\n","8656102463236116519 SolarPower 10 11 solarpower\n","8656102463236116519 SolarPower 13 16 Solar-power\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"pDKXOw1SMK7h"},"source":["The `match_id` is simply the hash value of the `string_ID` 'SolarPower'"]},{"cell_type":"markdown","metadata":{"id":"3AdCuYjjMK7j"},"source":["#### Setting pattern options and quantifiers\n","You can make token rules optional by passing an `'OP':'*'` argument. This lets us streamline our patterns list:"]},{"cell_type":"code","metadata":{"id":"B2PuwFAlMK7k"},"source":["# Redefine the patterns:\n","pattern1 = [{'LOWER': 'solarpower'}]\n","pattern2 = [{'LOWER': 'solar'}, {'IS_PUNCT': True, 'OP':'*'}, {'LOWER': 'power'}]\n","\n","# Remove the old patterns to avoid duplication:\n","matcher.remove('SolarPower')\n","\n","# Add the new set of patterns to the 'SolarPower' matcher:\n","matcher.add('SolarPower', None, pattern1, pattern2)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"e5MY_S6NMK7k","outputId":"19ba9dbf-56c4-4a91-a40d-4cc6e59e5d05"},"source":["found_matches = matcher(doc)\n","print(found_matches)"],"execution_count":null,"outputs":[{"output_type":"stream","text":["[(8656102463236116519, 1, 3), (8656102463236116519, 10, 11), (8656102463236116519, 13, 16)]\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"FayXwZLUMK7k"},"source":["This found both two-word patterns, with and without the hyphen!\n","\n","The following quantifiers can be passed to the `'OP'` key:\n","<table><tr><th>OP</th><th>Description</th></tr>\n","\n","<tr ><td><span >\\!</span></td><td>Negate the pattern, by requiring it to match exactly 0 times</td></tr>\n","<tr ><td><span >?</span></td><td>Make the pattern optional, by allowing it to match 0 or 1 times</td></tr>\n","<tr ><td><span >\\+</span></td><td>Require the pattern to match 1 or more times</td></tr>\n","<tr ><td><span >\\*</span></td><td>Allow the pattern to match zero or more times</td></tr>\n","</table>\n"]},{"cell_type":"markdown","metadata":{"id":"wmw_GbaIMK7l"},"source":["#### Be careful with lemmas!\n","If we wanted to match on both 'solar power' and 'solar powered', it might be tempting to look for the *lemma* of 'powered' and expect it to be 'power'. This is not always the case! The lemma of the *adjective* 'powered' is still 'powered':"]},{"cell_type":"code","metadata":{"id":"9FF-5NdWMK7l"},"source":["pattern1 = [{'LOWER': 'solarpower'}]\n","pattern2 = [{'LOWER': 'solar'}, {'IS_PUNCT': True, 'OP':'*'}, {'LEMMA': 'power'}] # CHANGE THIS PATTERN\n","\n","# Remove the old patterns to avoid duplication:\n","matcher.remove('SolarPower')\n","\n","# Add the new set of patterns to the 'SolarPower' matcher:\n","matcher.add('SolarPower', None, pattern1, pattern2)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"B4AN5KN5MK7l"},"source":["doc2 = nlp(u'Solar-powered energy runs solar-powered cars.')"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"kQiLRhwHMK7m","outputId":"7a80188a-e667-4c3c-877e-4c0d0669a454"},"source":["found_matches = matcher(doc2)\n","print(found_matches)"],"execution_count":null,"outputs":[{"output_type":"stream","text":["[(8656102463236116519, 0, 3)]\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"3mtPQ_WyMK7m"},"source":["<font color=green>The matcher found the first occurrence because the lemmatizer treated 'Solar-powered' as a verb, but not the second as it considered it an adjective.<br>For this case it may be better to set explicit token patterns.</font>"]},{"cell_type":"code","metadata":{"id":"8LtiR1wXMK7m"},"source":["pattern1 = [{'LOWER': 'solarpower'}]\n","pattern2 = [{'LOWER': 'solar'}, {'IS_PUNCT': True, 'OP':'*'}, {'LOWER': 'power'}]\n","pattern3 = [{'LOWER': 'solarpowered'}]\n","pattern4 = [{'LOWER': 'solar'}, {'IS_PUNCT': True, 'OP':'*'}, {'LOWER': 'powered'}]\n","\n","# Remove the old patterns to avoid duplication:\n","matcher.remove('SolarPower')\n","\n","# Add the new set of patterns to the 'SolarPower' matcher:\n","matcher.add('SolarPower', None, pattern1, pattern2, pattern3, pattern4)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"ULIw1wfCMK7n","outputId":"af0ed72f-fcd2-49b6-a10c-616235b56f67"},"source":["found_matches = matcher(doc2)\n","print(found_matches)"],"execution_count":null,"outputs":[{"output_type":"stream","text":["[(8656102463236116519, 0, 3), (8656102463236116519, 5, 8)]\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"_kKg6AViMK7n"},"source":["### Other token attributes\n","Besides lemmas, there are a variety of token attributes we can use to determine matching rules:\n","<table><tr><th>Attribute</th><th>Description</th></tr>\n","\n","<tr ><td><span >`ORTH`</span></td><td>The exact verbatim text of a token</td></tr>\n","<tr ><td><span >`LOWER`</span></td><td>The lowercase form of the token text</td></tr>\n","<tr ><td><span >`LENGTH`</span></td><td>The length of the token text</td></tr>\n","<tr ><td><span >`IS_ALPHA`, `IS_ASCII`, `IS_DIGIT`</span></td><td>Token text consists of alphanumeric characters, ASCII characters, digits</td></tr>\n","<tr ><td><span >`IS_LOWER`, `IS_UPPER`, `IS_TITLE`</span></td><td>Token text is in lowercase, uppercase, titlecase</td></tr>\n","<tr ><td><span >`IS_PUNCT`, `IS_SPACE`, `IS_STOP`</span></td><td>Token is punctuation, whitespace, stop word</td></tr>\n","<tr ><td><span >`LIKE_NUM`, `LIKE_URL`, `LIKE_EMAIL`</span></td><td>Token text resembles a number, URL, email</td></tr>\n","<tr ><td><span >`POS`, `TAG`, `DEP`, `LEMMA`, `SHAPE`</span></td><td>The token's simple and extended part-of-speech tag, dependency label, lemma, shape</td></tr>\n","<tr ><td><span >`ENT_TYPE`</span></td><td>The token's entity label</td></tr>\n","\n","</table>"]},{"cell_type":"markdown","metadata":{"id":"AwIWZF7jMK7o"},"source":["#### Token wildcard\n","You can pass an empty dictionary `{}` as a wildcard to represent **any token**. For example, you might want to retrieve hashtags without knowing what might follow the `#` character:\n",">`[{'ORTH': '#'}, {}]`"]},{"cell_type":"markdown","metadata":{"id":"_tVudSk9MK7p"},"source":["___\n","### PhraseMatcher\n","In the above section we used token patterns to perform rule-based matching. An alternative - and often more efficient - method is to match on terminology lists. In this case we use PhraseMatcher to create a Doc object from a list of phrases, and pass that into `matcher` instead."]},{"cell_type":"code","metadata":{"id":"93RU4ZEHMK7p"},"source":["# Perform standard imports, reset nlp\n","import spacy\n","nlp = spacy.load('en_core_web_sm')"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"5Lj9gKl-MK7q"},"source":["# Import the PhraseMatcher library\n","from spacy.matcher import PhraseMatcher\n","matcher = PhraseMatcher(nlp.vocab)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"e19oXeyOMK7q"},"source":["For this exercise we're going to import a Wikipedia article on *Reaganomics*<br>\n","Source: https://en.wikipedia.org/wiki/Reaganomics"]},{"cell_type":"code","metadata":{"id":"d4KDEU63MK7r"},"source":["with open('../TextFiles/reaganomics.txt', encoding='utf8') as f:\n","    doc3 = nlp(f.read())"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"wfVMn4e1MK7r"},"source":["# First, create a list of match phrases:\n","phrase_list = ['voodoo economics', 'supply-side economics', 'trickle-down economics', 'free-market economics']\n","\n","# Next, convert each phrase to a Doc object:\n","phrase_patterns = [nlp(text) for text in phrase_list]\n","\n","# Pass each Doc object into matcher (note the use of the asterisk!):\n","matcher.add('VoodooEconomics', None, *phrase_patterns)\n","\n","# Build a list of matches:\n","matches = matcher(doc3)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"rsPVoKIuMK7s","outputId":"59dc57af-5f6a-4c61-bbeb-5512ec0c7824"},"source":["# (match_id, start, end)\n","matches"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["[(3473369816841043438, 41, 45),\n"," (3473369816841043438, 49, 53),\n"," (3473369816841043438, 54, 56),\n"," (3473369816841043438, 61, 65),\n"," (3473369816841043438, 673, 677),\n"," (3473369816841043438, 2985, 2989)]"]},"metadata":{"tags":[]},"execution_count":18}]},{"cell_type":"markdown","metadata":{"id":"GIDWroiEMK7t"},"source":["<font color=green>The first four matches are where these terms are used in the definition of Reaganomics:</font>"]},{"cell_type":"code","metadata":{"id":"fI2igPNYMK7t","outputId":"5346ab29-8b9d-4f12-a4b3-8938dea309f8"},"source":["doc3[:70]"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["REAGANOMICS\n","https://en.wikipedia.org/wiki/Reaganomics\n","\n","Reaganomics (a portmanteau of [Ronald] Reagan and economics attributed to Paul Harvey)[1] refers to the economic policies promoted by U.S. President Ronald Reagan during the 1980s. These policies are commonly associated with supply-side economics, referred to as trickle-down economics or voodoo economics by political opponents, and free-market economics by political advocates.\n"]},"metadata":{"tags":[]},"execution_count":19}]},{"cell_type":"markdown","metadata":{"id":"oGhXUPRTMK7u"},"source":["### Viewing Matches\n","There are a few ways to fetch the text surrounding a match. The simplest is to grab a slice of tokens from the doc that is wider than the match:"]},{"cell_type":"code","metadata":{"id":"SKX4ei0qMK7u","outputId":"d11799c6-d5bd-42ea-e065-66cf064a6bb9"},"source":["doc3[665:685]  # Note that the fifth match starts at doc3[673]"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["same time he attracted a following from the supply-side economics movement, which formed in opposition to Keynesian"]},"metadata":{"tags":[]},"execution_count":20}]},{"cell_type":"code","metadata":{"id":"JCLwECW2MK7u","outputId":"e1e0ce5d-035b-4cb8-8728-52be3c9277ac"},"source":["doc3[2975:2995]  # The sixth match starts at doc3[2985]"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["against institutions.[66] His policies became widely known as \"trickle-down economics\", due to the significant"]},"metadata":{"tags":[]},"execution_count":21}]},{"cell_type":"markdown","metadata":{"id":"j9qo-wL_MK7v"},"source":["Another way is to first apply the `sentencizer` to the Doc, then iterate through the sentences to the match point:"]},{"cell_type":"code","metadata":{"id":"67E-c4CdMK7w","outputId":"b23ac190-c165-430d-97e5-aba6d31df3f9"},"source":["# Build a list of sentences\n","sents = [sent for sent in doc3.sents]\n","\n","# In the next section we'll see that sentences contain start and end token values:\n","print(sents[0].start, sents[0].end)"],"execution_count":null,"outputs":[{"output_type":"stream","text":["0 35\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"gdQfoF2SMK7w","outputId":"09b6d907-7c0e-4724-fa02-fe5a050e7319"},"source":["# Iterate over the sentence list until the sentence end value exceeds a match start value:\n","for sent in sents:\n","    if matches[4][1] < sent.end:  # this is the fifth match, that starts at doc3[673]\n","        print(sent)\n","        break"],"execution_count":null,"outputs":[{"output_type":"stream","text":["At the same time he attracted a following from the supply-side economics movement, which formed in opposition to Keynesian demand-stimulus economics.\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"iu4YQmDuMK7w"},"source":["For additional information visit https://spacy.io/usage/linguistic-features#section-rule-based-matching\n"]},{"cell_type":"markdown","metadata":{"id":"bnFXtNeY_1QF"},"source":["## Part of Speech Basics\n","The challenge of correctly identifying parts of speech is summed up nicely in the [spaCy docs](https://spacy.io/usage/linguistic-features):\n","<div class=\"alert alert-info\" style=\"margin: 20px\">Processing raw text intelligently is difficult: most words are rare, and it's common for words that look completely different to mean almost the same thing. The same words in a different order can mean something completely different. Even splitting text into useful word-like units can be difficult in many languages. While it's possible to solve some problems starting from only the raw characters, it's usually better to use linguistic knowledge to add useful information. That's exactly what spaCy is designed to do: you put in raw text, and get back a **Doc** object, that comes with a variety of annotations.</div>\n","In this section we'll take a closer look at coarse POS tags (noun, verb, adjective) and fine-grained tags (plural noun, past-tense verb, superlative adjective)."]},{"cell_type":"code","metadata":{"id":"PE3ti3DL_1QP"},"source":["# Perform standard imports\n","import spacy\n","nlp = spacy.load('en_core_web_sm')"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"L2QpCOAr_1QS"},"source":["# Create a simple Doc object\n","doc = nlp(u\"The quick brown fox jumped over the lazy dog's back.\")"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"uElJ7lXk_1QT"},"source":["### View token tags\n","Recall that you can obtain a particular token by its index position.\n","* To view the coarse POS tag use `token.pos_`\n","* To view the fine-grained tag use `token.tag_`\n","* To view the description of either type of tag use `spacy.explain(tag)`\n","\n","<div class=\"alert alert-success\">Note that `token.pos` and `token.tag` return integer hash values; by adding the underscores we get the text equivalent that lives in **doc.vocab**.</div>"]},{"cell_type":"code","metadata":{"id":"WAu5xWp3_1QU","outputId":"45d1512a-da52-4252-8acb-1a7d88eb53d3"},"source":["# Print the full text:\n","print(doc.text)"],"execution_count":null,"outputs":[{"output_type":"stream","text":["The quick brown fox jumped over the lazy dog's back.\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"4RYTVq7f_1QX","outputId":"ec0099d7-97b5-40a9-83ab-4bf1797fd687"},"source":["# Print the fifth word and associated tags:\n","print(doc[4].text, doc[4].pos_, doc[4].tag_, spacy.explain(doc[4].tag_))"],"execution_count":null,"outputs":[{"output_type":"stream","text":["jumped VERB VBD verb, past tense\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"xM8WciWX_1QZ"},"source":["We can apply this technique to the entire Doc object:"]},{"cell_type":"code","metadata":{"id":"IMAPRRsP_1QZ","outputId":"447f7d40-ada3-4013-e4c4-b27e9e5309ea"},"source":["for token in doc:\n","    print(f'{token.text:{10}} {token.pos_:{8}} {token.tag_:{6}} {spacy.explain(token.tag_)}')"],"execution_count":null,"outputs":[{"output_type":"stream","text":["The        DET      DT     determiner\n","quick      ADJ      JJ     adjective\n","brown      ADJ      JJ     adjective\n","fox        NOUN     NN     noun, singular or mass\n","jumped     VERB     VBD    verb, past tense\n","over       ADP      IN     conjunction, subordinating or preposition\n","the        DET      DT     determiner\n","lazy       ADJ      JJ     adjective\n","dog        NOUN     NN     noun, singular or mass\n","'s         PART     POS    possessive ending\n","back       NOUN     NN     noun, singular or mass\n",".          PUNCT    .      punctuation mark, sentence closer\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"collapsed":true,"id":"HknnRY0z_1Qa"},"source":["### Coarse-grained Part-of-speech Tags\n","Every token is assigned a POS Tag from the following list:\n","\n","\n","<table><tr><th>POS</th><th>DESCRIPTION</th><th>EXAMPLES</th></tr>\n","    \n","<tr><td>ADJ</td><td>adjective</td><td>*big, old, green, incomprehensible, first*</td></tr>\n","<tr><td>ADP</td><td>adposition</td><td>*in, to, during*</td></tr>\n","<tr><td>ADV</td><td>adverb</td><td>*very, tomorrow, down, where, there*</td></tr>\n","<tr><td>AUX</td><td>auxiliary</td><td>*is, has (done), will (do), should (do)*</td></tr>\n","<tr><td>CONJ</td><td>conjunction</td><td>*and, or, but*</td></tr>\n","<tr><td>CCONJ</td><td>coordinating conjunction</td><td>*and, or, but*</td></tr>\n","<tr><td>DET</td><td>determiner</td><td>*a, an, the*</td></tr>\n","<tr><td>INTJ</td><td>interjection</td><td>*psst, ouch, bravo, hello*</td></tr>\n","<tr><td>NOUN</td><td>noun</td><td>*girl, cat, tree, air, beauty*</td></tr>\n","<tr><td>NUM</td><td>numeral</td><td>*1, 2017, one, seventy-seven, IV, MMXIV*</td></tr>\n","<tr><td>PART</td><td>particle</td><td>*'s, not,*</td></tr>\n","<tr><td>PRON</td><td>pronoun</td><td>*I, you, he, she, myself, themselves, somebody*</td></tr>\n","<tr><td>PROPN</td><td>proper noun</td><td>*Mary, John, London, NATO, HBO*</td></tr>\n","<tr><td>PUNCT</td><td>punctuation</td><td>*., (, ), ?*</td></tr>\n","<tr><td>SCONJ</td><td>subordinating conjunction</td><td>*if, while, that*</td></tr>\n","<tr><td>SYM</td><td>symbol</td><td>*$, %, §, ©, +, −, ×, ÷, =, :), 😝*</td></tr>\n","<tr><td>VERB</td><td>verb</td><td>*run, runs, running, eat, ate, eating*</td></tr>\n","<tr><td>X</td><td>other</td><td>*sfpksdpsxmsa*</td></tr>\n","<tr><td>SPACE</td><td>space</td></tr>"]},{"cell_type":"markdown","metadata":{"id":"8aRalgOz_1Qh"},"source":["___\n","### Fine-grained Part-of-speech Tags\n","Tokens are subsequently given a fine-grained tag as determined by morphology:\n","<table>\n","<tr><th>POS</th><th>Description</th><th>Fine-grained Tag</th><th>Description</th><th>Morphology</th></tr>\n","<tr><td>ADJ</td><td>adjective</td><td>AFX</td><td>affix</td><td>Hyph=yes</td></tr>\n","<tr><td>ADJ</td><td></td><td>JJ</td><td>adjective</td><td>Degree=pos</td></tr>\n","<tr><td>ADJ</td><td></td><td>JJR</td><td>adjective, comparative</td><td>Degree=comp</td></tr>\n","<tr><td>ADJ</td><td></td><td>JJS</td><td>adjective, superlative</td><td>Degree=sup</td></tr>\n","<tr><td>ADJ</td><td></td><td>PDT</td><td>predeterminer</td><td>AdjType=pdt PronType=prn</td></tr>\n","<tr><td>ADJ</td><td></td><td>PRP\\$</td><td>pronoun, possessive</td><td>PronType=prs Poss=yes</td></tr>\n","<tr><td>ADJ</td><td></td><td>WDT</td><td>wh-determiner</td><td>PronType=int rel</td></tr>\n","<tr><td>ADJ</td><td></td><td>WP\\$</td><td>wh-pronoun, possessive</td><td>Poss=yes PronType=int rel</td></tr>\n","<tr><td>ADP</td><td>adposition</td><td>IN</td><td>conjunction, subordinating or preposition</td><td></td></tr>\n","<tr><td>ADV</td><td>adverb</td><td>EX</td><td>existential there</td><td>AdvType=ex</td></tr>\n","<tr><td>ADV</td><td></td><td>RB</td><td>adverb</td><td>Degree=pos</td></tr>\n","<tr><td>ADV</td><td></td><td>RBR</td><td>adverb, comparative</td><td>Degree=comp</td></tr>\n","<tr><td>ADV</td><td></td><td>RBS</td><td>adverb, superlative</td><td>Degree=sup</td></tr>\n","<tr><td>ADV</td><td></td><td>WRB</td><td>wh-adverb</td><td>PronType=int rel</td></tr>\n","<tr><td>CONJ</td><td>conjunction</td><td>CC</td><td>conjunction, coordinating</td><td>ConjType=coor</td></tr>\n","<tr><td>DET</td><td>determiner</td><td>DT</td><td>determiner</td><td></td></tr>\n","<tr><td>INTJ</td><td>interjection</td><td>UH</td><td>interjection</td><td></td></tr>\n","<tr><td>NOUN</td><td>noun</td><td>NN</td><td>noun, singular or mass</td><td>Number=sing</td></tr>\n","<tr><td>NOUN</td><td></td><td>NNS</td><td>noun, plural</td><td>Number=plur</td></tr>\n","<tr><td>NOUN</td><td></td><td>WP</td><td>wh-pronoun, personal</td><td>PronType=int rel</td></tr>\n","<tr><td>NUM</td><td>numeral</td><td>CD</td><td>cardinal number</td><td>NumType=card</td></tr>\n","<tr><td>PART</td><td>particle</td><td>POS</td><td>possessive ending</td><td>Poss=yes</td></tr>\n","<tr><td>PART</td><td></td><td>RP</td><td>adverb, particle</td><td></td></tr>\n","<tr><td>PART</td><td></td><td>TO</td><td>infinitival to</td><td>PartType=inf VerbForm=inf</td></tr>\n","<tr><td>PRON</td><td>pronoun</td><td>PRP</td><td>pronoun, personal</td><td>PronType=prs</td></tr>\n","<tr><td>PROPN</td><td>proper noun</td><td>NNP</td><td>noun, proper singular</td><td>NounType=prop Number=sign</td></tr>\n","<tr><td>PROPN</td><td></td><td>NNPS</td><td>noun, proper plural</td><td>NounType=prop Number=plur</td></tr>\n","<tr><td>PUNCT</td><td>punctuation</td><td>-LRB-</td><td>left round bracket</td><td>PunctType=brck PunctSide=ini</td></tr>\n","<tr><td>PUNCT</td><td></td><td>-RRB-</td><td>right round bracket</td><td>PunctType=brck PunctSide=fin</td></tr>\n","<tr><td>PUNCT</td><td></td><td>,</td><td>punctuation mark, comma</td><td>PunctType=comm</td></tr>\n","<tr><td>PUNCT</td><td></td><td>:</td><td>punctuation mark, colon or ellipsis</td><td></td></tr>\n","<tr><td>PUNCT</td><td></td><td>.</td><td>punctuation mark, sentence closer</td><td>PunctType=peri</td></tr>\n","<tr><td>PUNCT</td><td></td><td>''</td><td>closing quotation mark</td><td>PunctType=quot PunctSide=fin</td></tr>\n","<tr><td>PUNCT</td><td></td><td>\"\"</td><td>closing quotation mark</td><td>PunctType=quot PunctSide=fin</td></tr>\n","<tr><td>PUNCT</td><td></td><td>``</td><td>opening quotation mark</td><td>PunctType=quot PunctSide=ini</td></tr>\n","<tr><td>PUNCT</td><td></td><td>HYPH</td><td>punctuation mark, hyphen</td><td>PunctType=dash</td></tr>\n","<tr><td>PUNCT</td><td></td><td>LS</td><td>list item marker</td><td>NumType=ord</td></tr>\n","<tr><td>PUNCT</td><td></td><td>NFP</td><td>superfluous punctuation</td><td></td></tr>\n","<tr><td>SYM</td><td>symbol</td><td>#</td><td>symbol, number sign</td><td>SymType=numbersign</td></tr>\n","<tr><td>SYM</td><td></td><td>\\$</td><td>symbol, currency</td><td>SymType=currency</td></tr>\n","<tr><td>SYM</td><td></td><td>SYM</td><td>symbol</td><td></td></tr>\n","<tr><td>VERB</td><td>verb</td><td>BES</td><td>auxiliary \"be\"</td><td></td></tr>\n","<tr><td>VERB</td><td></td><td>HVS</td><td>forms of \"have\"</td><td></td></tr>\n","<tr><td>VERB</td><td></td><td>MD</td><td>verb, modal auxiliary</td><td>VerbType=mod</td></tr>\n","<tr><td>VERB</td><td></td><td>VB</td><td>verb, base form</td><td>VerbForm=inf</td></tr>\n","<tr><td>VERB</td><td></td><td>VBD</td><td>verb, past tense</td><td>VerbForm=fin Tense=past</td></tr>\n","<tr><td>VERB</td><td></td><td>VBG</td><td>verb, gerund or present participle</td><td>VerbForm=part Tense=pres Aspect=prog</td></tr>\n","<tr><td>VERB</td><td></td><td>VBN</td><td>verb, past participle</td><td>VerbForm=part Tense=past Aspect=perf</td></tr>\n","<tr><td>VERB</td><td></td><td>VBP</td><td>verb, non-3rd person singular present</td><td>VerbForm=fin Tense=pres</td></tr>\n","<tr><td>VERB</td><td></td><td>VBZ</td><td>verb, 3rd person singular present</td><td>VerbForm=fin Tense=pres Number=sing Person=3</td></tr>\n","<tr><td>X</td><td>other</td><td>ADD</td><td>email</td><td></td></tr>\n","<tr><td>X</td><td></td><td>FW</td><td>foreign word</td><td>Foreign=yes</td></tr>\n","<tr><td>X</td><td></td><td>GW</td><td>additional word in multi-word expression</td><td></td></tr>\n","<tr><td>X</td><td></td><td>XX</td><td>unknown</td><td></td></tr>\n","<tr><td>SPACE</td><td>space</td><td>_SP</td><td>space</td><td></td></tr>\n","<tr><td></td><td></td><td>NIL</td><td>missing tag</td><td></td></tr>\n","</table>"]},{"cell_type":"markdown","metadata":{"id":"wrGTuME5_1Qr"},"source":["For a current list of tags for all languages visit https://spacy.io/api/annotation#pos-tagging"]},{"cell_type":"markdown","metadata":{"id":"mqCfYJrb_1Q2"},"source":["### Working with POS Tags\n","In the English language, the same string of characters can have different meanings, even within the same sentence. For this reason, morphology is important. **spaCy** uses machine learning algorithms to best predict the use of a token in a sentence. Is *\"I read books on NLP\"* present or past tense? Is *wind* a verb or a noun?"]},{"cell_type":"code","metadata":{"id":"QLIrN2dg_1Q2","outputId":"6be59fbf-5f93-436e-d456-3bdbdd808f49"},"source":["doc = nlp(u'I read books on NLP.')\n","r = doc[1]\n","\n","print(f'{r.text:{10}} {r.pos_:{8}} {r.tag_:{6}} {spacy.explain(r.tag_)}')"],"execution_count":null,"outputs":[{"output_type":"stream","text":["read       VERB     VBP    verb, non-3rd person singular present\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"fpIikEwg_1Q4","outputId":"a88cde20-ba92-4032-b6c8-83ae42875df3"},"source":["doc = nlp(u'I read a book on NLP.')\n","r = doc[1]\n","\n","print(f'{r.text:{10}} {r.pos_:{8}} {r.tag_:{6}} {spacy.explain(r.tag_)}')"],"execution_count":null,"outputs":[{"output_type":"stream","text":["read       VERB     VBD    verb, past tense\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"pEgXnS7K_1Q5"},"source":["In the first example, with no other cues to work from, spaCy assumed that ***read*** was present tense.<br>In the second example the present tense form would be ***I am reading a book***, so spaCy assigned the past tense."]},{"cell_type":"markdown","metadata":{"id":"4G2bibxA_1Q6"},"source":["### Counting POS Tags\n","The `Doc.count_by()` method accepts a specific token attribute as its argument, and returns a frequency count of the given attribute as a dictionary object. Keys in the dictionary are the integer values of the given attribute ID, and values are the frequency. Counts of zero are not included."]},{"cell_type":"code","metadata":{"id":"52Lh7G4w_1Q7","outputId":"072adde5-e186-4fb8-8cb7-69887c330864"},"source":["doc = nlp(u\"The quick brown fox jumped over the lazy dog's back.\")\n","\n","# Count the frequencies of different coarse-grained POS tags:\n","POS_counts = doc.count_by(spacy.attrs.POS)\n","POS_counts"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["{83: 3, 84: 1, 89: 2, 91: 3, 93: 1, 96: 1, 99: 1}"]},"metadata":{"tags":[]},"execution_count":8}]},{"cell_type":"markdown","metadata":{"id":"wb9t8oL4_1RB"},"source":["This isn't very helpful until you decode the attribute ID:"]},{"cell_type":"code","metadata":{"id":"PUtF0RK4_1RC","outputId":"17de4d8b-2102-4ec7-dfa9-89497d9e9667"},"source":["doc.vocab[83].text"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["'ADJ'"]},"metadata":{"tags":[]},"execution_count":9}]},{"cell_type":"markdown","metadata":{"id":"vOfj6cn9_1RD"},"source":["#### Create a frequency list of POS tags from the entire document\n","Since `POS_counts` returns a dictionary, we can obtain a list of keys with `POS_counts.items()`.<br>By sorting the list we have access to the tag and its count, in order."]},{"cell_type":"code","metadata":{"id":"lUn5t2kA_1RE","outputId":"5c67e035-7d97-484b-b035-1cdac0e6b129"},"source":["for k,v in sorted(POS_counts.items()):\n","    print(f'{k}. {doc.vocab[k].text:{5}}: {v}')"],"execution_count":null,"outputs":[{"output_type":"stream","text":["83. ADJ  : 3\n","84. ADP  : 1\n","89. DET  : 2\n","91. NOUN : 3\n","93. PART : 1\n","96. PUNCT: 1\n","99. VERB : 1\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"8sy2IjVo_1RG","outputId":"766edb58-b4d7-425c-9fb4-e5c5468f3bde"},"source":["# Count the different fine-grained tags:\n","TAG_counts = doc.count_by(spacy.attrs.TAG)\n","\n","for k,v in sorted(TAG_counts.items()):\n","    print(f'{k}. {doc.vocab[k].text:{4}}: {v}')"],"execution_count":null,"outputs":[{"output_type":"stream","text":["74. POS : 1\n","1292078113972184607. IN  : 1\n","10554686591937588953. JJ  : 3\n","12646065887601541794. .   : 1\n","15267657372422890137. DT  : 2\n","15308085513773655218. NN  : 3\n","17109001835818727656. VBD : 1\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"b3l5q4PK_1RH"},"source":["<div class=\"alert alert-success\">**Why did the ID numbers get so big?** In spaCy, certain text values are hardcoded into `Doc.vocab` and take up the first several hundred ID numbers. Strings like 'NOUN' and 'VERB' are used frequently by internal operations. Others, like fine-grained tags, are assigned hash values as needed.</div>\n","<div class=\"alert alert-success\">**Why don't SPACE tags appear?** In spaCy, only strings of spaces (two or more) are assigned tokens. Single spaces are not.</div>"]},{"cell_type":"code","metadata":{"id":"lQBge6L2_1RI","outputId":"464b29a0-2a5b-4ae4-97e9-793fa32da9d8"},"source":["# Count the different dependencies:\n","DEP_counts = doc.count_by(spacy.attrs.DEP)\n","\n","for k,v in sorted(DEP_counts.items()):\n","    print(f'{k}. {doc.vocab[k].text:{4}}: {v}')"],"execution_count":null,"outputs":[{"output_type":"stream","text":["399. amod: 3\n","412. det : 2\n","426. nsubj: 1\n","436. pobj: 1\n","437. poss: 1\n","440. prep: 1\n","442. punct: 1\n","8110129090154140942. case: 1\n","8206900633647566924. ROOT: 1\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"2F6sRB69_1RK"},"source":["Here we've shown `spacy.attrs.POS`, `spacy.attrs.TAG` and `spacy.attrs.DEP`.<br>Refer back to the **Vocabulary and Matching** lecture from the previous section for a table of **Other token attributes**."]},{"cell_type":"markdown","metadata":{"id":"wDRSNO7L_1RL"},"source":["___\n","### Fine-grained POS Tag Examples\n","These are some grammatical examples (shown in **bold**) of specific fine-grained tags. We've removed punctuation and rarely used tags:\n","<table>\n","<tr><th>POS</th><th>TAG</th><th>DESCRIPTION</th><th>EXAMPLE</th></tr>\n","<tr><td>ADJ</td><td>AFX</td><td>affix</td><td>The Flintstones were a **pre**-historic family.</td></tr>\n","<tr><td>ADJ</td><td>JJ</td><td>adjective</td><td>This is a **good** sentence.</td></tr>\n","<tr><td>ADJ</td><td>JJR</td><td>adjective, comparative</td><td>This is a **better** sentence.</td></tr>\n","<tr><td>ADJ</td><td>JJS</td><td>adjective, superlative</td><td>This is the **best** sentence.</td></tr>\n","<tr><td>ADJ</td><td>PDT</td><td>predeterminer</td><td>Waking up is **half** the battle.</td></tr>\n","<tr><td>ADJ</td><td>PRP\\$</td><td>pronoun, possessive</td><td>**His** arm hurts.</td></tr>\n","<tr><td>ADJ</td><td>WDT</td><td>wh-determiner</td><td>It's blue, **which** is odd.</td></tr>\n","<tr><td>ADJ</td><td>WP\\$</td><td>wh-pronoun, possessive</td><td>We don't know **whose** it is.</td></tr>\n","<tr><td>ADP</td><td>IN</td><td>conjunction, subordinating or preposition</td><td>It arrived **in** a box.</td></tr>\n","<tr><td>ADV</td><td>EX</td><td>existential there</td><td>**There** is cake.</td></tr>\n","<tr><td>ADV</td><td>RB</td><td>adverb</td><td>He ran **quickly**.</td></tr>\n","<tr><td>ADV</td><td>RBR</td><td>adverb, comparative</td><td>He ran **quicker**.</td></tr>\n","<tr><td>ADV</td><td>RBS</td><td>adverb, superlative</td><td>He ran **fastest**.</td></tr>\n","<tr><td>ADV</td><td>WRB</td><td>wh-adverb</td><td>**When** was that?</td></tr>\n","<tr><td>CONJ</td><td>CC</td><td>conjunction, coordinating</td><td>The balloon popped **and** everyone jumped.</td></tr>\n","<tr><td>DET</td><td>DT</td><td>determiner</td><td>**This** is **a** sentence.</td></tr>\n","<tr><td>INTJ</td><td>UH</td><td>interjection</td><td>**Um**, I don't know.</td></tr>\n","<tr><td>NOUN</td><td>NN</td><td>noun, singular or mass</td><td>This is a **sentence**.</td></tr>\n","<tr><td>NOUN</td><td>NNS</td><td>noun, plural</td><td>These are **words**.</td></tr>\n","<tr><td>NOUN</td><td>WP</td><td>wh-pronoun, personal</td><td>**Who** was that?</td></tr>\n","<tr><td>NUM</td><td>CD</td><td>cardinal number</td><td>I want **three** things.</td></tr>\n","<tr><td>PART</td><td>POS</td><td>possessive ending</td><td>Fred**'s** name is short.</td></tr>\n","<tr><td>PART</td><td>RP</td><td>adverb, particle</td><td>Put it **back**!</td></tr>\n","<tr><td>PART</td><td>TO</td><td>infinitival to</td><td>I want **to** go.</td></tr>\n","<tr><td>PRON</td><td>PRP</td><td>pronoun, personal</td><td>**I** want **you** to go.</td></tr>\n","<tr><td>PROPN</td><td>NNP</td><td>noun, proper singular</td><td>**Kilroy** was here.</td></tr>\n","<tr><td>PROPN</td><td>NNPS</td><td>noun, proper plural</td><td>The **Flintstones** were a pre-historic family.</td></tr>\n","<tr><td>VERB</td><td>MD</td><td>verb, modal auxiliary</td><td>This **could** work.</td></tr>\n","<tr><td>VERB</td><td>VB</td><td>verb, base form</td><td>I want to **go**.</td></tr>\n","<tr><td>VERB</td><td>VBD</td><td>verb, past tense</td><td>This **was** a sentence.</td></tr>\n","<tr><td>VERB</td><td>VBG</td><td>verb, gerund or present participle</td><td>I am **going**.</td></tr>\n","<tr><td>VERB</td><td>VBN</td><td>verb, past participle</td><td>The treasure was **lost**.</td></tr>\n","<tr><td>VERB</td><td>VBP</td><td>verb, non-3rd person singular present</td><td>I **want** to go.</td></tr>\n","<tr><td>VERB</td><td>VBZ</td><td>verb, 3rd person singular present</td><td>He **wants** to go.</td></tr>\n","</table>"]},{"cell_type":"markdown","metadata":{"id":"tavn-oC7EtGd"},"source":["### Visualizing Parts of Speech\n","spaCy offers an outstanding visualizer called **displaCy**:"]},{"cell_type":"code","metadata":{"id":"CeP3IWMYEtGf"},"source":["# Perform standard imports\n","import spacy\n","nlp = spacy.load('en_core_web_sm')\n","\n","# Import the displaCy library\n","from spacy import displacy"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"FOP76Mu3EtGg"},"source":["# Create a simple Doc object\n","doc = nlp(u\"The quick brown fox jumped over the lazy dog's back.\")"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"38ovRBKhEtGi","outputId":"9a80d36e-5761-415a-9f70-9d00ed10f625"},"source":["# Render the dependency parse immediately inside Jupyter:\n","displacy.render(doc, style='dep', jupyter=True, options={'distance': 110})"],"execution_count":null,"outputs":[{"output_type":"display_data","data":{"text/html":["<svg xmlns=\"http://www.w3.org/2000/svg\" xmlns:xlink=\"http://www.w3.org/1999/xlink\" id=\"0\" class=\"displacy\" width=\"1260\" height=\"357.0\" style=\"max-width: none; height: 357.0px; color: #000000; background: #ffffff; font-family: Arial\">\n","<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"267.0\">\n","    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"50\">The</tspan>\n","    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"50\">DET</tspan>\n","</text>\n","\n","<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"267.0\">\n","    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"160\">quick</tspan>\n","    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"160\">ADJ</tspan>\n","</text>\n","\n","<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"267.0\">\n","    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"270\">brown</tspan>\n","    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"270\">ADJ</tspan>\n","</text>\n","\n","<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"267.0\">\n","    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"380\">fox</tspan>\n","    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"380\">NOUN</tspan>\n","</text>\n","\n","<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"267.0\">\n","    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"490\">jumped</tspan>\n","    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"490\">VERB</tspan>\n","</text>\n","\n","<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"267.0\">\n","    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"600\">over</tspan>\n","    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"600\">ADP</tspan>\n","</text>\n","\n","<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"267.0\">\n","    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"710\">the</tspan>\n","    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"710\">DET</tspan>\n","</text>\n","\n","<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"267.0\">\n","    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"820\">lazy</tspan>\n","    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"820\">ADJ</tspan>\n","</text>\n","\n","<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"267.0\">\n","    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"930\">dog</tspan>\n","    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"930\">NOUN</tspan>\n","</text>\n","\n","<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"267.0\">\n","    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"1040\">'s</tspan>\n","    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"1040\">PART</tspan>\n","</text>\n","\n","<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"267.0\">\n","    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"1150\">back.</tspan>\n","    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"1150\">NOUN</tspan>\n","</text>\n","\n","<g class=\"displacy-arrow\">\n","    <path class=\"displacy-arc\" id=\"arrow-0-0\" stroke-width=\"2px\" d=\"M70,222.0 C70,57.0 375.0,57.0 375.0,222.0\" fill=\"none\" stroke=\"currentColor\"/>\n","    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n","        <textPath xlink:href=\"#arrow-0-0\" class=\"displacy-label\" startOffset=\"50%\" fill=\"currentColor\" text-anchor=\"middle\">det</textPath>\n","    </text>\n","    <path class=\"displacy-arrowhead\" d=\"M70,224.0 L62,212.0 78,212.0\" fill=\"currentColor\"/>\n","</g>\n","\n","<g class=\"displacy-arrow\">\n","    <path class=\"displacy-arc\" id=\"arrow-0-1\" stroke-width=\"2px\" d=\"M180,222.0 C180,112.0 370.0,112.0 370.0,222.0\" fill=\"none\" stroke=\"currentColor\"/>\n","    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n","        <textPath xlink:href=\"#arrow-0-1\" class=\"displacy-label\" startOffset=\"50%\" fill=\"currentColor\" text-anchor=\"middle\">amod</textPath>\n","    </text>\n","    <path class=\"displacy-arrowhead\" d=\"M180,224.0 L172,212.0 188,212.0\" fill=\"currentColor\"/>\n","</g>\n","\n","<g class=\"displacy-arrow\">\n","    <path class=\"displacy-arc\" id=\"arrow-0-2\" stroke-width=\"2px\" d=\"M290,222.0 C290,167.0 365.0,167.0 365.0,222.0\" fill=\"none\" stroke=\"currentColor\"/>\n","    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n","        <textPath xlink:href=\"#arrow-0-2\" class=\"displacy-label\" startOffset=\"50%\" fill=\"currentColor\" text-anchor=\"middle\">amod</textPath>\n","    </text>\n","    <path class=\"displacy-arrowhead\" d=\"M290,224.0 L282,212.0 298,212.0\" fill=\"currentColor\"/>\n","</g>\n","\n","<g class=\"displacy-arrow\">\n","    <path class=\"displacy-arc\" id=\"arrow-0-3\" stroke-width=\"2px\" d=\"M400,222.0 C400,167.0 475.0,167.0 475.0,222.0\" fill=\"none\" stroke=\"currentColor\"/>\n","    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n","        <textPath xlink:href=\"#arrow-0-3\" class=\"displacy-label\" startOffset=\"50%\" fill=\"currentColor\" text-anchor=\"middle\">nsubj</textPath>\n","    </text>\n","    <path class=\"displacy-arrowhead\" d=\"M400,224.0 L392,212.0 408,212.0\" fill=\"currentColor\"/>\n","</g>\n","\n","<g class=\"displacy-arrow\">\n","    <path class=\"displacy-arc\" id=\"arrow-0-4\" stroke-width=\"2px\" d=\"M510,222.0 C510,167.0 585.0,167.0 585.0,222.0\" fill=\"none\" stroke=\"currentColor\"/>\n","    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n","        <textPath xlink:href=\"#arrow-0-4\" class=\"displacy-label\" startOffset=\"50%\" fill=\"currentColor\" text-anchor=\"middle\">prep</textPath>\n","    </text>\n","    <path class=\"displacy-arrowhead\" d=\"M585.0,224.0 L593.0,212.0 577.0,212.0\" fill=\"currentColor\"/>\n","</g>\n","\n","<g class=\"displacy-arrow\">\n","    <path class=\"displacy-arc\" id=\"arrow-0-5\" stroke-width=\"2px\" d=\"M730,222.0 C730,112.0 920.0,112.0 920.0,222.0\" fill=\"none\" stroke=\"currentColor\"/>\n","    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n","        <textPath xlink:href=\"#arrow-0-5\" class=\"displacy-label\" startOffset=\"50%\" fill=\"currentColor\" text-anchor=\"middle\">det</textPath>\n","    </text>\n","    <path class=\"displacy-arrowhead\" d=\"M730,224.0 L722,212.0 738,212.0\" fill=\"currentColor\"/>\n","</g>\n","\n","<g class=\"displacy-arrow\">\n","    <path class=\"displacy-arc\" id=\"arrow-0-6\" stroke-width=\"2px\" d=\"M840,222.0 C840,167.0 915.0,167.0 915.0,222.0\" fill=\"none\" stroke=\"currentColor\"/>\n","    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n","        <textPath xlink:href=\"#arrow-0-6\" class=\"displacy-label\" startOffset=\"50%\" fill=\"currentColor\" text-anchor=\"middle\">amod</textPath>\n","    </text>\n","    <path class=\"displacy-arrowhead\" d=\"M840,224.0 L832,212.0 848,212.0\" fill=\"currentColor\"/>\n","</g>\n","\n","<g class=\"displacy-arrow\">\n","    <path class=\"displacy-arc\" id=\"arrow-0-7\" stroke-width=\"2px\" d=\"M950,222.0 C950,112.0 1140.0,112.0 1140.0,222.0\" fill=\"none\" stroke=\"currentColor\"/>\n","    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n","        <textPath xlink:href=\"#arrow-0-7\" class=\"displacy-label\" startOffset=\"50%\" fill=\"currentColor\" text-anchor=\"middle\">poss</textPath>\n","    </text>\n","    <path class=\"displacy-arrowhead\" d=\"M950,224.0 L942,212.0 958,212.0\" fill=\"currentColor\"/>\n","</g>\n","\n","<g class=\"displacy-arrow\">\n","    <path class=\"displacy-arc\" id=\"arrow-0-8\" stroke-width=\"2px\" d=\"M950,222.0 C950,167.0 1025.0,167.0 1025.0,222.0\" fill=\"none\" stroke=\"currentColor\"/>\n","    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n","        <textPath xlink:href=\"#arrow-0-8\" class=\"displacy-label\" startOffset=\"50%\" fill=\"currentColor\" text-anchor=\"middle\">case</textPath>\n","    </text>\n","    <path class=\"displacy-arrowhead\" d=\"M1025.0,224.0 L1033.0,212.0 1017.0,212.0\" fill=\"currentColor\"/>\n","</g>\n","\n","<g class=\"displacy-arrow\">\n","    <path class=\"displacy-arc\" id=\"arrow-0-9\" stroke-width=\"2px\" d=\"M620,222.0 C620,2.0 1150.0,2.0 1150.0,222.0\" fill=\"none\" stroke=\"currentColor\"/>\n","    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n","        <textPath xlink:href=\"#arrow-0-9\" class=\"displacy-label\" startOffset=\"50%\" fill=\"currentColor\" text-anchor=\"middle\">pobj</textPath>\n","    </text>\n","    <path class=\"displacy-arrowhead\" d=\"M1150.0,224.0 L1158.0,212.0 1142.0,212.0\" fill=\"currentColor\"/>\n","</g>\n","</svg>"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{"tags":[]}}]},{"cell_type":"markdown","metadata":{"id":"Nh1vt2QOEtGw"},"source":["The dependency parse shows the coarse POS tag for each token, as well as the **dependency tag** if given:"]},{"cell_type":"code","metadata":{"scrolled":true,"id":"aNF57j8kEtGy","outputId":"a7c12343-8849-4a20-fb41-0622c1406f66"},"source":["for token in doc:\n","    print(f'{token.text:{10}} {token.pos_:{7}} {token.dep_:{7}} {spacy.explain(token.dep_)}')"],"execution_count":null,"outputs":[{"output_type":"stream","text":["The        DET     det     determiner\n","quick      ADJ     amod    adjectival modifier\n","brown      ADJ     amod    adjectival modifier\n","fox        NOUN    nsubj   nominal subject\n","jumped     VERB    ROOT    None\n","over       ADP     prep    prepositional modifier\n","the        DET     det     determiner\n","lazy       ADJ     amod    adjectival modifier\n","dog        NOUN    poss    possession modifier\n","'s         PART    case    None\n","back       NOUN    pobj    object of preposition\n",".          PUNCT   punct   punctuation\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"-eVPtMBXEtGz"},"source":["___\n","## Creating Visualizations Outside of Jupyter\n","If you're using another Python IDE or writing a script, you can choose to have spaCy serve up HTML separately.\n","\n","Instead of `displacy.render()`, use `displacy.serve()`:"]},{"cell_type":"code","metadata":{"id":"CdtRwVNxEtG0","outputId":"87b9a5f2-6e97-4275-b179-5098d8bbba8e"},"source":["displacy.serve(doc, style='dep', options={'distance': 110})"],"execution_count":null,"outputs":[{"output_type":"stream","text":["\n","    Serving on port 5000...\n","    Using the 'dep' visualizer\n","\n"],"name":"stdout"},{"output_type":"stream","text":["127.0.0.1 - - [12/Oct/2018 16:54:07] \"GET / HTTP/1.1\" 200 8304\n","127.0.0.1 - - [12/Oct/2018 16:54:07] \"GET /favicon.ico HTTP/1.1\" 200 8304\n"],"name":"stderr"},{"output_type":"stream","text":["\n","    Shutting down server on port 5000.\n","\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"06jfn3_bEtG2"},"source":["<font color=blue>**After running the cell above, click the link below to view the dependency parse**:</font>\n","\n","http://127.0.0.1:5000\n","<br><br>\n","<font color=red>**To shut down the server and return to jupyter**, interrupt the kernel either through the **Kernel** menu above, by hitting the black square on the toolbar, or by typing the keyboard shortcut `Esc`, `I`, `I`</font>"]},{"cell_type":"markdown","metadata":{"id":"hhnSKhenEtG3"},"source":["<font color=green>**NOTE**: We'll use this method moving forward because, at this time, several of the customizations we want to show don't work well in Jupyter.</font>"]},{"cell_type":"markdown","metadata":{"id":"Y6-bx9ENEtG3"},"source":["___\n","### Handling Large Text\n","`displacy.serve()` accepts a single Doc or list of Doc objects. Since large texts are difficult to view in one line, you may want to pass a list of spans instead. Each span will appear on its own line:"]},{"cell_type":"code","metadata":{"id":"kIc3Ivs0EtG4","outputId":"423e2a4b-ffcd-4488-ca21-280ee052053a"},"source":["doc2 = nlp(u\"This is a sentence. This is another, possibly longer sentence.\")\n","\n","# Create spans from Doc.sents:\n","spans = list(doc2.sents)\n","\n","displacy.serve(spans, style='dep', options={'distance': 110})"],"execution_count":null,"outputs":[{"output_type":"stream","text":["\n","    Serving on port 5000...\n","    Using the 'dep' visualizer\n","\n"],"name":"stdout"},{"output_type":"stream","text":["127.0.0.1 - - [12/Oct/2018 16:57:03] \"GET / HTTP/1.1\" 200 7328\n","127.0.0.1 - - [12/Oct/2018 16:57:03] \"GET /favicon.ico HTTP/1.1\" 200 7328\n"],"name":"stderr"},{"output_type":"stream","text":["\n","    Shutting down server on port 5000.\n","\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"rCKU2boTEtG6"},"source":["**Click this link to view the dependency**: http://127.0.0.1:5000\n","<br>Interrupt the kernel to return to jupyter."]},{"cell_type":"markdown","metadata":{"id":"rLej84BEEtG7"},"source":["___\n","### Customizing the Appearance\n","Besides setting the distance between tokens, you can pass other arguments to the `options` parameter:\n","\n","<table>\n","<tr><th>NAME</th><th>TYPE</th><th>DESCRIPTION</th><th>DEFAULT</th></tr>\n","<tr><td>`compact`</td><td>bool</td><td>\"Compact mode\" with square arrows that takes up less space.</td><td>`False`</td></tr>\n","<tr><td>`color`</td><td>unicode</td><td>Text color (HEX, RGB or color names).</td><td>`#000000`</td></tr>\n","<tr><td>`bg`</td><td>unicode</td><td>Background color (HEX, RGB or color names).</td><td>`#ffffff`</td></tr>\n","<tr><td>`font`</td><td>unicode</td><td>Font name or font family for all text.</td><td>`Arial`</td></tr>\n","</table>\n","\n","For a full list of options visit https://spacy.io/api/top-level#displacy_options"]},{"cell_type":"code","metadata":{"scrolled":true,"id":"Z1fqGqSWEtG8","outputId":"dd4fa2e9-72a1-48e5-f2d9-bea8d2a89053"},"source":["options = {'distance': 110, 'compact': 'True', 'color': 'yellow', 'bg': '#09a3d5', 'font': 'Times'}\n","\n","displacy.serve(doc, style='dep', options=options)"],"execution_count":null,"outputs":[{"output_type":"stream","text":["\n","    Serving on port 5000...\n","    Using the 'dep' visualizer\n","\n"],"name":"stdout"},{"output_type":"stream","text":["127.0.0.1 - - [12/Oct/2018 17:02:17] \"GET / HTTP/1.1\" 200 8533\n","127.0.0.1 - - [12/Oct/2018 17:02:17] \"GET /favicon.ico HTTP/1.1\" 200 8533\n"],"name":"stderr"},{"output_type":"stream","text":["\n","    Shutting down server on port 5000.\n","\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"KzsutDfqEtG9"},"source":["**Click this link to view the dependency**: http://127.0.0.1:5000\n","<br>Interrupt the kernel to return to jupyter."]},{"cell_type":"markdown","metadata":{"collapsed":true,"id":"JX51fF8ZEtG-"},"source":["___\n","Great! Now you should be familiar with visualizing spaCy's dependency parse. For more info on **displaCy** visit https://spacy.io/usage/visualizers\n","<br>In the next section we'll look at Named Entity Recognition, followed by displaCy's NER visualizer.\n","\n"]},{"cell_type":"code","metadata":{"id":"rGOCMRRJFV-1"},"source":["# Perform standard imports\n","import spacy\n","nlp = spacy.load('en_core_web_sm')"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"BG5wr3dcFV-2"},"source":["# Write a function to display basic entity info:\n","def show_ents(doc):\n","    if doc.ents:\n","        for ent in doc.ents:\n","            print(ent.text+' - '+ent.label_+' - '+str(spacy.explain(ent.label_)))\n","    else:\n","        print('No named entities found.')"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"scrolled":true,"id":"gIAs3bc5FV-3","outputId":"eba53544-54b7-4aa0-cbf1-d3eeda72860c"},"source":["doc = nlp(u'May I go to Washington, DC next May to see the Washington Monument?')\n","\n","show_ents(doc)"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Washington, DC - GPE - Countries, cities, states\n","next May - DATE - Absolute or relative dates or periods\n","the Washington Monument - ORG - Companies, agencies, institutions, etc.\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"iI4XaGH2FV-4"},"source":["Here we see tokens combine to form the entities `Washington, DC`, `next May` and `the Washington Monument`"]},{"cell_type":"markdown","metadata":{"id":"LoHBK60uFV-4"},"source":["### Entity annotations\n","`Doc.ents` are token spans with their own set of annotations.\n","<table>\n","<tr><td>`ent.text`</td><td>The original entity text</td></tr>\n","<tr><td>`ent.label`</td><td>The entity type's hash value</td></tr>\n","<tr><td>`ent.label_`</td><td>The entity type's string description</td></tr>\n","<tr><td>`ent.start`</td><td>The token span's *start* index position in the Doc</td></tr>\n","<tr><td>`ent.end`</td><td>The token span's *stop* index position in the Doc</td></tr>\n","<tr><td>`ent.start_char`</td><td>The entity text's *start* index position in the Doc</td></tr>\n","<tr><td>`ent.end_char`</td><td>The entity text's *stop* index position in the Doc</td></tr>\n","</table>\n","\n"]},{"cell_type":"code","metadata":{"id":"n11Vbj67FV-5","outputId":"71bf48ba-4167-466a-a1d2-364178bd2c6f"},"source":["doc = nlp(u'Can I please borrow 500 dollars from you to buy some Microsoft stock?')\n","\n","for ent in doc.ents:\n","    print(ent.text, ent.start, ent.end, ent.start_char, ent.end_char, ent.label_)"],"execution_count":null,"outputs":[{"output_type":"stream","text":["500 dollars 4 6 20 31 MONEY\n","Microsoft 11 12 53 62 ORG\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"moXX8nX4FV-5"},"source":["### NER Tags\n","Tags are accessible through the `.label_` property of an entity.\n","<table>\n","<tr><th>TYPE</th><th>DESCRIPTION</th><th>EXAMPLE</th></tr>\n","<tr><td>`PERSON`</td><td>People, including fictional.</td><td>*Fred Flintstone*</td></tr>\n","<tr><td>`NORP`</td><td>Nationalities or religious or political groups.</td><td>*The Republican Party*</td></tr>\n","<tr><td>`FAC`</td><td>Buildings, airports, highways, bridges, etc.</td><td>*Logan International Airport, The Golden Gate*</td></tr>\n","<tr><td>`ORG`</td><td>Companies, agencies, institutions, etc.</td><td>*Microsoft, FBI, MIT*</td></tr>\n","<tr><td>`GPE`</td><td>Countries, cities, states.</td><td>*France, UAR, Chicago, Idaho*</td></tr>\n","<tr><td>`LOC`</td><td>Non-GPE locations, mountain ranges, bodies of water.</td><td>*Europe, Nile River, Midwest*</td></tr>\n","<tr><td>`PRODUCT`</td><td>Objects, vehicles, foods, etc. (Not services.)</td><td>*Formula 1*</td></tr>\n","<tr><td>`EVENT`</td><td>Named hurricanes, battles, wars, sports events, etc.</td><td>*Olympic Games*</td></tr>\n","<tr><td>`WORK_OF_ART`</td><td>Titles of books, songs, etc.</td><td>*The Mona Lisa*</td></tr>\n","<tr><td>`LAW`</td><td>Named documents made into laws.</td><td>*Roe v. Wade*</td></tr>\n","<tr><td>`LANGUAGE`</td><td>Any named language.</td><td>*English*</td></tr>\n","<tr><td>`DATE`</td><td>Absolute or relative dates or periods.</td><td>*20 July 1969*</td></tr>\n","<tr><td>`TIME`</td><td>Times smaller than a day.</td><td>*Four hours*</td></tr>\n","<tr><td>`PERCENT`</td><td>Percentage, including \"%\".</td><td>*Eighty percent*</td></tr>\n","<tr><td>`MONEY`</td><td>Monetary values, including unit.</td><td>*Twenty Cents*</td></tr>\n","<tr><td>`QUANTITY`</td><td>Measurements, as of weight or distance.</td><td>*Several kilometers, 55kg*</td></tr>\n","<tr><td>`ORDINAL`</td><td>\"first\", \"second\", etc.</td><td>*9th, Ninth*</td></tr>\n","<tr><td>`CARDINAL`</td><td>Numerals that do not fall under another type.</td><td>*2, Two, Fifty-two*</td></tr>\n","</table>"]},{"cell_type":"markdown","metadata":{"id":"fl9cZ4u8FV-6"},"source":["___\n","### Adding a Named Entity to a Span\n","Normally we would have spaCy build a library of named entities by training it on several samples of text.<br>In this case, we only want to add one value:"]},{"cell_type":"code","metadata":{"id":"yudm_p3fFV-6","outputId":"12e84fbc-37ef-4921-ce42-7e12bf2f6f10"},"source":["doc = nlp(u'Tesla to build a U.K. factory for $6 million')\n","\n","show_ents(doc)"],"execution_count":null,"outputs":[{"output_type":"stream","text":["U.K. - GPE - Countries, cities, states\n","$6 million - MONEY - Monetary values, including unit\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"okH5V2qaFV-7"},"source":["<font color=green>Right now, spaCy does not recognize \"Tesla\" as a company.</font>"]},{"cell_type":"code","metadata":{"id":"__O4OF7-FV-7"},"source":["from spacy.tokens import Span\n","\n","# Get the hash value of the ORG entity label\n","ORG = doc.vocab.strings[u'ORG']  \n","\n","# Create a Span for the new entity\n","new_ent = Span(doc, 0, 1, label=ORG)\n","\n","# Add the entity to the existing Doc object\n","doc.ents = list(doc.ents) + [new_ent]"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"ybrNgpC4FV-8"},"source":["<font color=green>In the code above, the arguments passed to `Span()` are:</font>\n","-  `doc` - the name of the Doc object\n","-  `0` - the *start* index position of the span\n","-  `1` - the *stop* index position (exclusive)\n","-  `label=ORG` - the label assigned to our entity"]},{"cell_type":"code","metadata":{"scrolled":true,"id":"Be2VQBj8FV-9","outputId":"3d9c12ad-5226-4abc-8655-6902ff28b056"},"source":["show_ents(doc)"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Tesla - ORG - Companies, agencies, institutions, etc.\n","U.K. - GPE - Countries, cities, states\n","$6 million - MONEY - Monetary values, including unit\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"CBI9uWJzFV-9"},"source":["___\n","### Adding Named Entities to All Matching Spans\n","What if we want to tag *all* occurrences of \"Tesla\"? In this section we show how to use the PhraseMatcher to identify a series of spans in the Doc:"]},{"cell_type":"code","metadata":{"id":"KU0AF4E_FV--","outputId":"b634d874-b173-4510-a8d1-59b4602f74b5"},"source":["doc = nlp(u'Our company plans to introduce a new vacuum cleaner. '\n","          u'If successful, the vacuum cleaner will be our first product.')\n","\n","show_ents(doc)"],"execution_count":null,"outputs":[{"output_type":"stream","text":["first - ORDINAL - \"first\", \"second\", etc.\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"iAYfZbr9FV-_"},"source":["# Import PhraseMatcher and create a matcher object:\n","from spacy.matcher import PhraseMatcher\n","matcher = PhraseMatcher(nlp.vocab)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"UNoxrq2cFV-_"},"source":["# Create the desired phrase patterns:\n","phrase_list = ['vacuum cleaner', 'vacuum-cleaner']\n","phrase_patterns = [nlp(text) for text in phrase_list]"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"-s2sLqDyFV_A","outputId":"f4b304f0-81d3-4947-938a-fc10f9cc867a"},"source":["# Apply the patterns to our matcher object:\n","matcher.add('newproduct', None, *phrase_patterns)\n","\n","# Apply the matcher to our Doc object:\n","matches = matcher(doc)\n","\n","# See what matches occur:\n","matches"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["[(2689272359382549672, 7, 9), (2689272359382549672, 14, 16)]"]},"metadata":{"tags":[]},"execution_count":11}]},{"cell_type":"code","metadata":{"id":"zn8COg4jFV_A"},"source":["# Here we create Spans from each match, and create named entities from them:\n","from spacy.tokens import Span\n","\n","PROD = doc.vocab.strings[u'PRODUCT']\n","\n","new_ents = [Span(doc, match[1],match[2],label=PROD) for match in matches]\n","\n","doc.ents = list(doc.ents) + new_ents"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"SvA2yCMzFV_B","outputId":"33b59a20-b550-44a1-ba9d-f3572fdd0a51"},"source":["show_ents(doc)"],"execution_count":null,"outputs":[{"output_type":"stream","text":["vacuum cleaner - PRODUCT - Objects, vehicles, foods, etc. (not services)\n","vacuum cleaner - PRODUCT - Objects, vehicles, foods, etc. (not services)\n","first - ORDINAL - \"first\", \"second\", etc.\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"C-sMMvwFFV_D"},"source":["___\n","### Counting Entities\n","While spaCy may not have a built-in tool for counting entities, we can pass a conditional statement into a list comprehension:"]},{"cell_type":"code","metadata":{"id":"_U6AQWdAFV_D","outputId":"3fd78eb3-1fac-41c6-afd1-80a046e47e22"},"source":["doc = nlp(u'Originally priced at $29.50, the sweater was marked down to five dollars.')\n","\n","show_ents(doc)"],"execution_count":null,"outputs":[{"output_type":"stream","text":["29.50 - MONEY - Monetary values, including unit\n","five dollars - MONEY - Monetary values, including unit\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"Um0TqsauFV_E","outputId":"114fa842-d9bc-4cc6-aee4-d4fc258b7fe2"},"source":["len([ent for ent in doc.ents if ent.label_=='MONEY'])"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["2"]},"metadata":{"tags":[]},"execution_count":15}]},{"cell_type":"markdown","metadata":{"id":"D4tBTiJvFV_F"},"source":["### <font color=blue>Problem with Line Breaks</font>\n","\n","<div class=\"alert alert-info\" style=\"margin: 20px\">There's a <a href='https://github.com/explosion/spaCy/issues/1717'>known issue</a> with <strong>spaCy v2.0.12</strong> where some linebreaks are interpreted as `GPE` entities:</div>"]},{"cell_type":"code","metadata":{"id":"V4_6XyPwFV_F","outputId":"ac6b0894-dff5-419d-f639-09c7958edf1f"},"source":["spacy.__version__"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["'2.0.12'"]},"metadata":{"tags":[]},"execution_count":16}]},{"cell_type":"code","metadata":{"id":"wGdNLIwnFV_G","outputId":"599218e0-24a8-412a-e2b9-a1ededb13584"},"source":["doc = nlp(u'Originally priced at $29.50,\\nthe sweater was marked down to five dollars.')\n","\n","show_ents(doc)"],"execution_count":null,"outputs":[{"output_type":"stream","text":["29.50 - MONEY - Monetary values, including unit\n","\n"," - GPE - Countries, cities, states\n","five dollars - MONEY - Monetary values, including unit\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"R0pUiW4dFV_G"},"source":["#### <font color=blue>However, there is a simple fix that can be added to the nlp pipeline:</font>"]},{"cell_type":"code","metadata":{"id":"ANbiKKLgFV_H"},"source":["# Quick function to remove ents formed on whitespace:\n","def remove_whitespace_entities(doc):\n","    doc.ents = [e for e in doc.ents if not e.text.isspace()]\n","    return doc\n","\n","# Insert this into the pipeline AFTER the ner component:\n","nlp.add_pipe(remove_whitespace_entities, after='ner')"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"jZdFy2gxFV_H","outputId":"1c10e4ed-aacd-4d50-be1b-49aa9c62bc11"},"source":["# Rerun nlp on the text above, and show ents:\n","doc = nlp(u'Originally priced at $29.50,\\nthe sweater was marked down to five dollars.')\n","\n","show_ents(doc)"],"execution_count":null,"outputs":[{"output_type":"stream","text":["29.50 - MONEY - Monetary values, including unit\n","five dollars - MONEY - Monetary values, including unit\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"TCMu4aWrFV_I"},"source":["For more on **Named Entity Recognition** visit https://spacy.io/usage/linguistic-features#101"]},{"cell_type":"markdown","metadata":{"id":"gJSIb8lzFV_I"},"source":["___\n","### Noun Chunks\n","`Doc.noun_chunks` are *base noun phrases*: token spans that include the noun and words describing the noun. Noun chunks cannot be nested, cannot overlap, and do not involve prepositional phrases or relative clauses.<br>\n","Where `Doc.ents` rely on the **ner** pipeline component, `Doc.noun_chunks` are provided by the **parser**."]},{"cell_type":"markdown","metadata":{"id":"a3UkliAsFV_J"},"source":["#### `noun_chunks` components:\n","<table>\n","<tr><td>`.text`</td><td>The original noun chunk text.</td></tr>\n","<tr><td>`.root.text`</td><td>The original text of the word connecting the noun chunk to the rest of the parse.</td></tr>\n","<tr><td>`.root.dep_`</td><td>Dependency relation connecting the root to its head.</td></tr>\n","<tr><td>`.root.head.text`</td><td>The text of the root token's head.</td></tr>\n","</table>"]},{"cell_type":"code","metadata":{"id":"NkEvDuNjFV_J","outputId":"23e13eed-6b9e-46fe-e343-1008efd236f0"},"source":["doc = nlp(u\"Autonomous cars shift insurance liability toward manufacturers.\")\n","\n","for chunk in doc.noun_chunks:\n","    print(chunk.text+' - '+chunk.root.text+' - '+chunk.root.dep_+' - '+chunk.root.head.text)"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Autonomous cars - cars - nsubj - shift\n","insurance liability - liability - dobj - shift\n","manufacturers - manufacturers - pobj - toward\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"k0RPXCFQFV_K"},"source":["#### `Doc.noun_chunks` is a  generator function\n","Previously we mentioned that `Doc` objects do not retain a list of sentences, but they're available through the `Doc.sents` generator.<br>It's the same with `Doc.noun_chunks` - lists can be created if needed:"]},{"cell_type":"markdown","metadata":{"id":"3QyZnw5ZImkY"},"source":["## Named Entity Recognition (NER)\n","spaCy has an **'ner'** pipeline component that identifies token spans fitting a predetermined set of named entities. These are available as the `ents` property of a `Doc` object."]},{"cell_type":"code","metadata":{"id":"lqRtIDInImky"},"source":["# Perform standard imports\n","import spacy\n","nlp = spacy.load('en_core_web_sm')"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"bhp05UzDImkz"},"source":["# Write a function to display basic entity info:\n","def show_ents(doc):\n","    if doc.ents:\n","        for ent in doc.ents:\n","            print(ent.text+' - '+ent.label_+' - '+str(spacy.explain(ent.label_)))\n","    else:\n","        print('No named entities found.')"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"scrolled":true,"id":"U9R-KMdEImkz","outputId":"eba53544-54b7-4aa0-cbf1-d3eeda72860c"},"source":["doc = nlp(u'May I go to Washington, DC next May to see the Washington Monument?')\n","\n","show_ents(doc)"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Washington, DC - GPE - Countries, cities, states\n","next May - DATE - Absolute or relative dates or periods\n","the Washington Monument - ORG - Companies, agencies, institutions, etc.\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"K8goAuPrImk1"},"source":["Here we see tokens combine to form the entities `Washington, DC`, `next May` and `the Washington Monument`"]},{"cell_type":"markdown","metadata":{"id":"Q_cz7j61Imk2"},"source":["### Entity annotations\n","`Doc.ents` are token spans with their own set of annotations.\n","<table>\n","<tr><td>`ent.text`</td><td>The original entity text</td></tr>\n","<tr><td>`ent.label`</td><td>The entity type's hash value</td></tr>\n","<tr><td>`ent.label_`</td><td>The entity type's string description</td></tr>\n","<tr><td>`ent.start`</td><td>The token span's *start* index position in the Doc</td></tr>\n","<tr><td>`ent.end`</td><td>The token span's *stop* index position in the Doc</td></tr>\n","<tr><td>`ent.start_char`</td><td>The entity text's *start* index position in the Doc</td></tr>\n","<tr><td>`ent.end_char`</td><td>The entity text's *stop* index position in the Doc</td></tr>\n","</table>\n","\n"]},{"cell_type":"code","metadata":{"id":"E8HHyhhWImk3","outputId":"71bf48ba-4167-466a-a1d2-364178bd2c6f"},"source":["doc = nlp(u'Can I please borrow 500 dollars from you to buy some Microsoft stock?')\n","\n","for ent in doc.ents:\n","    print(ent.text, ent.start, ent.end, ent.start_char, ent.end_char, ent.label_)"],"execution_count":null,"outputs":[{"output_type":"stream","text":["500 dollars 4 6 20 31 MONEY\n","Microsoft 11 12 53 62 ORG\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"AZilPyGXImk4"},"source":["### NER Tags\n","Tags are accessible through the `.label_` property of an entity.\n","<table>\n","<tr><th>TYPE</th><th>DESCRIPTION</th><th>EXAMPLE</th></tr>\n","<tr><td>`PERSON`</td><td>People, including fictional.</td><td>*Fred Flintstone*</td></tr>\n","<tr><td>`NORP`</td><td>Nationalities or religious or political groups.</td><td>*The Republican Party*</td></tr>\n","<tr><td>`FAC`</td><td>Buildings, airports, highways, bridges, etc.</td><td>*Logan International Airport, The Golden Gate*</td></tr>\n","<tr><td>`ORG`</td><td>Companies, agencies, institutions, etc.</td><td>*Microsoft, FBI, MIT*</td></tr>\n","<tr><td>`GPE`</td><td>Countries, cities, states.</td><td>*France, UAR, Chicago, Idaho*</td></tr>\n","<tr><td>`LOC`</td><td>Non-GPE locations, mountain ranges, bodies of water.</td><td>*Europe, Nile River, Midwest*</td></tr>\n","<tr><td>`PRODUCT`</td><td>Objects, vehicles, foods, etc. (Not services.)</td><td>*Formula 1*</td></tr>\n","<tr><td>`EVENT`</td><td>Named hurricanes, battles, wars, sports events, etc.</td><td>*Olympic Games*</td></tr>\n","<tr><td>`WORK_OF_ART`</td><td>Titles of books, songs, etc.</td><td>*The Mona Lisa*</td></tr>\n","<tr><td>`LAW`</td><td>Named documents made into laws.</td><td>*Roe v. Wade*</td></tr>\n","<tr><td>`LANGUAGE`</td><td>Any named language.</td><td>*English*</td></tr>\n","<tr><td>`DATE`</td><td>Absolute or relative dates or periods.</td><td>*20 July 1969*</td></tr>\n","<tr><td>`TIME`</td><td>Times smaller than a day.</td><td>*Four hours*</td></tr>\n","<tr><td>`PERCENT`</td><td>Percentage, including \"%\".</td><td>*Eighty percent*</td></tr>\n","<tr><td>`MONEY`</td><td>Monetary values, including unit.</td><td>*Twenty Cents*</td></tr>\n","<tr><td>`QUANTITY`</td><td>Measurements, as of weight or distance.</td><td>*Several kilometers, 55kg*</td></tr>\n","<tr><td>`ORDINAL`</td><td>\"first\", \"second\", etc.</td><td>*9th, Ninth*</td></tr>\n","<tr><td>`CARDINAL`</td><td>Numerals that do not fall under another type.</td><td>*2, Two, Fifty-two*</td></tr>\n","</table>"]},{"cell_type":"markdown","metadata":{"id":"4BhWsk7aImk4"},"source":["___\n","### Adding a Named Entity to a Span\n","Normally we would have spaCy build a library of named entities by training it on several samples of text.<br>In this case, we only want to add one value:"]},{"cell_type":"code","metadata":{"id":"BwZsY8D7Imk7","outputId":"12e84fbc-37ef-4921-ce42-7e12bf2f6f10"},"source":["doc = nlp(u'Tesla to build a U.K. factory for $6 million')\n","\n","show_ents(doc)"],"execution_count":null,"outputs":[{"output_type":"stream","text":["U.K. - GPE - Countries, cities, states\n","$6 million - MONEY - Monetary values, including unit\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"6nV-LFALImk8"},"source":["<font color=green>Right now, spaCy does not recognize \"Tesla\" as a company.</font>"]},{"cell_type":"code","metadata":{"id":"iFNY0gcvImk9"},"source":["from spacy.tokens import Span\n","\n","# Get the hash value of the ORG entity label\n","ORG = doc.vocab.strings[u'ORG']  \n","\n","# Create a Span for the new entity\n","new_ent = Span(doc, 0, 1, label=ORG)\n","\n","# Add the entity to the existing Doc object\n","doc.ents = list(doc.ents) + [new_ent]"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"fYRQnOIaImk9"},"source":["<font color=green>In the code above, the arguments passed to `Span()` are:</font>\n","-  `doc` - the name of the Doc object\n","-  `0` - the *start* index position of the span\n","-  `1` - the *stop* index position (exclusive)\n","-  `label=ORG` - the label assigned to our entity"]},{"cell_type":"code","metadata":{"scrolled":true,"id":"vndQrkWUImk-","outputId":"3d9c12ad-5226-4abc-8655-6902ff28b056"},"source":["show_ents(doc)"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Tesla - ORG - Companies, agencies, institutions, etc.\n","U.K. - GPE - Countries, cities, states\n","$6 million - MONEY - Monetary values, including unit\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"6EMH_xsPImk_"},"source":["___\n","### Adding Named Entities to All Matching Spans\n","What if we want to tag *all* occurrences of \"Tesla\"? In this section we show how to use the PhraseMatcher to identify a series of spans in the Doc:"]},{"cell_type":"code","metadata":{"id":"ilapMLfoImk_","outputId":"b634d874-b173-4510-a8d1-59b4602f74b5"},"source":["doc = nlp(u'Our company plans to introduce a new vacuum cleaner. '\n","          u'If successful, the vacuum cleaner will be our first product.')\n","\n","show_ents(doc)"],"execution_count":null,"outputs":[{"output_type":"stream","text":["first - ORDINAL - \"first\", \"second\", etc.\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"ceHsOoycImlA"},"source":["# Import PhraseMatcher and create a matcher object:\n","from spacy.matcher import PhraseMatcher\n","matcher = PhraseMatcher(nlp.vocab)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"i7hMdSapImlA"},"source":["# Create the desired phrase patterns:\n","phrase_list = ['vacuum cleaner', 'vacuum-cleaner']\n","phrase_patterns = [nlp(text) for text in phrase_list]"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"yQnumN2LImlA","outputId":"f4b304f0-81d3-4947-938a-fc10f9cc867a"},"source":["# Apply the patterns to our matcher object:\n","matcher.add('newproduct', None, *phrase_patterns)\n","\n","# Apply the matcher to our Doc object:\n","matches = matcher(doc)\n","\n","# See what matches occur:\n","matches"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["[(2689272359382549672, 7, 9), (2689272359382549672, 14, 16)]"]},"metadata":{"tags":[]},"execution_count":11}]},{"cell_type":"code","metadata":{"id":"yovkCF86ImlB"},"source":["# Here we create Spans from each match, and create named entities from them:\n","from spacy.tokens import Span\n","\n","PROD = doc.vocab.strings[u'PRODUCT']\n","\n","new_ents = [Span(doc, match[1],match[2],label=PROD) for match in matches]\n","\n","doc.ents = list(doc.ents) + new_ents"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"UGo9XqZIImlB","outputId":"33b59a20-b550-44a1-ba9d-f3572fdd0a51"},"source":["show_ents(doc)"],"execution_count":null,"outputs":[{"output_type":"stream","text":["vacuum cleaner - PRODUCT - Objects, vehicles, foods, etc. (not services)\n","vacuum cleaner - PRODUCT - Objects, vehicles, foods, etc. (not services)\n","first - ORDINAL - \"first\", \"second\", etc.\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"GFS8YcMkImlC"},"source":["___\n","### Counting Entities\n","While spaCy may not have a built-in tool for counting entities, we can pass a conditional statement into a list comprehension:"]},{"cell_type":"code","metadata":{"id":"48g2HUbpImlC","outputId":"3fd78eb3-1fac-41c6-afd1-80a046e47e22"},"source":["doc = nlp(u'Originally priced at $29.50, the sweater was marked down to five dollars.')\n","\n","show_ents(doc)"],"execution_count":null,"outputs":[{"output_type":"stream","text":["29.50 - MONEY - Monetary values, including unit\n","five dollars - MONEY - Monetary values, including unit\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"Y8jsK31jImlC","outputId":"114fa842-d9bc-4cc6-aee4-d4fc258b7fe2"},"source":["len([ent for ent in doc.ents if ent.label_=='MONEY'])"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["2"]},"metadata":{"tags":[]},"execution_count":15}]},{"cell_type":"markdown","metadata":{"id":"IE5lrVndImlD"},"source":["### <font color=blue>Problem with Line Breaks</font>\n","\n","<div class=\"alert alert-info\" style=\"margin: 20px\">There's a <a href='https://github.com/explosion/spaCy/issues/1717'>known issue</a> with <strong>spaCy v2.0.12</strong> where some linebreaks are interpreted as `GPE` entities:</div>"]},{"cell_type":"code","metadata":{"id":"gIv2odsYImlD","outputId":"ac6b0894-dff5-419d-f639-09c7958edf1f"},"source":["spacy.__version__"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["'2.0.12'"]},"metadata":{"tags":[]},"execution_count":16}]},{"cell_type":"code","metadata":{"id":"lG2TouRPImlD","outputId":"599218e0-24a8-412a-e2b9-a1ededb13584"},"source":["doc = nlp(u'Originally priced at $29.50,\\nthe sweater was marked down to five dollars.')\n","\n","show_ents(doc)"],"execution_count":null,"outputs":[{"output_type":"stream","text":["29.50 - MONEY - Monetary values, including unit\n","\n"," - GPE - Countries, cities, states\n","five dollars - MONEY - Monetary values, including unit\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"fjsEGRleImlE"},"source":["#### <font color=blue>However, there is a simple fix that can be added to the nlp pipeline:</font>"]},{"cell_type":"code","metadata":{"id":"F3XLKdNkImlE"},"source":["# Quick function to remove ents formed on whitespace:\n","def remove_whitespace_entities(doc):\n","    doc.ents = [e for e in doc.ents if not e.text.isspace()]\n","    return doc\n","\n","# Insert this into the pipeline AFTER the ner component:\n","nlp.add_pipe(remove_whitespace_entities, after='ner')"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"JAWg7F2DImlE","outputId":"1c10e4ed-aacd-4d50-be1b-49aa9c62bc11"},"source":["# Rerun nlp on the text above, and show ents:\n","doc = nlp(u'Originally priced at $29.50,\\nthe sweater was marked down to five dollars.')\n","\n","show_ents(doc)"],"execution_count":null,"outputs":[{"output_type":"stream","text":["29.50 - MONEY - Monetary values, including unit\n","five dollars - MONEY - Monetary values, including unit\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"jLXmOfFVImlE"},"source":["For more on **Named Entity Recognition** visit https://spacy.io/usage/linguistic-features#101"]},{"cell_type":"markdown","metadata":{"id":"wZQRkSvkImlE"},"source":["___\n","### Noun Chunks\n","`Doc.noun_chunks` are *base noun phrases*: token spans that include the noun and words describing the noun. Noun chunks cannot be nested, cannot overlap, and do not involve prepositional phrases or relative clauses.<br>\n","Where `Doc.ents` rely on the **ner** pipeline component, `Doc.noun_chunks` are provided by the **parser**."]},{"cell_type":"markdown","metadata":{"id":"ov6u0ENXImlE"},"source":["#### `noun_chunks` components:\n","<table>\n","<tr><td>`.text`</td><td>The original noun chunk text.</td></tr>\n","<tr><td>`.root.text`</td><td>The original text of the word connecting the noun chunk to the rest of the parse.</td></tr>\n","<tr><td>`.root.dep_`</td><td>Dependency relation connecting the root to its head.</td></tr>\n","<tr><td>`.root.head.text`</td><td>The text of the root token's head.</td></tr>\n","</table>"]},{"cell_type":"code","metadata":{"id":"wGyu12wSImlF","outputId":"23e13eed-6b9e-46fe-e343-1008efd236f0"},"source":["doc = nlp(u\"Autonomous cars shift insurance liability toward manufacturers.\")\n","\n","for chunk in doc.noun_chunks:\n","    print(chunk.text+' - '+chunk.root.text+' - '+chunk.root.dep_+' - '+chunk.root.head.text)"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Autonomous cars - cars - nsubj - shift\n","insurance liability - liability - dobj - shift\n","manufacturers - manufacturers - pobj - toward\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"MC-bT6dBImlF"},"source":["#### `Doc.noun_chunks` is a  generator function\n","Previously we mentioned that `Doc` objects do not retain a list of sentences, but they're available through the `Doc.sents` generator.<br>It's the same with `Doc.noun_chunks` - lists can be created if needed:"]},{"cell_type":"code","metadata":{"id":"epE-C7tLImlF","outputId":"ddd9c91b-a254-4b97-9a1c-f4f923a5c06e"},"source":["len(doc.noun_chunks)"],"execution_count":null,"outputs":[{"output_type":"error","ename":"TypeError","evalue":"object of type 'generator' has no len()","traceback":["\u001b[1;31m---------------------------------------------------------------------------\u001b[0m","\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)","\u001b[1;32m<ipython-input-21-8b52b37c204e>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdoc\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnoun_chunks\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m","\u001b[1;31mTypeError\u001b[0m: object of type 'generator' has no len()"]}]},{"cell_type":"code","metadata":{"id":"6r_yavB7ImlF","outputId":"e345187a-7402-4590-8200-2e76921192c6"},"source":["len(list(doc.noun_chunks))"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["3"]},"metadata":{"tags":[]},"execution_count":22}]},{"cell_type":"markdown","metadata":{"id":"4DQYKUKvImlF"},"source":["For more on **noun_chunks** visit https://spacy.io/usage/linguistic-features#noun-chunks"]},{"cell_type":"code","metadata":{"id":"CAJVYP7xFV_K","outputId":"ddd9c91b-a254-4b97-9a1c-f4f923a5c06e"},"source":["len(doc.noun_chunks)"],"execution_count":null,"outputs":[{"output_type":"error","ename":"TypeError","evalue":"object of type 'generator' has no len()","traceback":["\u001b[1;31m---------------------------------------------------------------------------\u001b[0m","\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)","\u001b[1;32m<ipython-input-21-8b52b37c204e>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdoc\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnoun_chunks\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m","\u001b[1;31mTypeError\u001b[0m: object of type 'generator' has no len()"]}]},{"cell_type":"code","metadata":{"id":"vABPMAvXFV_K","outputId":"e345187a-7402-4590-8200-2e76921192c6"},"source":["len(list(doc.noun_chunks))"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["3"]},"metadata":{"tags":[]},"execution_count":22}]},{"cell_type":"markdown","metadata":{"id":"kAGIrwmVFV_L"},"source":["For more on **noun_chunks** visit https://spacy.io/usage/linguistic-features#noun-chunks"]},{"cell_type":"markdown","metadata":{"id":"2ECVN1UlIjHt"},"source":["### Visualizing Named Entities\n","Besides viewing Part of Speech dependencies with `style='dep'`, **displaCy** offers a `style='ent'` visualizer:"]},{"cell_type":"code","metadata":{"id":"0tPK6ZuGIjHt"},"source":["# Perform standard imports\n","import spacy\n","nlp = spacy.load('en_core_web_sm')\n","\n","# Import the displaCy library\n","from spacy import displacy"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"P08CWIWWIjHu","outputId":"016e7684-8d13-4d13-cf2e-f684a01934c2"},"source":["doc = nlp(u'Over the last quarter Apple sold nearly 20 thousand iPods for a profit of $6 million. '\n","         u'By contrast, Sony sold only 7 thousand Walkman music players.')\n","\n","displacy.render(doc, style='ent', jupyter=True)"],"execution_count":null,"outputs":[{"output_type":"display_data","data":{"text/html":["<div class=\"entities\" style=\"line-height: 2.5\">Over \n","<mark class=\"entity\" style=\"background: #bfe1d9; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em; box-decoration-break: clone; -webkit-box-decoration-break: clone\">\n","    the last quarter\n","    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; text-transform: uppercase; vertical-align: middle; margin-left: 0.5rem\">DATE</span>\n","</mark>\n"," \n","<mark class=\"entity\" style=\"background: #7aecec; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em; box-decoration-break: clone; -webkit-box-decoration-break: clone\">\n","    Apple\n","    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; text-transform: uppercase; vertical-align: middle; margin-left: 0.5rem\">ORG</span>\n","</mark>\n"," sold \n","<mark class=\"entity\" style=\"background: #e4e7d2; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em; box-decoration-break: clone; -webkit-box-decoration-break: clone\">\n","    nearly 20 thousand\n","    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; text-transform: uppercase; vertical-align: middle; margin-left: 0.5rem\">CARDINAL</span>\n","</mark>\n"," \n","<mark class=\"entity\" style=\"background: #bfeeb7; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em; box-decoration-break: clone; -webkit-box-decoration-break: clone\">\n","    iPods\n","    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; text-transform: uppercase; vertical-align: middle; margin-left: 0.5rem\">PRODUCT</span>\n","</mark>\n"," for a profit of \n","<mark class=\"entity\" style=\"background: #e4e7d2; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em; box-decoration-break: clone; -webkit-box-decoration-break: clone\">\n","    $6 million\n","    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; text-transform: uppercase; vertical-align: middle; margin-left: 0.5rem\">MONEY</span>\n","</mark>\n",". By contrast, \n","<mark class=\"entity\" style=\"background: #7aecec; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em; box-decoration-break: clone; -webkit-box-decoration-break: clone\">\n","    Sony\n","    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; text-transform: uppercase; vertical-align: middle; margin-left: 0.5rem\">ORG</span>\n","</mark>\n"," sold \n","<mark class=\"entity\" style=\"background: #e4e7d2; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em; box-decoration-break: clone; -webkit-box-decoration-break: clone\">\n","    only 7 thousand\n","    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; text-transform: uppercase; vertical-align: middle; margin-left: 0.5rem\">CARDINAL</span>\n","</mark>\n"," \n","<mark class=\"entity\" style=\"background: #bfeeb7; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em; box-decoration-break: clone; -webkit-box-decoration-break: clone\">\n","    Walkman\n","    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; text-transform: uppercase; vertical-align: middle; margin-left: 0.5rem\">PRODUCT</span>\n","</mark>\n"," music players.</div>"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{"tags":[]}}]},{"cell_type":"markdown","metadata":{"id":"EhKCYlXzIjHw"},"source":["___\n","#### Viewing Sentences Line by Line\n","Unlike the **displaCy** dependency parse, the NER viewer has to take in a Doc object with an `ents` attribute. For this reason, we can't just pass a list of spans to `.render()`, we have to create a new Doc from each `span.text`:"]},{"cell_type":"code","metadata":{"id":"v_Oq_8iKIjHw","outputId":"4a761a6d-0ac0-4b5e-d6fe-f380ac55a195"},"source":["for sent in doc.sents:\n","    displacy.render(nlp(sent.text), style='ent', jupyter=True)"],"execution_count":null,"outputs":[{"output_type":"display_data","data":{"text/html":["<div class=\"entities\" style=\"line-height: 2.5\">Over \n","<mark class=\"entity\" style=\"background: #bfe1d9; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em; box-decoration-break: clone; -webkit-box-decoration-break: clone\">\n","    the last quarter\n","    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; text-transform: uppercase; vertical-align: middle; margin-left: 0.5rem\">DATE</span>\n","</mark>\n"," \n","<mark class=\"entity\" style=\"background: #7aecec; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em; box-decoration-break: clone; -webkit-box-decoration-break: clone\">\n","    Apple\n","    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; text-transform: uppercase; vertical-align: middle; margin-left: 0.5rem\">ORG</span>\n","</mark>\n"," sold \n","<mark class=\"entity\" style=\"background: #e4e7d2; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em; box-decoration-break: clone; -webkit-box-decoration-break: clone\">\n","    nearly 20 thousand\n","    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; text-transform: uppercase; vertical-align: middle; margin-left: 0.5rem\">CARDINAL</span>\n","</mark>\n"," \n","<mark class=\"entity\" style=\"background: #bfeeb7; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em; box-decoration-break: clone; -webkit-box-decoration-break: clone\">\n","    iPods\n","    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; text-transform: uppercase; vertical-align: middle; margin-left: 0.5rem\">PRODUCT</span>\n","</mark>\n"," for a profit of \n","<mark class=\"entity\" style=\"background: #e4e7d2; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em; box-decoration-break: clone; -webkit-box-decoration-break: clone\">\n","    $6 million\n","    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; text-transform: uppercase; vertical-align: middle; margin-left: 0.5rem\">MONEY</span>\n","</mark>\n",".</div>"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{"tags":[]}},{"output_type":"display_data","data":{"text/html":["<div class=\"entities\" style=\"line-height: 2.5\">By contrast, \n","<mark class=\"entity\" style=\"background: #7aecec; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em; box-decoration-break: clone; -webkit-box-decoration-break: clone\">\n","    Sony\n","    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; text-transform: uppercase; vertical-align: middle; margin-left: 0.5rem\">ORG</span>\n","</mark>\n"," sold \n","<mark class=\"entity\" style=\"background: #e4e7d2; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em; box-decoration-break: clone; -webkit-box-decoration-break: clone\">\n","    only 7 thousand\n","    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; text-transform: uppercase; vertical-align: middle; margin-left: 0.5rem\">CARDINAL</span>\n","</mark>\n"," \n","<mark class=\"entity\" style=\"background: #bfeeb7; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em; box-decoration-break: clone; -webkit-box-decoration-break: clone\">\n","    Walkman\n","    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; text-transform: uppercase; vertical-align: middle; margin-left: 0.5rem\">PRODUCT</span>\n","</mark>\n"," music players.</div>"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{"tags":[]}}]},{"cell_type":"markdown","metadata":{"id":"QcIvcsCZIjHx"},"source":["<div class=\"alert alert-info\"><font color=black>**NOTE**: If a span does not contain any entities, displaCy will issue a harmless warning:</font></div>"]},{"cell_type":"code","metadata":{"id":"gxdpx5_LIjHx"},"source":["doc2 = nlp(u'Over the last quarter Apple sold nearly 20 thousand iPods for a profit of $6 million. '\n","         u'By contrast, my kids sold a lot of lemonade.')"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"MxpaOrpdIjHx","outputId":"20dda23b-b1c2-42a6-e6a0-c5b04e26e680"},"source":["for sent in doc2.sents:\n","    displacy.render(nlp(sent.text), style='ent', jupyter=True)"],"execution_count":null,"outputs":[{"output_type":"display_data","data":{"text/html":["<div class=\"entities\" style=\"line-height: 2.5\">Over \n","<mark class=\"entity\" style=\"background: #bfe1d9; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em; box-decoration-break: clone; -webkit-box-decoration-break: clone\">\n","    the last quarter\n","    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; text-transform: uppercase; vertical-align: middle; margin-left: 0.5rem\">DATE</span>\n","</mark>\n"," \n","<mark class=\"entity\" style=\"background: #7aecec; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em; box-decoration-break: clone; -webkit-box-decoration-break: clone\">\n","    Apple\n","    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; text-transform: uppercase; vertical-align: middle; margin-left: 0.5rem\">ORG</span>\n","</mark>\n"," sold \n","<mark class=\"entity\" style=\"background: #e4e7d2; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em; box-decoration-break: clone; -webkit-box-decoration-break: clone\">\n","    nearly 20 thousand\n","    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; text-transform: uppercase; vertical-align: middle; margin-left: 0.5rem\">CARDINAL</span>\n","</mark>\n"," \n","<mark class=\"entity\" style=\"background: #bfeeb7; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em; box-decoration-break: clone; -webkit-box-decoration-break: clone\">\n","    iPods\n","    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; text-transform: uppercase; vertical-align: middle; margin-left: 0.5rem\">PRODUCT</span>\n","</mark>\n"," for a profit of \n","<mark class=\"entity\" style=\"background: #e4e7d2; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em; box-decoration-break: clone; -webkit-box-decoration-break: clone\">\n","    $6 million\n","    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; text-transform: uppercase; vertical-align: middle; margin-left: 0.5rem\">MONEY</span>\n","</mark>\n",".</div>"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{"tags":[]}},{"output_type":"stream","text":["C:\\Anaconda3\\lib\\runpy.py:193: UserWarning: [W006] No entities to visualize found in Doc object. If this is surprising to you, make sure the Doc was processed using a model that supports named entity recognition, and check the `doc.ents` property manually if necessary.\n","  \"__main__\", mod_spec)\n"],"name":"stderr"},{"output_type":"display_data","data":{"text/html":["<div class=\"entities\" style=\"line-height: 2.5\">By contrast, my kids sold a lot of lemonade.</div>"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{"tags":[]}}]},{"cell_type":"markdown","metadata":{"id":"EmSx6LISIjHy"},"source":["<div class=\"alert alert-info\"><font color=black>**WORKAROUND:** We can avert this with an additional bit of code:</font></div>"]},{"cell_type":"code","metadata":{"id":"jD4GFUZMIjHy","outputId":"aa206c03-a799-4da5-904e-a29875c6f264"},"source":["for sent in doc2.sents:\n","    docx = nlp(sent.text)\n","    if docx.ents:\n","        displacy.render(docx, style='ent', jupyter=True)\n","    else:\n","        print(docx.text)"],"execution_count":null,"outputs":[{"output_type":"display_data","data":{"text/html":["<div class=\"entities\" style=\"line-height: 2.5\">Over \n","<mark class=\"entity\" style=\"background: #bfe1d9; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em; box-decoration-break: clone; -webkit-box-decoration-break: clone\">\n","    the last quarter\n","    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; text-transform: uppercase; vertical-align: middle; margin-left: 0.5rem\">DATE</span>\n","</mark>\n"," \n","<mark class=\"entity\" style=\"background: #7aecec; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em; box-decoration-break: clone; -webkit-box-decoration-break: clone\">\n","    Apple\n","    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; text-transform: uppercase; vertical-align: middle; margin-left: 0.5rem\">ORG</span>\n","</mark>\n"," sold \n","<mark class=\"entity\" style=\"background: #e4e7d2; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em; box-decoration-break: clone; -webkit-box-decoration-break: clone\">\n","    nearly 20 thousand\n","    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; text-transform: uppercase; vertical-align: middle; margin-left: 0.5rem\">CARDINAL</span>\n","</mark>\n"," \n","<mark class=\"entity\" style=\"background: #bfeeb7; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em; box-decoration-break: clone; -webkit-box-decoration-break: clone\">\n","    iPods\n","    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; text-transform: uppercase; vertical-align: middle; margin-left: 0.5rem\">PRODUCT</span>\n","</mark>\n"," for a profit of \n","<mark class=\"entity\" style=\"background: #e4e7d2; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em; box-decoration-break: clone; -webkit-box-decoration-break: clone\">\n","    $6 million\n","    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; text-transform: uppercase; vertical-align: middle; margin-left: 0.5rem\">MONEY</span>\n","</mark>\n",".</div>"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{"tags":[]}},{"output_type":"stream","text":["By contrast, my kids sold a lot of lemonade.\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"FA8bs-YTIjHz"},"source":["___\n","#### Viewing Specific Entities\n","You can pass a list of entity types to restrict the visualization:"]},{"cell_type":"code","metadata":{"id":"l9oCoDvuIjHz","outputId":"26e451c0-5dc7-49f9-9d9a-7fb8b4069637"},"source":["options = {'ents': ['ORG', 'PRODUCT']}\n","\n","displacy.render(doc, style='ent', jupyter=True, options=options)"],"execution_count":null,"outputs":[{"output_type":"display_data","data":{"text/html":["<div class=\"entities\" style=\"line-height: 2.5\">Over the last quarter \n","<mark class=\"entity\" style=\"background: #7aecec; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em; box-decoration-break: clone; -webkit-box-decoration-break: clone\">\n","    Apple\n","    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; text-transform: uppercase; vertical-align: middle; margin-left: 0.5rem\">ORG</span>\n","</mark>\n"," sold nearly 20 thousand \n","<mark class=\"entity\" style=\"background: #bfeeb7; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em; box-decoration-break: clone; -webkit-box-decoration-break: clone\">\n","    iPods\n","    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; text-transform: uppercase; vertical-align: middle; margin-left: 0.5rem\">PRODUCT</span>\n","</mark>\n"," for a profit of $6 million. By contrast, \n","<mark class=\"entity\" style=\"background: #7aecec; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em; box-decoration-break: clone; -webkit-box-decoration-break: clone\">\n","    Sony\n","    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; text-transform: uppercase; vertical-align: middle; margin-left: 0.5rem\">ORG</span>\n","</mark>\n"," sold only 7 thousand \n","<mark class=\"entity\" style=\"background: #bfeeb7; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em; box-decoration-break: clone; -webkit-box-decoration-break: clone\">\n","    Walkman\n","    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; text-transform: uppercase; vertical-align: middle; margin-left: 0.5rem\">PRODUCT</span>\n","</mark>\n"," music players.</div>"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{"tags":[]}}]},{"cell_type":"markdown","metadata":{"id":"oZaWDXy1IjH0"},"source":["___\n","#### Customizing Colors and Effects\n","You can also pass background color and gradient options:"]},{"cell_type":"code","metadata":{"id":"D7YnkZZ8IjH0","outputId":"a0784a99-4fc5-4541-edd7-5731bccaa76d"},"source":["colors = {'ORG': 'linear-gradient(90deg, #aa9cfc, #fc9ce7)', 'PRODUCT': 'radial-gradient(yellow, green)'}\n","\n","options = {'ents': ['ORG', 'PRODUCT'], 'colors':colors}\n","\n","displacy.render(doc, style='ent', jupyter=True, options=options)"],"execution_count":null,"outputs":[{"output_type":"display_data","data":{"text/html":["<div class=\"entities\" style=\"line-height: 2.5\">Over the last quarter \n","<mark class=\"entity\" style=\"background: linear-gradient(90deg, #aa9cfc, #fc9ce7); padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em; box-decoration-break: clone; -webkit-box-decoration-break: clone\">\n","    Apple\n","    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; text-transform: uppercase; vertical-align: middle; margin-left: 0.5rem\">ORG</span>\n","</mark>\n"," sold nearly 20 thousand \n","<mark class=\"entity\" style=\"background: radial-gradient(yellow, green); padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em; box-decoration-break: clone; -webkit-box-decoration-break: clone\">\n","    iPods\n","    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; text-transform: uppercase; vertical-align: middle; margin-left: 0.5rem\">PRODUCT</span>\n","</mark>\n"," for a profit of $6 million. By contrast, \n","<mark class=\"entity\" style=\"background: linear-gradient(90deg, #aa9cfc, #fc9ce7); padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em; box-decoration-break: clone; -webkit-box-decoration-break: clone\">\n","    Sony\n","    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; text-transform: uppercase; vertical-align: middle; margin-left: 0.5rem\">ORG</span>\n","</mark>\n"," sold only 7 thousand \n","<mark class=\"entity\" style=\"background: radial-gradient(yellow, green); padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em; box-decoration-break: clone; -webkit-box-decoration-break: clone\">\n","    Walkman\n","    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; text-transform: uppercase; vertical-align: middle; margin-left: 0.5rem\">PRODUCT</span>\n","</mark>\n"," music players.</div>"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{"tags":[]}}]},{"cell_type":"markdown","metadata":{"id":"0j6JgEgXIjH1"},"source":["For more on applying CSS background colors and gradients, visit https://www.w3schools.com/css/css3_gradients.asp"]},{"cell_type":"markdown","metadata":{"id":"WzrYFYH5IjH1"},"source":["___\n","### Creating Visualizations Outside of Jupyter\n","If you're using another Python IDE or writing a script, you can choose to have spaCy serve up HTML separately.\n","\n","Instead of `displacy.render()`, use `displacy.serve()`:"]},{"cell_type":"code","metadata":{"id":"2ch3C7ghIjH2","outputId":"a6221c2a-e21e-4acf-b279-4e4a40511295"},"source":["displacy.serve(doc, style='ent', options=options)"],"execution_count":null,"outputs":[{"output_type":"stream","text":["\n","    Serving on port 5000...\n","    Using the 'ent' visualizer\n","\n"],"name":"stdout"},{"output_type":"stream","text":["127.0.0.1 - - [04/Dec/2018 13:21:26] \"GET / HTTP/1.1\" 200 2210\n","127.0.0.1 - - [04/Dec/2018 13:21:26] \"GET /favicon.ico HTTP/1.1\" 200 2210\n"],"name":"stderr"},{"output_type":"stream","text":["\n","    Shutting down server on port 5000.\n","\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"itimUrGLIjH2"},"source":["<font color=blue>**After running the cell above, click the link below to view the dependency parse**:</font>\n","\n","http://127.0.0.1:5000\n","<br><br>\n","<font color=red>**To shut down the server and return to jupyter**, interrupt the kernel either through the **Kernel** menu above, by hitting the black square on the toolbar, or by typing the keyboard shortcut `Esc`, `I`, `I`</font>"]},{"cell_type":"markdown","metadata":{"collapsed":true,"id":"ktSuyQuDIjH3"},"source":["For more on **Visualizing the entity recognizer** visit https://spacy.io/usage/visualizers#ent\n"]},{"cell_type":"markdown","metadata":{"id":"sEafZ4htLISm"},"source":["## Sentence Segmentation\n","In **spaCy Basics** we saw briefly how Doc objects are divided into sentences. In this section we'll learn how sentence segmentation works, and how to set our own segmentation rules."]},{"cell_type":"code","metadata":{"id":"K0nBnme4LISn"},"source":["# Perform standard imports\n","import spacy\n","nlp = spacy.load('en_core_web_sm')"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"eIzAfZY3LISo","outputId":"65c11a79-47dc-490f-f820-e294317ce556"},"source":["# From Spacy Basics:\n","doc = nlp(u'This is the first sentence. This is another sentence. This is the last sentence.')\n","\n","for sent in doc.sents:\n","    print(sent)"],"execution_count":null,"outputs":[{"output_type":"stream","text":["This is the first sentence.\n","This is another sentence.\n","This is the last sentence.\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"6f0_CSpmLISp"},"source":["### `Doc.sents` is a generator\n","It is important to note that `doc.sents` is a *generator*. That is, a Doc is not segmented until `doc.sents` is called. This means that, where you could print the second Doc token with `print(doc[1])`, you can't call the \"second Doc sentence\" with `print(doc.sents[1])`:"]},{"cell_type":"code","metadata":{"id":"7c1wzwl9LISq","outputId":"ecb2ef2e-1c94-4a83-e7e0-493be9ec5dc3"},"source":["print(doc[1])"],"execution_count":null,"outputs":[{"output_type":"stream","text":["is\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"zZWgieQkLISr","outputId":"feaa51aa-b8a6-429d-d644-b582b14712ec"},"source":["print(doc.sents[1])"],"execution_count":null,"outputs":[{"output_type":"error","ename":"TypeError","evalue":"'generator' object is not subscriptable","traceback":["\u001b[1;31m---------------------------------------------------------------------------\u001b[0m","\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)","\u001b[1;32m<ipython-input-4-2bc012eee1da>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdoc\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msents\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m","\u001b[1;31mTypeError\u001b[0m: 'generator' object is not subscriptable"]}]},{"cell_type":"markdown","metadata":{"id":"uMIAcySSLISr"},"source":["However, you *can* build a sentence collection by running `doc.sents` and saving the result to a list:"]},{"cell_type":"code","metadata":{"id":"DgnZEYSCLISs","outputId":"38a4a06d-345f-47c2-d8c4-503b80f7cb69"},"source":["doc_sents = [sent for sent in doc.sents]\n","doc_sents"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["[This is the first sentence.,\n"," This is another sentence.,\n"," This is the last sentence.]"]},"metadata":{"tags":[]},"execution_count":5}]},{"cell_type":"markdown","metadata":{"id":"Z0dUbrHmLISs"},"source":["<font color=green>**NOTE**: `list(doc.sents)` also works. We show a list comprehension as it allows you to pass in conditionals.</font>"]},{"cell_type":"code","metadata":{"id":"gHOvfzhOLISt","outputId":"71f96c33-39dc-4f5c-bd84-c0ece8b08ecb"},"source":["# Now you can access individual sentences:\n","print(doc_sents[1])"],"execution_count":null,"outputs":[{"output_type":"stream","text":["This is another sentence.\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"549i9weMLISt"},"source":["### `sents` are Spans\n","At first glance it looks like each `sent` contains text from the original Doc object. In fact they're just Spans with start and end token pointers."]},{"cell_type":"code","metadata":{"id":"FqYGVuhALISu","outputId":"ab415317-9ba3-4f9a-9608-eac091a425b9"},"source":["type(doc_sents[1])"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["spacy.tokens.span.Span"]},"metadata":{"tags":[]},"execution_count":7}]},{"cell_type":"code","metadata":{"id":"E-wR8hUiLISu","outputId":"6b59fb86-8c85-4138-8087-20373c01844d"},"source":["print(doc_sents[1].start, doc_sents[1].end)"],"execution_count":null,"outputs":[{"output_type":"stream","text":["6 11\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"KadojT_mLISu"},"source":["### Adding Rules\n","spaCy's built-in `sentencizer` relies on the dependency parse and end-of-sentence punctuation to determine segmentation rules. We can add rules of our own, but they have to be added *before* the creation of the Doc object, as that is where the parsing of segment start tokens happens:"]},{"cell_type":"code","metadata":{"id":"UNGflij5LISv","outputId":"3d8e8137-1b5d-4d66-fa17-95bc9bb4ca27"},"source":["# Parsing the segmentation start tokens happens during the nlp pipeline\n","doc2 = nlp(u'This is a sentence. This is a sentence. This is a sentence.')\n","\n","for token in doc2:\n","    print(token.is_sent_start, ' '+token.text)"],"execution_count":null,"outputs":[{"output_type":"stream","text":["None  This\n","None  is\n","None  a\n","None  sentence\n","None  .\n","True  This\n","None  is\n","None  a\n","None  sentence\n","None  .\n","True  This\n","None  is\n","None  a\n","None  sentence\n","None  .\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"clFz2JbvLISv"},"source":["<font color=green>Notice we haven't run `doc2.sents`, and yet `token.is_sent_start` was set to True on two tokens in the Doc.</font>"]},{"cell_type":"markdown","metadata":{"id":"1nLwANiELISw"},"source":["Let's add a semicolon to our existing segmentation rules. That is, whenever the sentencizer encounters a semicolon, the next token should start a new segment."]},{"cell_type":"code","metadata":{"id":"liWzTk_3LISw","outputId":"fe823bf7-8bd7-48a0-e3f7-029e6c74632a"},"source":["# SPACY'S DEFAULT BEHAVIOR\n","doc3 = nlp(u'\"Management is doing things right; leadership is doing the right things.\" -Peter Drucker')\n","\n","for sent in doc3.sents:\n","    print(sent)"],"execution_count":null,"outputs":[{"output_type":"stream","text":["\"Management is doing things right; leadership is doing the right things.\"\n","-Peter Drucker\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"rvj5qLZrLISw","outputId":"1a902e18-41a4-453e-c3ff-de823056b57e"},"source":["# ADD A NEW RULE TO THE PIPELINE\n","def set_custom_boundaries(doc):\n","    for token in doc[:-1]:\n","        if token.text == ';':\n","            doc[token.i+1].is_sent_start = True\n","    return doc\n","\n","nlp.add_pipe(set_custom_boundaries, before='parser')\n","\n","nlp.pipe_names"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["['tagger', 'set_custom_boundaries', 'parser', 'ner']"]},"metadata":{"tags":[]},"execution_count":2}]},{"cell_type":"markdown","metadata":{"id":"4v6rGcL7LISx"},"source":["<font color=green>The new rule has to run before the document is parsed. Here we can either pass the argument `before='parser'` or `first=True`."]},{"cell_type":"code","metadata":{"id":"KcERiwJvLISx","outputId":"5819b8c2-1fef-415d-c551-19cd4c063dfe"},"source":["# Re-run the Doc object creation:\n","doc4 = nlp(u'\"Management is doing things right; leadership is doing the right things.\" -Peter Drucker')\n","\n","for sent in doc4.sents:\n","    print(sent)"],"execution_count":null,"outputs":[{"output_type":"stream","text":["\"Management is doing things right;\n","leadership is doing the right things.\"\n","-Peter Drucker\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"--e7l7giLISx","outputId":"6efaa12d-3337-4e30-af05-15468334b2fb"},"source":["# And yet the new rule doesn't apply to the older Doc object:\n","for sent in doc3.sents:\n","    print(sent)"],"execution_count":null,"outputs":[{"output_type":"stream","text":["\"Management is doing things right; leadership is doing the right things.\"\n","-Peter Drucker\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"0imL7RBbLISy"},"source":["#### Why not change the token directly?\n","Why not simply set the `.is_sent_start` value to True on existing tokens?"]},{"cell_type":"code","metadata":{"id":"KyYj8TyVLISy","outputId":"6315677f-fe32-42ec-8d5c-9dbeca4c87cc"},"source":["# Find the token we want to change:\n","doc3[7]"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["leadership"]},"metadata":{"tags":[]},"execution_count":4}]},{"cell_type":"code","metadata":{"id":"rb82lbi5LISy","outputId":"15a266c2-11f5-4814-f10f-725d720cbb7c"},"source":["# Try to change the .is_sent_start attribute:\n","doc3[7].is_sent_start = True"],"execution_count":null,"outputs":[{"output_type":"error","ename":"ValueError","evalue":"[E043] Refusing to write to token.sent_start if its document is parsed, because this may cause inconsistent state.","traceback":["\u001b[1;31m---------------------------------------------------------------------------\u001b[0m","\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)","\u001b[1;32m<ipython-input-5-bcec3fe6a9a2>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;31m# Try to change the .is_sent_start attribute:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0mdoc3\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m7\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mis_sent_start\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mTrue\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m","\u001b[1;32mtoken.pyx\u001b[0m in \u001b[0;36mspacy.tokens.token.Token.is_sent_start.__set__\u001b[1;34m()\u001b[0m\n","\u001b[1;31mValueError\u001b[0m: [E043] Refusing to write to token.sent_start if its document is parsed, because this may cause inconsistent state."]}]},{"cell_type":"markdown","metadata":{"id":"7Lop1z-TLISz"},"source":["<font color=green>spaCy refuses to change the tag after the document is parsed to prevent inconsistencies in the data.</font>"]},{"cell_type":"markdown","metadata":{"id":"tWGwvzgcLISz"},"source":["### Changing the Rules\n","In some cases we want to *replace* spaCy's default sentencizer with our own set of rules. In this section we'll see how the default sentencizer breaks on periods. We'll then replace this behavior with a sentencizer that breaks on linebreaks."]},{"cell_type":"code","metadata":{"id":"Qq-uMlhdLISz","outputId":"bc015753-078f-4c5d-ea28-bf62dfcf47eb"},"source":["nlp = spacy.load('en_core_web_sm')  # reset to the original\n","\n","mystring = u\"This is a sentence. This is another.\\n\\nThis is a \\nthird sentence.\"\n","\n","# SPACY DEFAULT BEHAVIOR:\n","doc = nlp(mystring)\n","\n","for sent in doc.sents:\n","    print([token.text for token in sent])"],"execution_count":null,"outputs":[{"output_type":"stream","text":["['This', 'is', 'a', 'sentence', '.']\n","['This', 'is', 'another', '.', '\\n\\n']\n","['This', 'is', 'a', '\\n', 'third', 'sentence', '.']\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"FhzrIdQnLIS0"},"source":["# CHANGING THE RULES\n","from spacy.pipeline import SentenceSegmenter\n","\n","def split_on_newlines(doc):\n","    start = 0\n","    seen_newline = False\n","    for word in doc:\n","        if seen_newline:\n","            yield doc[start:word.i]\n","            start = word.i\n","            seen_newline = False\n","        elif word.text.startswith('\\n'): # handles multiple occurrences\n","            seen_newline = True\n","    yield doc[start:]      # handles the last group of tokens\n","\n","\n","sbd = SentenceSegmenter(nlp.vocab, strategy=split_on_newlines)\n","nlp.add_pipe(sbd)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"6RnPImjELIS1"},"source":["<font color=green>While the function `split_on_newlines` can be named anything we want, it's important to use the name `sbd` for the SentenceSegmenter.</font>"]},{"cell_type":"code","metadata":{"id":"IYIW6PTRLIS5","outputId":"e4a2ba6c-3e09-4f06-80fb-86950046bd75"},"source":["doc = nlp(mystring)\n","for sent in doc.sents:\n","    print([token.text for token in sent])"],"execution_count":null,"outputs":[{"output_type":"stream","text":["['This', 'is', 'a', 'sentence', '.', 'This', 'is', 'another', '.', '\\n\\n']\n","['This', 'is', 'a', '\\n']\n","['third', 'sentence', '.']\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"Lunpnz8mLIS6"},"source":["<font color=green>Here we see that periods no longer affect segmentation, only linebreaks do. This would be appropriate when working with a long list of tweets, for instance.</font>\n"]},{"cell_type":"markdown","metadata":{"id":"NVeSxcOrWrpN"},"source":["#Text Classification\r\n"]},{"cell_type":"markdown","metadata":{"id":"CBoV1oh-Wkgx"},"source":["## Scikit-learn Primer\n","\n","**Scikit-learn** (http://scikit-learn.org/) is an open-source machine learning library for Python that offers a variety of regression, classification and clustering algorithms.\n","\n","In this section we'll perform a fairly simple classification exercise with scikit-learn. In the next section we'll leverage the machine learning strength of scikit-learn to perform natural language classifications."]},{"cell_type":"markdown","metadata":{"id":"-Qm6A-DZWkgz"},"source":["### Installation and Setup\n","\n","### From the command line or terminal:\n","> `conda install scikit-learn`\n","> <br>*or*<br>\n","> `pip install -U scikit-learn`\n","\n","Scikit-learn additionally requires that NumPy and SciPy be installed. For more info visit http://scikit-learn.org/stable/install.html"]},{"cell_type":"markdown","metadata":{"id":"PsdG84QgWkg0"},"source":["## Perform Imports and Load Data\n","For this exercise we'll be using the **SMSSpamCollection** dataset from [UCI datasets](https://archive.ics.uci.edu/ml/datasets/SMS+Spam+Collection) that contains more than 5 thousand SMS phone messages.<br>You can check out the [**sms_readme**](../TextFiles/sms_readme.txt) file for more info.\n","\n","The file is a [tab-separated-values](https://en.wikipedia.org/wiki/Tab-separated_values) (tsv) file with four columns:\n","> **label** - every message is labeled as either ***ham*** or ***spam***<br>\n","> **message** - the message itself<br>\n","> **length** - the number of characters in each message<br>\n","> **punct** - the number of punctuation characters in each message"]},{"cell_type":"code","metadata":{"id":"y2lPqRTFWkg1","outputId":"7c6c9b7a-b58a-4a89-edb7-99927ca1b1e3"},"source":["import numpy as np\n","import pandas as pd\n","\n","df = pd.read_csv('../TextFiles/smsspamcollection.tsv', sep='\\t')\n","df.head()"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/html":["<div>\n","<style>\n","    .dataframe thead tr:only-child th {\n","        text-align: right;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: left;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>label</th>\n","      <th>message</th>\n","      <th>length</th>\n","      <th>punct</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>ham</td>\n","      <td>Go until jurong point, crazy.. Available only ...</td>\n","      <td>111</td>\n","      <td>9</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>ham</td>\n","      <td>Ok lar... Joking wif u oni...</td>\n","      <td>29</td>\n","      <td>6</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>spam</td>\n","      <td>Free entry in 2 a wkly comp to win FA Cup fina...</td>\n","      <td>155</td>\n","      <td>6</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>ham</td>\n","      <td>U dun say so early hor... U c already then say...</td>\n","      <td>49</td>\n","      <td>6</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>ham</td>\n","      <td>Nah I don't think he goes to usf, he lives aro...</td>\n","      <td>61</td>\n","      <td>2</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>"],"text/plain":["  label                                            message  length  punct\n","0   ham  Go until jurong point, crazy.. Available only ...     111      9\n","1   ham                      Ok lar... Joking wif u oni...      29      6\n","2  spam  Free entry in 2 a wkly comp to win FA Cup fina...     155      6\n","3   ham  U dun say so early hor... U c already then say...      49      6\n","4   ham  Nah I don't think he goes to usf, he lives aro...      61      2"]},"metadata":{"tags":[]},"execution_count":1}]},{"cell_type":"code","metadata":{"id":"tUtdnKXWWkg4","outputId":"83a015d0-77d6-4643-f5b2-911069cd6099"},"source":["len(df)"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["5572"]},"metadata":{"tags":[]},"execution_count":2}]},{"cell_type":"markdown","metadata":{"id":"Mq-w2_cSWkg5"},"source":["### Check for missing values:\n","Machine learning models usually require complete data."]},{"cell_type":"code","metadata":{"id":"DPyqyme2Wkg5","outputId":"8707a3d5-1389-4ab7-cab7-087d0603ae1b"},"source":["df.isnull().sum()"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["label      0\n","message    0\n","length     0\n","punct      0\n","dtype: int64"]},"metadata":{"tags":[]},"execution_count":3}]},{"cell_type":"markdown","metadata":{"id":"_zGGYHICWkg6"},"source":["### Take a quick look at the *ham* and *spam* `label` column:"]},{"cell_type":"code","metadata":{"scrolled":true,"id":"OFiw-WFmWkg7","outputId":"6929e512-cb61-46c5-e7af-1819faf65a4e"},"source":["df['label'].unique()"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["array(['ham', 'spam'], dtype=object)"]},"metadata":{"tags":[]},"execution_count":4}]},{"cell_type":"code","metadata":{"id":"qBNRoPbqWkg8","outputId":"9ecd17de-41db-45ef-bb98-725c36befded"},"source":["df['label'].value_counts()"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["ham     4825\n","spam     747\n","Name: label, dtype: int64"]},"metadata":{"tags":[]},"execution_count":5}]},{"cell_type":"markdown","metadata":{"id":"HDMNLNzqWkg-"},"source":["<font color=green>We see that 4825 out of 5572 messages, or 86.6%, are ham.<br>This means that any machine learning model we create has to perform **better than 86.6%** to beat random chance.</font>"]},{"cell_type":"markdown","metadata":{"id":"egWQWVqzWkhA"},"source":["### Visualize the data:\n","Since we're not ready to do anything with the message text, let's see if we can predict ham/spam labels based on message length and punctuation counts. We'll look at message `length` first:"]},{"cell_type":"code","metadata":{"id":"D6C8wNSgWkhB","outputId":"5acb2dfb-f618-4ee8-cb22-d128ee098a67"},"source":["df['length'].describe()"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["count    5572.000000\n","mean       80.489950\n","std        59.942907\n","min         2.000000\n","25%        36.000000\n","50%        62.000000\n","75%       122.000000\n","max       910.000000\n","Name: length, dtype: float64"]},"metadata":{"tags":[]},"execution_count":6}]},{"cell_type":"markdown","metadata":{"id":"ZqsKmoRTWkhC"},"source":["<font color=green>This dataset is extremely skewed. The mean value is 80.5 and yet the max length is 910. Let's plot this on a logarithmic x-axis.</font>"]},{"cell_type":"code","metadata":{"id":"3jpjjC0PWkhD","outputId":"e220e617-d1d8-4323-8900-4e9742097252"},"source":["import matplotlib.pyplot as plt\n","%matplotlib inline\n","\n","plt.xscale('log')\n","bins = 1.15**(np.arange(0,50))\n","plt.hist(df[df['label']=='ham']['length'],bins=bins,alpha=0.8)\n","plt.hist(df[df['label']=='spam']['length'],bins=bins,alpha=0.8)\n","plt.legend(('ham','spam'))\n","plt.show()"],"execution_count":null,"outputs":[{"output_type":"display_data","data":{"image/png":"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\n","text/plain":["<Figure size 432x288 with 1 Axes>"]},"metadata":{"tags":[]}}]},{"cell_type":"markdown","metadata":{"id":"JDlsRjb7WkhE"},"source":["<font color=green>It looks like there's a small range of values where a message is more likely to be spam than ham.</font>\n","\n","Now let's look at the `punct` column:"]},{"cell_type":"code","metadata":{"id":"41KToJYqWkhE","outputId":"5c2696f0-52c8-4d3f-a891-659450f8f305"},"source":["df['punct'].describe()"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["count    5572.000000\n","mean        4.177495\n","std         4.623919\n","min         0.000000\n","25%         2.000000\n","50%         3.000000\n","75%         6.000000\n","max       133.000000\n","Name: punct, dtype: float64"]},"metadata":{"tags":[]},"execution_count":8}]},{"cell_type":"code","metadata":{"id":"emvFi6S6WkhF","outputId":"b3488294-74ac-4818-a83c-372a78e57016"},"source":["plt.xscale('log')\n","bins = 1.5**(np.arange(0,15))\n","plt.hist(df[df['label']=='ham']['punct'],bins=bins,alpha=0.8)\n","plt.hist(df[df['label']=='spam']['punct'],bins=bins,alpha=0.8)\n","plt.legend(('ham','spam'))\n","plt.show()"],"execution_count":null,"outputs":[{"output_type":"display_data","data":{"image/png":"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\n","text/plain":["<Figure size 432x288 with 1 Axes>"]},"metadata":{"tags":[]}}]},{"cell_type":"markdown","metadata":{"id":"iCWeWZnyWkhG"},"source":["<font color=green>This looks even worse - there seem to be no values where one would pick spam over ham. We'll still try to build a machine learning classification model, but we should expect poor results.</font>"]},{"cell_type":"markdown","metadata":{"id":"qzSX3ATEWkhG"},"source":["___\n","## Split the data into train & test sets:\n","\n","If we wanted to divide the DataFrame into two smaller sets, we could use\n","> `train, test = train_test_split(df)`\n","\n","For our purposes let's also set up our Features (X) and Labels (y). The Label is simple - we're trying to predict the `label` column in our data. For Features we'll use the `length` and `punct` columns. *By convention, **X** is capitalized and **y** is lowercase.*"]},{"cell_type":"markdown","metadata":{"id":"5om_XCMgWkhH"},"source":["### Selecting features\n","There are two ways to build a feature set from the columns we want. If the number of features is small, then we can pass those in directly:\n","> `X = df[['length','punct']]`\n","\n","If the number of features is large, then it may be easier to drop the Label and any other unwanted columns:\n","> `X = df.drop(['label','message'], axis=1)`\n","\n","These operations make copies of **df**, but do not change the original DataFrame in place. All the original data is preserved."]},{"cell_type":"code","metadata":{"id":"IRYBgvsxWkhI"},"source":["# Create Feature and Label sets\n","X = df[['length','punct']]  # note the double set of brackets\n","y = df['label']"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"-FSg0Xj9WkhI"},"source":["### Additional train/test/split arguments:\n","The default test size for `train_test_split` is 30%. Here we'll assign 33% of the data for testing.<br>\n","Also, we can set a `random_state` seed value to ensure that everyone uses the same \"random\" training & testing sets."]},{"cell_type":"code","metadata":{"id":"DYA3xlwKWkhJ","outputId":"51946eb6-4116-488e-f2d4-827dc3390a6f"},"source":["from sklearn.model_selection import train_test_split\n","\n","X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.33, random_state=42)\n","\n","print('Training Data Shape:', X_train.shape)\n","print('Testing Data Shape: ', X_test.shape)"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Training Data Shape: (3733, 2)\n","Testing Data Shape:  (1839, 2)\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"fsjanXx0WkhL"},"source":["Now we can pass these sets into a series of different training & testing algorithms and compare their results."]},{"cell_type":"markdown","metadata":{"id":"bqLC7hGDWkhM"},"source":["___\n","## Train a Logistic Regression classifier\n","One of the simplest multi-class classification tools is [logistic regression](https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LogisticRegression.html). Scikit-learn offers a variety of algorithmic solvers; we'll use [L-BFGS](https://en.wikipedia.org/wiki/Limited-memory_BFGS). "]},{"cell_type":"code","metadata":{"id":"a7ulwrAAWkhN","outputId":"d2e52c9b-ddb6-413c-9937-3f162620d355"},"source":["from sklearn.linear_model import LogisticRegression\n","\n","lr_model = LogisticRegression(solver='lbfgs')\n","\n","lr_model.fit(X_train, y_train)"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n","          intercept_scaling=1, max_iter=100, multi_class='warn',\n","          n_jobs=None, penalty='l2', random_state=None, solver='lbfgs',\n","          tol=0.0001, verbose=0, warm_start=False)"]},"metadata":{"tags":[]},"execution_count":12}]},{"cell_type":"markdown","metadata":{"id":"c3yE1jlbWkhN"},"source":["### Test the Accuracy of the Model"]},{"cell_type":"code","metadata":{"id":"2kNLGR2VWkhO","outputId":"cd2672c6-259f-4e7e-e62f-dd3800c394af"},"source":["from sklearn import metrics\n","\n","# Create a prediction set:\n","predictions = lr_model.predict(X_test)\n","\n","# Print a confusion matrix\n","print(metrics.confusion_matrix(y_test,predictions))"],"execution_count":null,"outputs":[{"output_type":"stream","text":["[[1547   46]\n"," [ 241    5]]\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"2z7z2lLtWkhP","outputId":"e5caa111-dc6d-4723-ad5d-4da579f0bd31"},"source":["# You can make the confusion matrix less confusing by adding labels:\n","df = pd.DataFrame(metrics.confusion_matrix(y_test,predictions), index=['ham','spam'], columns=['ham','spam'])\n","df"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/html":["<div>\n","<style>\n","    .dataframe thead tr:only-child th {\n","        text-align: right;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: left;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>ham</th>\n","      <th>spam</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>ham</th>\n","      <td>1547</td>\n","      <td>46</td>\n","    </tr>\n","    <tr>\n","      <th>spam</th>\n","      <td>241</td>\n","      <td>5</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>"],"text/plain":["       ham  spam\n","ham   1547    46\n","spam   241     5"]},"metadata":{"tags":[]},"execution_count":14}]},{"cell_type":"markdown","metadata":{"id":"esWuekn4WkhR"},"source":["<font color=green>These results are terrible! More spam messages were confused as ham (241) than correctly identified as spam (5), although a relatively small number of ham messages (46) were confused as spam.</font>"]},{"cell_type":"code","metadata":{"id":"50HulbGcWkhS","outputId":"f5ad4fff-b7b0-428d-e8e8-c8bba4f058bf"},"source":["# Print a classification report\n","print(metrics.classification_report(y_test,predictions))"],"execution_count":null,"outputs":[{"output_type":"stream","text":["              precision    recall  f1-score   support\n","\n","         ham       0.87      0.97      0.92      1593\n","        spam       0.10      0.02      0.03       246\n","\n","   micro avg       0.84      0.84      0.84      1839\n","   macro avg       0.48      0.50      0.47      1839\n","weighted avg       0.76      0.84      0.80      1839\n","\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"cRmaZyc3WkhS","outputId":"782921cb-e1bc-442c-f52f-71a31f8fe881"},"source":["# Print the overall accuracy\n","print(metrics.accuracy_score(y_test,predictions))"],"execution_count":null,"outputs":[{"output_type":"stream","text":["0.84393692224\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"GZfLa7eDWkhT"},"source":["<font color=green>This model performed *worse* than a classifier that assigned all messages as \"ham\" would have!</font>"]},{"cell_type":"markdown","metadata":{"id":"a-ef_AoFWkhT"},"source":["___\n","## Train a naïve Bayes classifier:\n","One of the most common - and successful - classifiers is [naïve Bayes](http://scikit-learn.org/stable/modules/naive_bayes.html#naive-bayes)."]},{"cell_type":"code","metadata":{"id":"aFgItUgCWkhU","outputId":"9bfadf65-eaf8-4d3a-9f9e-b24204c8acb4"},"source":["from sklearn.naive_bayes import MultinomialNB\n","\n","nb_model = MultinomialNB()\n","\n","nb_model.fit(X_train, y_train)"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["MultinomialNB(alpha=1.0, class_prior=None, fit_prior=True)"]},"metadata":{"tags":[]},"execution_count":17}]},{"cell_type":"markdown","metadata":{"id":"IM8FVzcoWkhV"},"source":["### Run predictions and report on metrics"]},{"cell_type":"code","metadata":{"id":"pUtGX5bgWkhV","outputId":"210b09a6-8296-49cb-d7ab-104014aeeb7e"},"source":["predictions = nb_model.predict(X_test)\n","print(metrics.confusion_matrix(y_test,predictions))"],"execution_count":null,"outputs":[{"output_type":"stream","text":["[[1583   10]\n"," [ 246    0]]\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"IKW70Q4AWkhW"},"source":["<font color=green>The total number of confusions dropped from **287** to **256**. [241+46=287, 246+10=256]</font>"]},{"cell_type":"code","metadata":{"id":"ySLSYQtuWkhW","outputId":"2ce77af7-cccf-470d-b897-7e488de60f19"},"source":["print(metrics.classification_report(y_test,predictions))"],"execution_count":null,"outputs":[{"output_type":"stream","text":["              precision    recall  f1-score   support\n","\n","         ham       0.87      0.99      0.93      1593\n","        spam       0.00      0.00      0.00       246\n","\n","   micro avg       0.86      0.86      0.86      1839\n","   macro avg       0.43      0.50      0.46      1839\n","weighted avg       0.75      0.86      0.80      1839\n","\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"scrolled":true,"id":"VYR80ucWWkhX","outputId":"4eabd3da-95ac-48e9-db20-d8312c687f8f"},"source":["print(metrics.accuracy_score(y_test,predictions))"],"execution_count":null,"outputs":[{"output_type":"stream","text":["0.860793909734\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"z0gZLcDnWkhY"},"source":["<font color=green>Better, but still less accurate than 86.6%</font>"]},{"cell_type":"markdown","metadata":{"id":"0C2Mu3loWkhY"},"source":["___\n","## Train a support vector machine (SVM) classifier\n","Among the SVM options available, we'll use [C-Support Vector Classification (SVC)](https://scikit-learn.org/stable/modules/generated/sklearn.svm.SVC.html#sklearn.svm.SVC)"]},{"cell_type":"code","metadata":{"id":"4sBDZyAFWkhZ","outputId":"15743bc4-e42a-40a6-9d5f-f6da69361cd4"},"source":["from sklearn.svm import SVC\n","svc_model = SVC(gamma='auto')\n","svc_model.fit(X_train,y_train)"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["SVC(C=1.0, cache_size=200, class_weight=None, coef0=0.0,\n","  decision_function_shape='ovr', degree=3, gamma='auto', kernel='rbf',\n","  max_iter=-1, probability=False, random_state=None, shrinking=True,\n","  tol=0.001, verbose=False)"]},"metadata":{"tags":[]},"execution_count":21}]},{"cell_type":"markdown","metadata":{"id":"DEFY4kQHWkha"},"source":["### Run predictions and report on metrics"]},{"cell_type":"code","metadata":{"id":"l4H-0CjZWkhb","outputId":"ac69a6b6-d0c6-499a-f8d5-960dbe2a3d17"},"source":["predictions = svc_model.predict(X_test)\n","print(metrics.confusion_matrix(y_test,predictions))"],"execution_count":null,"outputs":[{"output_type":"stream","text":["[[1515   78]\n"," [ 131  115]]\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"jawwGyMnWkhc"},"source":["<font color=green>The total number of confusions dropped even further to **209**.</font>"]},{"cell_type":"code","metadata":{"id":"PABRUdo9Wkhd","outputId":"4f540827-87a2-4c9b-8f66-63d245bfaed0"},"source":["print(metrics.classification_report(y_test,predictions))"],"execution_count":null,"outputs":[{"output_type":"stream","text":["              precision    recall  f1-score   support\n","\n","         ham       0.92      0.95      0.94      1593\n","        spam       0.60      0.47      0.52       246\n","\n","   micro avg       0.89      0.89      0.89      1839\n","   macro avg       0.76      0.71      0.73      1839\n","weighted avg       0.88      0.89      0.88      1839\n","\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"2rnyRFLDWkhe","outputId":"4b395c9d-aa36-4b15-a97a-d38165277b80"},"source":["print(metrics.accuracy_score(y_test,predictions))"],"execution_count":null,"outputs":[{"output_type":"stream","text":["0.886351277868\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"BhkGKcEIWkhf"},"source":["<font color=green>And finally we have a model that performs *slightly* better than random chance.</font>"]},{"cell_type":"markdown","metadata":{"id":"xTbnHakjWkhf"},"source":["Great! Now you should be able to load a dataset, divide it into training and testing sets, and perform simple analyses using scikit-learn.\n"]},{"cell_type":"markdown","metadata":{"id":"0PIVjew3lhHp"},"source":["___\n","\n","<a href='http://www.pieriandata.com'> <img src='../Pierian_Data_Logo.png' /></a>\n","___\n","\n","# Keras Basics\n","\n","Welcome to the section on deep learning! We'll be using Keras with a TensorFlow backend to perform our deep learning operations.\n","\n","This means we should get familiar with some Keras fundamentals and basics!\n","\n","## Imports\n","\n"]},{"cell_type":"code","metadata":{"collapsed":true,"id":"uqhwgQvRlhHt"},"source":["import numpy as np"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"cSWIf8C1lhHu"},"source":["## Dataset\n","\n","We will use the famous Iris Data set.\n","_____\n","More info on the data set:\n","https://en.wikipedia.org/wiki/Iris_flower_data_set\n","\n","## Reading in the Data Set\n","\n","We've already downloaded the dataset, its in this folder. So let's open it up. "]},{"cell_type":"code","metadata":{"collapsed":true,"id":"4GAgYXknlhHv"},"source":["from sklearn.datasets import load_iris"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"collapsed":true,"id":"kXT9f1kelhHw"},"source":["iris = load_iris()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"8wBZeaK3lhHw","outputId":"8cb63486-e02c-4703-ef96-126e1f00c88e"},"source":["type(iris)"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["sklearn.utils.Bunch"]},"metadata":{"tags":[]},"execution_count":148}]},{"cell_type":"code","metadata":{"id":"_3XzLj7-lhHx","outputId":"0f0e6467-fc4c-4e88-981b-af1179837ba4"},"source":["print(iris.DESCR)"],"execution_count":null,"outputs":[{"output_type":"stream","text":[".. _iris_dataset:\n","\n","Iris plants dataset\n","--------------------\n","\n","**Data Set Characteristics:**\n","\n","    :Number of Instances: 150 (50 in each of three classes)\n","    :Number of Attributes: 4 numeric, predictive attributes and the class\n","    :Attribute Information:\n","        - sepal length in cm\n","        - sepal width in cm\n","        - petal length in cm\n","        - petal width in cm\n","        - class:\n","                - Iris-Setosa\n","                - Iris-Versicolour\n","                - Iris-Virginica\n","                \n","    :Summary Statistics:\n","\n","    ============== ==== ==== ======= ===== ====================\n","                    Min  Max   Mean    SD   Class Correlation\n","    ============== ==== ==== ======= ===== ====================\n","    sepal length:   4.3  7.9   5.84   0.83    0.7826\n","    sepal width:    2.0  4.4   3.05   0.43   -0.4194\n","    petal length:   1.0  6.9   3.76   1.76    0.9490  (high!)\n","    petal width:    0.1  2.5   1.20   0.76    0.9565  (high!)\n","    ============== ==== ==== ======= ===== ====================\n","\n","    :Missing Attribute Values: None\n","    :Class Distribution: 33.3% for each of 3 classes.\n","    :Creator: R.A. Fisher\n","    :Donor: Michael Marshall (MARSHALL%PLU@io.arc.nasa.gov)\n","    :Date: July, 1988\n","\n","The famous Iris database, first used by Sir R.A. Fisher. The dataset is taken\n","from Fisher's paper. Note that it's the same as in R, but not as in the UCI\n","Machine Learning Repository, which has two wrong data points.\n","\n","This is perhaps the best known database to be found in the\n","pattern recognition literature.  Fisher's paper is a classic in the field and\n","is referenced frequently to this day.  (See Duda & Hart, for example.)  The\n","data set contains 3 classes of 50 instances each, where each class refers to a\n","type of iris plant.  One class is linearly separable from the other 2; the\n","latter are NOT linearly separable from each other.\n","\n",".. topic:: References\n","\n","   - Fisher, R.A. \"The use of multiple measurements in taxonomic problems\"\n","     Annual Eugenics, 7, Part II, 179-188 (1936); also in \"Contributions to\n","     Mathematical Statistics\" (John Wiley, NY, 1950).\n","   - Duda, R.O., & Hart, P.E. (1973) Pattern Classification and Scene Analysis.\n","     (Q327.D83) John Wiley & Sons.  ISBN 0-471-22361-1.  See page 218.\n","   - Dasarathy, B.V. (1980) \"Nosing Around the Neighborhood: A New System\n","     Structure and Classification Rule for Recognition in Partially Exposed\n","     Environments\".  IEEE Transactions on Pattern Analysis and Machine\n","     Intelligence, Vol. PAMI-2, No. 1, 67-71.\n","   - Gates, G.W. (1972) \"The Reduced Nearest Neighbor Rule\".  IEEE Transactions\n","     on Information Theory, May 1972, 431-433.\n","   - See also: 1988 MLC Proceedings, 54-64.  Cheeseman et al\"s AUTOCLASS II\n","     conceptual clustering system finds 3 classes in the data.\n","   - Many, many more ...\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"collapsed":true,"id":"6RS5nxcblhHx"},"source":["X = iris.data"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"-ywgmdUXlhHy","outputId":"d6f02e8d-f53e-4a17-af1c-138f657ac5e5"},"source":["X"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["array([[5.1, 3.5, 1.4, 0.2],\n","       [4.9, 3. , 1.4, 0.2],\n","       [4.7, 3.2, 1.3, 0.2],\n","       [4.6, 3.1, 1.5, 0.2],\n","       [5. , 3.6, 1.4, 0.2],\n","       [5.4, 3.9, 1.7, 0.4],\n","       [4.6, 3.4, 1.4, 0.3],\n","       [5. , 3.4, 1.5, 0.2],\n","       [4.4, 2.9, 1.4, 0.2],\n","       [4.9, 3.1, 1.5, 0.1],\n","       [5.4, 3.7, 1.5, 0.2],\n","       [4.8, 3.4, 1.6, 0.2],\n","       [4.8, 3. , 1.4, 0.1],\n","       [4.3, 3. , 1.1, 0.1],\n","       [5.8, 4. , 1.2, 0.2],\n","       [5.7, 4.4, 1.5, 0.4],\n","       [5.4, 3.9, 1.3, 0.4],\n","       [5.1, 3.5, 1.4, 0.3],\n","       [5.7, 3.8, 1.7, 0.3],\n","       [5.1, 3.8, 1.5, 0.3],\n","       [5.4, 3.4, 1.7, 0.2],\n","       [5.1, 3.7, 1.5, 0.4],\n","       [4.6, 3.6, 1. , 0.2],\n","       [5.1, 3.3, 1.7, 0.5],\n","       [4.8, 3.4, 1.9, 0.2],\n","       [5. , 3. , 1.6, 0.2],\n","       [5. , 3.4, 1.6, 0.4],\n","       [5.2, 3.5, 1.5, 0.2],\n","       [5.2, 3.4, 1.4, 0.2],\n","       [4.7, 3.2, 1.6, 0.2],\n","       [4.8, 3.1, 1.6, 0.2],\n","       [5.4, 3.4, 1.5, 0.4],\n","       [5.2, 4.1, 1.5, 0.1],\n","       [5.5, 4.2, 1.4, 0.2],\n","       [4.9, 3.1, 1.5, 0.2],\n","       [5. , 3.2, 1.2, 0.2],\n","       [5.5, 3.5, 1.3, 0.2],\n","       [4.9, 3.6, 1.4, 0.1],\n","       [4.4, 3. , 1.3, 0.2],\n","       [5.1, 3.4, 1.5, 0.2],\n","       [5. , 3.5, 1.3, 0.3],\n","       [4.5, 2.3, 1.3, 0.3],\n","       [4.4, 3.2, 1.3, 0.2],\n","       [5. , 3.5, 1.6, 0.6],\n","       [5.1, 3.8, 1.9, 0.4],\n","       [4.8, 3. , 1.4, 0.3],\n","       [5.1, 3.8, 1.6, 0.2],\n","       [4.6, 3.2, 1.4, 0.2],\n","       [5.3, 3.7, 1.5, 0.2],\n","       [5. , 3.3, 1.4, 0.2],\n","       [7. , 3.2, 4.7, 1.4],\n","       [6.4, 3.2, 4.5, 1.5],\n","       [6.9, 3.1, 4.9, 1.5],\n","       [5.5, 2.3, 4. , 1.3],\n","       [6.5, 2.8, 4.6, 1.5],\n","       [5.7, 2.8, 4.5, 1.3],\n","       [6.3, 3.3, 4.7, 1.6],\n","       [4.9, 2.4, 3.3, 1. ],\n","       [6.6, 2.9, 4.6, 1.3],\n","       [5.2, 2.7, 3.9, 1.4],\n","       [5. , 2. , 3.5, 1. ],\n","       [5.9, 3. , 4.2, 1.5],\n","       [6. , 2.2, 4. , 1. ],\n","       [6.1, 2.9, 4.7, 1.4],\n","       [5.6, 2.9, 3.6, 1.3],\n","       [6.7, 3.1, 4.4, 1.4],\n","       [5.6, 3. , 4.5, 1.5],\n","       [5.8, 2.7, 4.1, 1. ],\n","       [6.2, 2.2, 4.5, 1.5],\n","       [5.6, 2.5, 3.9, 1.1],\n","       [5.9, 3.2, 4.8, 1.8],\n","       [6.1, 2.8, 4. , 1.3],\n","       [6.3, 2.5, 4.9, 1.5],\n","       [6.1, 2.8, 4.7, 1.2],\n","       [6.4, 2.9, 4.3, 1.3],\n","       [6.6, 3. , 4.4, 1.4],\n","       [6.8, 2.8, 4.8, 1.4],\n","       [6.7, 3. , 5. , 1.7],\n","       [6. , 2.9, 4.5, 1.5],\n","       [5.7, 2.6, 3.5, 1. ],\n","       [5.5, 2.4, 3.8, 1.1],\n","       [5.5, 2.4, 3.7, 1. ],\n","       [5.8, 2.7, 3.9, 1.2],\n","       [6. , 2.7, 5.1, 1.6],\n","       [5.4, 3. , 4.5, 1.5],\n","       [6. , 3.4, 4.5, 1.6],\n","       [6.7, 3.1, 4.7, 1.5],\n","       [6.3, 2.3, 4.4, 1.3],\n","       [5.6, 3. , 4.1, 1.3],\n","       [5.5, 2.5, 4. , 1.3],\n","       [5.5, 2.6, 4.4, 1.2],\n","       [6.1, 3. , 4.6, 1.4],\n","       [5.8, 2.6, 4. , 1.2],\n","       [5. , 2.3, 3.3, 1. ],\n","       [5.6, 2.7, 4.2, 1.3],\n","       [5.7, 3. , 4.2, 1.2],\n","       [5.7, 2.9, 4.2, 1.3],\n","       [6.2, 2.9, 4.3, 1.3],\n","       [5.1, 2.5, 3. , 1.1],\n","       [5.7, 2.8, 4.1, 1.3],\n","       [6.3, 3.3, 6. , 2.5],\n","       [5.8, 2.7, 5.1, 1.9],\n","       [7.1, 3. , 5.9, 2.1],\n","       [6.3, 2.9, 5.6, 1.8],\n","       [6.5, 3. , 5.8, 2.2],\n","       [7.6, 3. , 6.6, 2.1],\n","       [4.9, 2.5, 4.5, 1.7],\n","       [7.3, 2.9, 6.3, 1.8],\n","       [6.7, 2.5, 5.8, 1.8],\n","       [7.2, 3.6, 6.1, 2.5],\n","       [6.5, 3.2, 5.1, 2. ],\n","       [6.4, 2.7, 5.3, 1.9],\n","       [6.8, 3. , 5.5, 2.1],\n","       [5.7, 2.5, 5. , 2. ],\n","       [5.8, 2.8, 5.1, 2.4],\n","       [6.4, 3.2, 5.3, 2.3],\n","       [6.5, 3. , 5.5, 1.8],\n","       [7.7, 3.8, 6.7, 2.2],\n","       [7.7, 2.6, 6.9, 2.3],\n","       [6. , 2.2, 5. , 1.5],\n","       [6.9, 3.2, 5.7, 2.3],\n","       [5.6, 2.8, 4.9, 2. ],\n","       [7.7, 2.8, 6.7, 2. ],\n","       [6.3, 2.7, 4.9, 1.8],\n","       [6.7, 3.3, 5.7, 2.1],\n","       [7.2, 3.2, 6. , 1.8],\n","       [6.2, 2.8, 4.8, 1.8],\n","       [6.1, 3. , 4.9, 1.8],\n","       [6.4, 2.8, 5.6, 2.1],\n","       [7.2, 3. , 5.8, 1.6],\n","       [7.4, 2.8, 6.1, 1.9],\n","       [7.9, 3.8, 6.4, 2. ],\n","       [6.4, 2.8, 5.6, 2.2],\n","       [6.3, 2.8, 5.1, 1.5],\n","       [6.1, 2.6, 5.6, 1.4],\n","       [7.7, 3. , 6.1, 2.3],\n","       [6.3, 3.4, 5.6, 2.4],\n","       [6.4, 3.1, 5.5, 1.8],\n","       [6. , 3. , 4.8, 1.8],\n","       [6.9, 3.1, 5.4, 2.1],\n","       [6.7, 3.1, 5.6, 2.4],\n","       [6.9, 3.1, 5.1, 2.3],\n","       [5.8, 2.7, 5.1, 1.9],\n","       [6.8, 3.2, 5.9, 2.3],\n","       [6.7, 3.3, 5.7, 2.5],\n","       [6.7, 3. , 5.2, 2.3],\n","       [6.3, 2.5, 5. , 1.9],\n","       [6.5, 3. , 5.2, 2. ],\n","       [6.2, 3.4, 5.4, 2.3],\n","       [5.9, 3. , 5.1, 1.8]])"]},"metadata":{"tags":[]},"execution_count":151}]},{"cell_type":"code","metadata":{"collapsed":true,"id":"8NDFyBNhlhHy"},"source":["y = iris.target"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"318bsPYDlhHz","outputId":"49b3e099-9909-471c-8404-45527b74b094"},"source":["y"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n","       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n","       0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n","       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n","       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,\n","       2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,\n","       2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2])"]},"metadata":{"tags":[]},"execution_count":153}]},{"cell_type":"code","metadata":{"collapsed":true,"id":"ByRUTBxElhHz"},"source":["from keras.utils import to_categorical"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"collapsed":true,"id":"ykEzg-3ulhHz"},"source":["y = to_categorical(y)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"kIbkCT-vlhH0","outputId":"08ba383c-c0dd-4b68-cc62-50de31d103e7"},"source":["y.shape"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["(150, 3)"]},"metadata":{"tags":[]},"execution_count":157}]},{"cell_type":"code","metadata":{"id":"A-vRd8qUlhH0","outputId":"2c61d21c-3378-4c08-943f-fbf745ff45b6"},"source":["y"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["array([[1., 0., 0.],\n","       [1., 0., 0.],\n","       [1., 0., 0.],\n","       [1., 0., 0.],\n","       [1., 0., 0.],\n","       [1., 0., 0.],\n","       [1., 0., 0.],\n","       [1., 0., 0.],\n","       [1., 0., 0.],\n","       [1., 0., 0.],\n","       [1., 0., 0.],\n","       [1., 0., 0.],\n","       [1., 0., 0.],\n","       [1., 0., 0.],\n","       [1., 0., 0.],\n","       [1., 0., 0.],\n","       [1., 0., 0.],\n","       [1., 0., 0.],\n","       [1., 0., 0.],\n","       [1., 0., 0.],\n","       [1., 0., 0.],\n","       [1., 0., 0.],\n","       [1., 0., 0.],\n","       [1., 0., 0.],\n","       [1., 0., 0.],\n","       [1., 0., 0.],\n","       [1., 0., 0.],\n","       [1., 0., 0.],\n","       [1., 0., 0.],\n","       [1., 0., 0.],\n","       [1., 0., 0.],\n","       [1., 0., 0.],\n","       [1., 0., 0.],\n","       [1., 0., 0.],\n","       [1., 0., 0.],\n","       [1., 0., 0.],\n","       [1., 0., 0.],\n","       [1., 0., 0.],\n","       [1., 0., 0.],\n","       [1., 0., 0.],\n","       [1., 0., 0.],\n","       [1., 0., 0.],\n","       [1., 0., 0.],\n","       [1., 0., 0.],\n","       [1., 0., 0.],\n","       [1., 0., 0.],\n","       [1., 0., 0.],\n","       [1., 0., 0.],\n","       [1., 0., 0.],\n","       [1., 0., 0.],\n","       [0., 1., 0.],\n","       [0., 1., 0.],\n","       [0., 1., 0.],\n","       [0., 1., 0.],\n","       [0., 1., 0.],\n","       [0., 1., 0.],\n","       [0., 1., 0.],\n","       [0., 1., 0.],\n","       [0., 1., 0.],\n","       [0., 1., 0.],\n","       [0., 1., 0.],\n","       [0., 1., 0.],\n","       [0., 1., 0.],\n","       [0., 1., 0.],\n","       [0., 1., 0.],\n","       [0., 1., 0.],\n","       [0., 1., 0.],\n","       [0., 1., 0.],\n","       [0., 1., 0.],\n","       [0., 1., 0.],\n","       [0., 1., 0.],\n","       [0., 1., 0.],\n","       [0., 1., 0.],\n","       [0., 1., 0.],\n","       [0., 1., 0.],\n","       [0., 1., 0.],\n","       [0., 1., 0.],\n","       [0., 1., 0.],\n","       [0., 1., 0.],\n","       [0., 1., 0.],\n","       [0., 1., 0.],\n","       [0., 1., 0.],\n","       [0., 1., 0.],\n","       [0., 1., 0.],\n","       [0., 1., 0.],\n","       [0., 1., 0.],\n","       [0., 1., 0.],\n","       [0., 1., 0.],\n","       [0., 1., 0.],\n","       [0., 1., 0.],\n","       [0., 1., 0.],\n","       [0., 1., 0.],\n","       [0., 1., 0.],\n","       [0., 1., 0.],\n","       [0., 1., 0.],\n","       [0., 1., 0.],\n","       [0., 1., 0.],\n","       [0., 1., 0.],\n","       [0., 1., 0.],\n","       [0., 1., 0.],\n","       [0., 0., 1.],\n","       [0., 0., 1.],\n","       [0., 0., 1.],\n","       [0., 0., 1.],\n","       [0., 0., 1.],\n","       [0., 0., 1.],\n","       [0., 0., 1.],\n","       [0., 0., 1.],\n","       [0., 0., 1.],\n","       [0., 0., 1.],\n","       [0., 0., 1.],\n","       [0., 0., 1.],\n","       [0., 0., 1.],\n","       [0., 0., 1.],\n","       [0., 0., 1.],\n","       [0., 0., 1.],\n","       [0., 0., 1.],\n","       [0., 0., 1.],\n","       [0., 0., 1.],\n","       [0., 0., 1.],\n","       [0., 0., 1.],\n","       [0., 0., 1.],\n","       [0., 0., 1.],\n","       [0., 0., 1.],\n","       [0., 0., 1.],\n","       [0., 0., 1.],\n","       [0., 0., 1.],\n","       [0., 0., 1.],\n","       [0., 0., 1.],\n","       [0., 0., 1.],\n","       [0., 0., 1.],\n","       [0., 0., 1.],\n","       [0., 0., 1.],\n","       [0., 0., 1.],\n","       [0., 0., 1.],\n","       [0., 0., 1.],\n","       [0., 0., 1.],\n","       [0., 0., 1.],\n","       [0., 0., 1.],\n","       [0., 0., 1.],\n","       [0., 0., 1.],\n","       [0., 0., 1.],\n","       [0., 0., 1.],\n","       [0., 0., 1.],\n","       [0., 0., 1.],\n","       [0., 0., 1.],\n","       [0., 0., 1.],\n","       [0., 0., 1.],\n","       [0., 0., 1.],\n","       [0., 0., 1.]], dtype=float32)"]},"metadata":{"tags":[]},"execution_count":158}]},{"cell_type":"markdown","metadata":{"id":"AD_wtuqslhH0"},"source":["## Split the Data into Training and Test\n","\n","Its time to split the data into a train/test set. Keep in mind, sometimes people like to split 3 ways, train/test/validation. We'll keep things simple for now. **Remember to check out the video explanation as to why we split and what all the parameters mean!**"]},{"cell_type":"code","metadata":{"collapsed":true,"id":"wyFSeECylhH1"},"source":["from sklearn.model_selection import train_test_split"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"collapsed":true,"id":"NUpbNqjRlhH1"},"source":["X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.33, random_state=42)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"rjlCM5BKlhH2","outputId":"b1dc2a2d-f4f2-4e3e-bfeb-16fbec4d193d"},"source":["X_train"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["array([[5.7, 2.9, 4.2, 1.3],\n","       [7.6, 3. , 6.6, 2.1],\n","       [5.6, 3. , 4.5, 1.5],\n","       [5.1, 3.5, 1.4, 0.2],\n","       [7.7, 2.8, 6.7, 2. ],\n","       [5.8, 2.7, 4.1, 1. ],\n","       [5.2, 3.4, 1.4, 0.2],\n","       [5. , 3.5, 1.3, 0.3],\n","       [5.1, 3.8, 1.9, 0.4],\n","       [5. , 2. , 3.5, 1. ],\n","       [6.3, 2.7, 4.9, 1.8],\n","       [4.8, 3.4, 1.9, 0.2],\n","       [5. , 3. , 1.6, 0.2],\n","       [5.1, 3.3, 1.7, 0.5],\n","       [5.6, 2.7, 4.2, 1.3],\n","       [5.1, 3.4, 1.5, 0.2],\n","       [5.7, 3. , 4.2, 1.2],\n","       [7.7, 3.8, 6.7, 2.2],\n","       [4.6, 3.2, 1.4, 0.2],\n","       [6.2, 2.9, 4.3, 1.3],\n","       [5.7, 2.5, 5. , 2. ],\n","       [5.5, 4.2, 1.4, 0.2],\n","       [6. , 3. , 4.8, 1.8],\n","       [5.8, 2.7, 5.1, 1.9],\n","       [6. , 2.2, 4. , 1. ],\n","       [5.4, 3. , 4.5, 1.5],\n","       [6.2, 3.4, 5.4, 2.3],\n","       [5.5, 2.3, 4. , 1.3],\n","       [5.4, 3.9, 1.7, 0.4],\n","       [5. , 2.3, 3.3, 1. ],\n","       [6.4, 2.7, 5.3, 1.9],\n","       [5. , 3.3, 1.4, 0.2],\n","       [5. , 3.2, 1.2, 0.2],\n","       [5.5, 2.4, 3.8, 1.1],\n","       [6.7, 3. , 5. , 1.7],\n","       [4.9, 3.1, 1.5, 0.2],\n","       [5.8, 2.8, 5.1, 2.4],\n","       [5. , 3.4, 1.5, 0.2],\n","       [5. , 3.5, 1.6, 0.6],\n","       [5.9, 3.2, 4.8, 1.8],\n","       [5.1, 2.5, 3. , 1.1],\n","       [6.9, 3.2, 5.7, 2.3],\n","       [6. , 2.7, 5.1, 1.6],\n","       [6.1, 2.6, 5.6, 1.4],\n","       [7.7, 3. , 6.1, 2.3],\n","       [5.5, 2.5, 4. , 1.3],\n","       [4.4, 2.9, 1.4, 0.2],\n","       [4.3, 3. , 1.1, 0.1],\n","       [6. , 2.2, 5. , 1.5],\n","       [7.2, 3.2, 6. , 1.8],\n","       [4.6, 3.1, 1.5, 0.2],\n","       [5.1, 3.5, 1.4, 0.3],\n","       [4.4, 3. , 1.3, 0.2],\n","       [6.3, 2.5, 4.9, 1.5],\n","       [6.3, 3.4, 5.6, 2.4],\n","       [4.6, 3.4, 1.4, 0.3],\n","       [6.8, 3. , 5.5, 2.1],\n","       [6.3, 3.3, 6. , 2.5],\n","       [4.7, 3.2, 1.3, 0.2],\n","       [6.1, 2.9, 4.7, 1.4],\n","       [6.5, 2.8, 4.6, 1.5],\n","       [6.2, 2.8, 4.8, 1.8],\n","       [7. , 3.2, 4.7, 1.4],\n","       [6.4, 3.2, 5.3, 2.3],\n","       [5.1, 3.8, 1.6, 0.2],\n","       [6.9, 3.1, 5.4, 2.1],\n","       [5.9, 3. , 4.2, 1.5],\n","       [6.5, 3. , 5.2, 2. ],\n","       [5.7, 2.6, 3.5, 1. ],\n","       [5.2, 2.7, 3.9, 1.4],\n","       [6.1, 3. , 4.6, 1.4],\n","       [4.5, 2.3, 1.3, 0.3],\n","       [6.6, 2.9, 4.6, 1.3],\n","       [5.5, 2.6, 4.4, 1.2],\n","       [5.3, 3.7, 1.5, 0.2],\n","       [5.6, 3. , 4.1, 1.3],\n","       [7.3, 2.9, 6.3, 1.8],\n","       [6.7, 3.3, 5.7, 2.1],\n","       [5.1, 3.7, 1.5, 0.4],\n","       [4.9, 2.4, 3.3, 1. ],\n","       [6.7, 3.3, 5.7, 2.5],\n","       [7.2, 3. , 5.8, 1.6],\n","       [4.9, 3.6, 1.4, 0.1],\n","       [6.7, 3.1, 5.6, 2.4],\n","       [4.9, 3. , 1.4, 0.2],\n","       [6.9, 3.1, 4.9, 1.5],\n","       [7.4, 2.8, 6.1, 1.9],\n","       [6.3, 2.9, 5.6, 1.8],\n","       [5.7, 2.8, 4.1, 1.3],\n","       [6.5, 3. , 5.5, 1.8],\n","       [6.3, 2.3, 4.4, 1.3],\n","       [6.4, 2.9, 4.3, 1.3],\n","       [5.6, 2.8, 4.9, 2. ],\n","       [5.9, 3. , 5.1, 1.8],\n","       [5.4, 3.4, 1.7, 0.2],\n","       [6.1, 2.8, 4. , 1.3],\n","       [4.9, 2.5, 4.5, 1.7],\n","       [5.8, 4. , 1.2, 0.2],\n","       [5.8, 2.6, 4. , 1.2],\n","       [7.1, 3. , 5.9, 2.1]])"]},"metadata":{"tags":[]},"execution_count":111}]},{"cell_type":"code","metadata":{"id":"zpvhVF84lhH2","outputId":"b2be5a5c-77ca-4579-bfce-55c5fac9704f"},"source":["X_test"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["array([[6.1, 2.8, 4.7, 1.2],\n","       [5.7, 3.8, 1.7, 0.3],\n","       [7.7, 2.6, 6.9, 2.3],\n","       [6. , 2.9, 4.5, 1.5],\n","       [6.8, 2.8, 4.8, 1.4],\n","       [5.4, 3.4, 1.5, 0.4],\n","       [5.6, 2.9, 3.6, 1.3],\n","       [6.9, 3.1, 5.1, 2.3],\n","       [6.2, 2.2, 4.5, 1.5],\n","       [5.8, 2.7, 3.9, 1.2],\n","       [6.5, 3.2, 5.1, 2. ],\n","       [4.8, 3. , 1.4, 0.1],\n","       [5.5, 3.5, 1.3, 0.2],\n","       [4.9, 3.1, 1.5, 0.1],\n","       [5.1, 3.8, 1.5, 0.3],\n","       [6.3, 3.3, 4.7, 1.6],\n","       [6.5, 3. , 5.8, 2.2],\n","       [5.6, 2.5, 3.9, 1.1],\n","       [5.7, 2.8, 4.5, 1.3],\n","       [6.4, 2.8, 5.6, 2.2],\n","       [4.7, 3.2, 1.6, 0.2],\n","       [6.1, 3. , 4.9, 1.8],\n","       [5. , 3.4, 1.6, 0.4],\n","       [6.4, 2.8, 5.6, 2.1],\n","       [7.9, 3.8, 6.4, 2. ],\n","       [6.7, 3. , 5.2, 2.3],\n","       [6.7, 2.5, 5.8, 1.8],\n","       [6.8, 3.2, 5.9, 2.3],\n","       [4.8, 3. , 1.4, 0.3],\n","       [4.8, 3.1, 1.6, 0.2],\n","       [4.6, 3.6, 1. , 0.2],\n","       [5.7, 4.4, 1.5, 0.4],\n","       [6.7, 3.1, 4.4, 1.4],\n","       [4.8, 3.4, 1.6, 0.2],\n","       [4.4, 3.2, 1.3, 0.2],\n","       [6.3, 2.5, 5. , 1.9],\n","       [6.4, 3.2, 4.5, 1.5],\n","       [5.2, 3.5, 1.5, 0.2],\n","       [5. , 3.6, 1.4, 0.2],\n","       [5.2, 4.1, 1.5, 0.1],\n","       [5.8, 2.7, 5.1, 1.9],\n","       [6. , 3.4, 4.5, 1.6],\n","       [6.7, 3.1, 4.7, 1.5],\n","       [5.4, 3.9, 1.3, 0.4],\n","       [5.4, 3.7, 1.5, 0.2],\n","       [5.5, 2.4, 3.7, 1. ],\n","       [6.3, 2.8, 5.1, 1.5],\n","       [6.4, 3.1, 5.5, 1.8],\n","       [6.6, 3. , 4.4, 1.4],\n","       [7.2, 3.6, 6.1, 2.5]])"]},"metadata":{"tags":[]},"execution_count":112}]},{"cell_type":"code","metadata":{"id":"cPhm0h8PlhH2","outputId":"87c23575-cfc8-4132-b916-61c6250f8a95"},"source":["y_train"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["array([[0., 1., 0.],\n","       [0., 0., 1.],\n","       [0., 1., 0.],\n","       [1., 0., 0.],\n","       [0., 0., 1.],\n","       [0., 1., 0.],\n","       [1., 0., 0.],\n","       [1., 0., 0.],\n","       [1., 0., 0.],\n","       [0., 1., 0.],\n","       [0., 0., 1.],\n","       [1., 0., 0.],\n","       [1., 0., 0.],\n","       [1., 0., 0.],\n","       [0., 1., 0.],\n","       [1., 0., 0.],\n","       [0., 1., 0.],\n","       [0., 0., 1.],\n","       [1., 0., 0.],\n","       [0., 1., 0.],\n","       [0., 0., 1.],\n","       [1., 0., 0.],\n","       [0., 0., 1.],\n","       [0., 0., 1.],\n","       [0., 1., 0.],\n","       [0., 1., 0.],\n","       [0., 0., 1.],\n","       [0., 1., 0.],\n","       [1., 0., 0.],\n","       [0., 1., 0.],\n","       [0., 0., 1.],\n","       [1., 0., 0.],\n","       [1., 0., 0.],\n","       [0., 1., 0.],\n","       [0., 1., 0.],\n","       [1., 0., 0.],\n","       [0., 0., 1.],\n","       [1., 0., 0.],\n","       [1., 0., 0.],\n","       [0., 1., 0.],\n","       [0., 1., 0.],\n","       [0., 0., 1.],\n","       [0., 1., 0.],\n","       [0., 0., 1.],\n","       [0., 0., 1.],\n","       [0., 1., 0.],\n","       [1., 0., 0.],\n","       [1., 0., 0.],\n","       [0., 0., 1.],\n","       [0., 0., 1.],\n","       [1., 0., 0.],\n","       [1., 0., 0.],\n","       [1., 0., 0.],\n","       [0., 1., 0.],\n","       [0., 0., 1.],\n","       [1., 0., 0.],\n","       [0., 0., 1.],\n","       [0., 0., 1.],\n","       [1., 0., 0.],\n","       [0., 1., 0.],\n","       [0., 1., 0.],\n","       [0., 0., 1.],\n","       [0., 1., 0.],\n","       [0., 0., 1.],\n","       [1., 0., 0.],\n","       [0., 0., 1.],\n","       [0., 1., 0.],\n","       [0., 0., 1.],\n","       [0., 1., 0.],\n","       [0., 1., 0.],\n","       [0., 1., 0.],\n","       [1., 0., 0.],\n","       [0., 1., 0.],\n","       [0., 1., 0.],\n","       [1., 0., 0.],\n","       [0., 1., 0.],\n","       [0., 0., 1.],\n","       [0., 0., 1.],\n","       [1., 0., 0.],\n","       [0., 1., 0.],\n","       [0., 0., 1.],\n","       [0., 0., 1.],\n","       [1., 0., 0.],\n","       [0., 0., 1.],\n","       [1., 0., 0.],\n","       [0., 1., 0.],\n","       [0., 0., 1.],\n","       [0., 0., 1.],\n","       [0., 1., 0.],\n","       [0., 0., 1.],\n","       [0., 1., 0.],\n","       [0., 1., 0.],\n","       [0., 0., 1.],\n","       [0., 0., 1.],\n","       [1., 0., 0.],\n","       [0., 1., 0.],\n","       [0., 0., 1.],\n","       [1., 0., 0.],\n","       [0., 1., 0.],\n","       [0., 0., 1.]], dtype=float32)"]},"metadata":{"tags":[]},"execution_count":113}]},{"cell_type":"code","metadata":{"id":"9zQnzNCglhH3"},"source":["y_test"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"o-_5CHkBlhH3"},"source":["## Standardizing the Data\n","\n","Usually when using Neural Networks, you will get better performance when you standardize the data. Standardization just means normalizing the values to all fit between a certain range, like 0-1, or -1 to 1.\n","\n","The scikit learn library also provides a nice function for this.\n","\n","http://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.MinMaxScaler.html"]},{"cell_type":"code","metadata":{"collapsed":true,"id":"bclqHTAMlhH3"},"source":["from sklearn.preprocessing import MinMaxScaler"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"collapsed":true,"id":"nkMFN9dMlhH3"},"source":["scaler_object = MinMaxScaler()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"bKdorbfSlhH4","outputId":"26cd8e0e-7625-4425-d29f-64b2b15872b0"},"source":["scaler_object.fit(X_train)"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["MinMaxScaler(copy=True, feature_range=(0, 1))"]},"metadata":{"tags":[]},"execution_count":117}]},{"cell_type":"code","metadata":{"collapsed":true,"id":"HCdt_YgelhH4"},"source":["scaled_X_train = scaler_object.transform(X_train)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"collapsed":true,"id":"OurRYLvMlhH4"},"source":["scaled_X_test = scaler_object.transform(X_test)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"bF4ilrmklhH5"},"source":["Ok, now we have the data scaled!"]},{"cell_type":"code","metadata":{"id":"4_UP8-z0lhH5","outputId":"b4597d7c-5dea-46a1-efae-daebf6bd118c"},"source":["X_train.max()"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["7.7"]},"metadata":{"tags":[]},"execution_count":120}]},{"cell_type":"code","metadata":{"id":"UPF0K1VelhH5","outputId":"8dedb8a5-8b0c-4519-88fd-ee14d1b521d2"},"source":["scaled_X_train.max()"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["1.0"]},"metadata":{"tags":[]},"execution_count":121}]},{"cell_type":"code","metadata":{"id":"cw0o3t_NlhH6","outputId":"d285cd25-0dd4-49ad-be95-8e2bd1e3920d"},"source":["X_train"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["array([[5.7, 2.9, 4.2, 1.3],\n","       [7.6, 3. , 6.6, 2.1],\n","       [5.6, 3. , 4.5, 1.5],\n","       [5.1, 3.5, 1.4, 0.2],\n","       [7.7, 2.8, 6.7, 2. ],\n","       [5.8, 2.7, 4.1, 1. ],\n","       [5.2, 3.4, 1.4, 0.2],\n","       [5. , 3.5, 1.3, 0.3],\n","       [5.1, 3.8, 1.9, 0.4],\n","       [5. , 2. , 3.5, 1. ],\n","       [6.3, 2.7, 4.9, 1.8],\n","       [4.8, 3.4, 1.9, 0.2],\n","       [5. , 3. , 1.6, 0.2],\n","       [5.1, 3.3, 1.7, 0.5],\n","       [5.6, 2.7, 4.2, 1.3],\n","       [5.1, 3.4, 1.5, 0.2],\n","       [5.7, 3. , 4.2, 1.2],\n","       [7.7, 3.8, 6.7, 2.2],\n","       [4.6, 3.2, 1.4, 0.2],\n","       [6.2, 2.9, 4.3, 1.3],\n","       [5.7, 2.5, 5. , 2. ],\n","       [5.5, 4.2, 1.4, 0.2],\n","       [6. , 3. , 4.8, 1.8],\n","       [5.8, 2.7, 5.1, 1.9],\n","       [6. , 2.2, 4. , 1. ],\n","       [5.4, 3. , 4.5, 1.5],\n","       [6.2, 3.4, 5.4, 2.3],\n","       [5.5, 2.3, 4. , 1.3],\n","       [5.4, 3.9, 1.7, 0.4],\n","       [5. , 2.3, 3.3, 1. ],\n","       [6.4, 2.7, 5.3, 1.9],\n","       [5. , 3.3, 1.4, 0.2],\n","       [5. , 3.2, 1.2, 0.2],\n","       [5.5, 2.4, 3.8, 1.1],\n","       [6.7, 3. , 5. , 1.7],\n","       [4.9, 3.1, 1.5, 0.2],\n","       [5.8, 2.8, 5.1, 2.4],\n","       [5. , 3.4, 1.5, 0.2],\n","       [5. , 3.5, 1.6, 0.6],\n","       [5.9, 3.2, 4.8, 1.8],\n","       [5.1, 2.5, 3. , 1.1],\n","       [6.9, 3.2, 5.7, 2.3],\n","       [6. , 2.7, 5.1, 1.6],\n","       [6.1, 2.6, 5.6, 1.4],\n","       [7.7, 3. , 6.1, 2.3],\n","       [5.5, 2.5, 4. , 1.3],\n","       [4.4, 2.9, 1.4, 0.2],\n","       [4.3, 3. , 1.1, 0.1],\n","       [6. , 2.2, 5. , 1.5],\n","       [7.2, 3.2, 6. , 1.8],\n","       [4.6, 3.1, 1.5, 0.2],\n","       [5.1, 3.5, 1.4, 0.3],\n","       [4.4, 3. , 1.3, 0.2],\n","       [6.3, 2.5, 4.9, 1.5],\n","       [6.3, 3.4, 5.6, 2.4],\n","       [4.6, 3.4, 1.4, 0.3],\n","       [6.8, 3. , 5.5, 2.1],\n","       [6.3, 3.3, 6. , 2.5],\n","       [4.7, 3.2, 1.3, 0.2],\n","       [6.1, 2.9, 4.7, 1.4],\n","       [6.5, 2.8, 4.6, 1.5],\n","       [6.2, 2.8, 4.8, 1.8],\n","       [7. , 3.2, 4.7, 1.4],\n","       [6.4, 3.2, 5.3, 2.3],\n","       [5.1, 3.8, 1.6, 0.2],\n","       [6.9, 3.1, 5.4, 2.1],\n","       [5.9, 3. , 4.2, 1.5],\n","       [6.5, 3. , 5.2, 2. ],\n","       [5.7, 2.6, 3.5, 1. ],\n","       [5.2, 2.7, 3.9, 1.4],\n","       [6.1, 3. , 4.6, 1.4],\n","       [4.5, 2.3, 1.3, 0.3],\n","       [6.6, 2.9, 4.6, 1.3],\n","       [5.5, 2.6, 4.4, 1.2],\n","       [5.3, 3.7, 1.5, 0.2],\n","       [5.6, 3. , 4.1, 1.3],\n","       [7.3, 2.9, 6.3, 1.8],\n","       [6.7, 3.3, 5.7, 2.1],\n","       [5.1, 3.7, 1.5, 0.4],\n","       [4.9, 2.4, 3.3, 1. ],\n","       [6.7, 3.3, 5.7, 2.5],\n","       [7.2, 3. , 5.8, 1.6],\n","       [4.9, 3.6, 1.4, 0.1],\n","       [6.7, 3.1, 5.6, 2.4],\n","       [4.9, 3. , 1.4, 0.2],\n","       [6.9, 3.1, 4.9, 1.5],\n","       [7.4, 2.8, 6.1, 1.9],\n","       [6.3, 2.9, 5.6, 1.8],\n","       [5.7, 2.8, 4.1, 1.3],\n","       [6.5, 3. , 5.5, 1.8],\n","       [6.3, 2.3, 4.4, 1.3],\n","       [6.4, 2.9, 4.3, 1.3],\n","       [5.6, 2.8, 4.9, 2. ],\n","       [5.9, 3. , 5.1, 1.8],\n","       [5.4, 3.4, 1.7, 0.2],\n","       [6.1, 2.8, 4. , 1.3],\n","       [4.9, 2.5, 4.5, 1.7],\n","       [5.8, 4. , 1.2, 0.2],\n","       [5.8, 2.6, 4. , 1.2],\n","       [7.1, 3. , 5.9, 2.1]])"]},"metadata":{"tags":[]},"execution_count":122}]},{"cell_type":"code","metadata":{"id":"AQ4j-TWGlhH6","outputId":"36645e05-44c3-421c-fc51-29729d99b948"},"source":["scaled_X_train"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["array([[0.41176471, 0.40909091, 0.55357143, 0.5       ],\n","       [0.97058824, 0.45454545, 0.98214286, 0.83333333],\n","       [0.38235294, 0.45454545, 0.60714286, 0.58333333],\n","       [0.23529412, 0.68181818, 0.05357143, 0.04166667],\n","       [1.        , 0.36363636, 1.        , 0.79166667],\n","       [0.44117647, 0.31818182, 0.53571429, 0.375     ],\n","       [0.26470588, 0.63636364, 0.05357143, 0.04166667],\n","       [0.20588235, 0.68181818, 0.03571429, 0.08333333],\n","       [0.23529412, 0.81818182, 0.14285714, 0.125     ],\n","       [0.20588235, 0.        , 0.42857143, 0.375     ],\n","       [0.58823529, 0.31818182, 0.67857143, 0.70833333],\n","       [0.14705882, 0.63636364, 0.14285714, 0.04166667],\n","       [0.20588235, 0.45454545, 0.08928571, 0.04166667],\n","       [0.23529412, 0.59090909, 0.10714286, 0.16666667],\n","       [0.38235294, 0.31818182, 0.55357143, 0.5       ],\n","       [0.23529412, 0.63636364, 0.07142857, 0.04166667],\n","       [0.41176471, 0.45454545, 0.55357143, 0.45833333],\n","       [1.        , 0.81818182, 1.        , 0.875     ],\n","       [0.08823529, 0.54545455, 0.05357143, 0.04166667],\n","       [0.55882353, 0.40909091, 0.57142857, 0.5       ],\n","       [0.41176471, 0.22727273, 0.69642857, 0.79166667],\n","       [0.35294118, 1.        , 0.05357143, 0.04166667],\n","       [0.5       , 0.45454545, 0.66071429, 0.70833333],\n","       [0.44117647, 0.31818182, 0.71428571, 0.75      ],\n","       [0.5       , 0.09090909, 0.51785714, 0.375     ],\n","       [0.32352941, 0.45454545, 0.60714286, 0.58333333],\n","       [0.55882353, 0.63636364, 0.76785714, 0.91666667],\n","       [0.35294118, 0.13636364, 0.51785714, 0.5       ],\n","       [0.32352941, 0.86363636, 0.10714286, 0.125     ],\n","       [0.20588235, 0.13636364, 0.39285714, 0.375     ],\n","       [0.61764706, 0.31818182, 0.75      , 0.75      ],\n","       [0.20588235, 0.59090909, 0.05357143, 0.04166667],\n","       [0.20588235, 0.54545455, 0.01785714, 0.04166667],\n","       [0.35294118, 0.18181818, 0.48214286, 0.41666667],\n","       [0.70588235, 0.45454545, 0.69642857, 0.66666667],\n","       [0.17647059, 0.5       , 0.07142857, 0.04166667],\n","       [0.44117647, 0.36363636, 0.71428571, 0.95833333],\n","       [0.20588235, 0.63636364, 0.07142857, 0.04166667],\n","       [0.20588235, 0.68181818, 0.08928571, 0.20833333],\n","       [0.47058824, 0.54545455, 0.66071429, 0.70833333],\n","       [0.23529412, 0.22727273, 0.33928571, 0.41666667],\n","       [0.76470588, 0.54545455, 0.82142857, 0.91666667],\n","       [0.5       , 0.31818182, 0.71428571, 0.625     ],\n","       [0.52941176, 0.27272727, 0.80357143, 0.54166667],\n","       [1.        , 0.45454545, 0.89285714, 0.91666667],\n","       [0.35294118, 0.22727273, 0.51785714, 0.5       ],\n","       [0.02941176, 0.40909091, 0.05357143, 0.04166667],\n","       [0.        , 0.45454545, 0.        , 0.        ],\n","       [0.5       , 0.09090909, 0.69642857, 0.58333333],\n","       [0.85294118, 0.54545455, 0.875     , 0.70833333],\n","       [0.08823529, 0.5       , 0.07142857, 0.04166667],\n","       [0.23529412, 0.68181818, 0.05357143, 0.08333333],\n","       [0.02941176, 0.45454545, 0.03571429, 0.04166667],\n","       [0.58823529, 0.22727273, 0.67857143, 0.58333333],\n","       [0.58823529, 0.63636364, 0.80357143, 0.95833333],\n","       [0.08823529, 0.63636364, 0.05357143, 0.08333333],\n","       [0.73529412, 0.45454545, 0.78571429, 0.83333333],\n","       [0.58823529, 0.59090909, 0.875     , 1.        ],\n","       [0.11764706, 0.54545455, 0.03571429, 0.04166667],\n","       [0.52941176, 0.40909091, 0.64285714, 0.54166667],\n","       [0.64705882, 0.36363636, 0.625     , 0.58333333],\n","       [0.55882353, 0.36363636, 0.66071429, 0.70833333],\n","       [0.79411765, 0.54545455, 0.64285714, 0.54166667],\n","       [0.61764706, 0.54545455, 0.75      , 0.91666667],\n","       [0.23529412, 0.81818182, 0.08928571, 0.04166667],\n","       [0.76470588, 0.5       , 0.76785714, 0.83333333],\n","       [0.47058824, 0.45454545, 0.55357143, 0.58333333],\n","       [0.64705882, 0.45454545, 0.73214286, 0.79166667],\n","       [0.41176471, 0.27272727, 0.42857143, 0.375     ],\n","       [0.26470588, 0.31818182, 0.5       , 0.54166667],\n","       [0.52941176, 0.45454545, 0.625     , 0.54166667],\n","       [0.05882353, 0.13636364, 0.03571429, 0.08333333],\n","       [0.67647059, 0.40909091, 0.625     , 0.5       ],\n","       [0.35294118, 0.27272727, 0.58928571, 0.45833333],\n","       [0.29411765, 0.77272727, 0.07142857, 0.04166667],\n","       [0.38235294, 0.45454545, 0.53571429, 0.5       ],\n","       [0.88235294, 0.40909091, 0.92857143, 0.70833333],\n","       [0.70588235, 0.59090909, 0.82142857, 0.83333333],\n","       [0.23529412, 0.77272727, 0.07142857, 0.125     ],\n","       [0.17647059, 0.18181818, 0.39285714, 0.375     ],\n","       [0.70588235, 0.59090909, 0.82142857, 1.        ],\n","       [0.85294118, 0.45454545, 0.83928571, 0.625     ],\n","       [0.17647059, 0.72727273, 0.05357143, 0.        ],\n","       [0.70588235, 0.5       , 0.80357143, 0.95833333],\n","       [0.17647059, 0.45454545, 0.05357143, 0.04166667],\n","       [0.76470588, 0.5       , 0.67857143, 0.58333333],\n","       [0.91176471, 0.36363636, 0.89285714, 0.75      ],\n","       [0.58823529, 0.40909091, 0.80357143, 0.70833333],\n","       [0.41176471, 0.36363636, 0.53571429, 0.5       ],\n","       [0.64705882, 0.45454545, 0.78571429, 0.70833333],\n","       [0.58823529, 0.13636364, 0.58928571, 0.5       ],\n","       [0.61764706, 0.40909091, 0.57142857, 0.5       ],\n","       [0.38235294, 0.36363636, 0.67857143, 0.79166667],\n","       [0.47058824, 0.45454545, 0.71428571, 0.70833333],\n","       [0.32352941, 0.63636364, 0.10714286, 0.04166667],\n","       [0.52941176, 0.36363636, 0.51785714, 0.5       ],\n","       [0.17647059, 0.22727273, 0.60714286, 0.66666667],\n","       [0.44117647, 0.90909091, 0.01785714, 0.04166667],\n","       [0.44117647, 0.27272727, 0.51785714, 0.45833333],\n","       [0.82352941, 0.45454545, 0.85714286, 0.83333333]])"]},"metadata":{"tags":[]},"execution_count":123}]},{"cell_type":"markdown","metadata":{"id":"ogAZ5dRVlhH7"},"source":["## Building the Network with Keras\n","\n","Let's build a simple neural network!"]},{"cell_type":"code","metadata":{"id":"DUgKzsuJlhH7"},"source":["from keras.models import Sequential\n","from keras.layers import Dense"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"UTkmMJk7lhH7"},"source":["model = Sequential()\n","model.add(Dense(  input_dim=4, activation='relu'))\n","model.add(Dense(8, input_dim=4, activation='relu'))\n","model.add(Dense(3, activation='softmax'))\n","model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"-BqvhTgtlhH7","outputId":"002afb95-1593-4bef-e03b-6472fcbc5391"},"source":["model.summary()"],"execution_count":null,"outputs":[{"output_type":"stream","text":["_________________________________________________________________\n","Layer (type)                 Output Shape              Param #   \n","=================================================================\n","dense_27 (Dense)             (None, 8)                 40        \n","_________________________________________________________________\n","dense_28 (Dense)             (None, 8)                 72        \n","_________________________________________________________________\n","dense_29 (Dense)             (None, 3)                 27        \n","=================================================================\n","Total params: 139\n","Trainable params: 139\n","Non-trainable params: 0\n","_________________________________________________________________\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"QtHVHeHQlhH8"},"source":["## Fit (Train) the Model"]},{"cell_type":"code","metadata":{"id":"5pnWvccolhH8","outputId":"7717a8d6-b8bb-419a-f252-f3ad64985681"},"source":["# Play around with number of epochs as well!\n","model.fit(scaled_X_train,y_train,epochs=150, verbose=2)"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Epoch 1/150\n"," - 0s - loss: 1.0926 - acc: 0.3400\n","Epoch 2/150\n"," - 0s - loss: 1.0871 - acc: 0.3400\n","Epoch 3/150\n"," - 0s - loss: 1.0814 - acc: 0.3400\n","Epoch 4/150\n"," - 0s - loss: 1.0760 - acc: 0.3400\n","Epoch 5/150\n"," - 0s - loss: 1.0700 - acc: 0.3400\n","Epoch 6/150\n"," - 0s - loss: 1.0640 - acc: 0.3500\n","Epoch 7/150\n"," - 0s - loss: 1.0581 - acc: 0.3700\n","Epoch 8/150\n"," - 0s - loss: 1.0520 - acc: 0.4200\n","Epoch 9/150\n"," - 0s - loss: 1.0467 - acc: 0.5100\n","Epoch 10/150\n"," - 0s - loss: 1.0416 - acc: 0.5700\n","Epoch 11/150\n"," - 0s - loss: 1.0361 - acc: 0.6300\n","Epoch 12/150\n"," - 0s - loss: 1.0308 - acc: 0.6300\n","Epoch 13/150\n"," - 0s - loss: 1.0249 - acc: 0.6300\n","Epoch 14/150\n"," - 0s - loss: 1.0187 - acc: 0.6200\n","Epoch 15/150\n"," - 0s - loss: 1.0124 - acc: 0.6300\n","Epoch 16/150\n"," - 0s - loss: 1.0062 - acc: 0.6300\n","Epoch 17/150\n"," - 0s - loss: 0.9994 - acc: 0.6400\n","Epoch 18/150\n"," - 0s - loss: 0.9919 - acc: 0.6400\n","Epoch 19/150\n"," - 0s - loss: 0.9836 - acc: 0.6400\n","Epoch 20/150\n"," - 0s - loss: 0.9748 - acc: 0.6400\n","Epoch 21/150\n"," - 0s - loss: 0.9649 - acc: 0.6400\n","Epoch 22/150\n"," - 0s - loss: 0.9552 - acc: 0.6400\n","Epoch 23/150\n"," - 0s - loss: 0.9448 - acc: 0.6500\n","Epoch 24/150\n"," - 0s - loss: 0.9337 - acc: 0.6500\n","Epoch 25/150\n"," - 0s - loss: 0.9225 - acc: 0.6500\n","Epoch 26/150\n"," - 0s - loss: 0.9111 - acc: 0.6500\n","Epoch 27/150\n"," - 0s - loss: 0.8992 - acc: 0.6500\n","Epoch 28/150\n"," - 0s - loss: 0.8882 - acc: 0.6500\n","Epoch 29/150\n"," - 0s - loss: 0.8766 - acc: 0.6500\n","Epoch 30/150\n"," - 0s - loss: 0.8658 - acc: 0.6500\n","Epoch 31/150\n"," - 0s - loss: 0.8555 - acc: 0.6500\n","Epoch 32/150\n"," - 0s - loss: 0.8446 - acc: 0.6500\n","Epoch 33/150\n"," - 0s - loss: 0.8337 - acc: 0.6500\n","Epoch 34/150\n"," - 0s - loss: 0.8225 - acc: 0.6500\n","Epoch 35/150\n"," - 0s - loss: 0.8112 - acc: 0.6500\n","Epoch 36/150\n"," - 0s - loss: 0.7998 - acc: 0.6500\n","Epoch 37/150\n"," - 0s - loss: 0.7886 - acc: 0.6500\n","Epoch 38/150\n"," - 0s - loss: 0.7768 - acc: 0.6500\n","Epoch 39/150\n"," - 0s - loss: 0.7650 - acc: 0.6500\n","Epoch 40/150\n"," - 0s - loss: 0.7539 - acc: 0.6500\n","Epoch 41/150\n"," - 0s - loss: 0.7428 - acc: 0.6500\n","Epoch 42/150\n"," - 0s - loss: 0.7318 - acc: 0.6500\n","Epoch 43/150\n"," - 0s - loss: 0.7213 - acc: 0.6500\n","Epoch 44/150\n"," - 0s - loss: 0.7111 - acc: 0.6500\n","Epoch 45/150\n"," - 0s - loss: 0.7007 - acc: 0.6500\n","Epoch 46/150\n"," - 0s - loss: 0.6907 - acc: 0.6500\n","Epoch 47/150\n"," - 0s - loss: 0.6806 - acc: 0.6500\n","Epoch 48/150\n"," - 0s - loss: 0.6714 - acc: 0.6500\n","Epoch 49/150\n"," - 0s - loss: 0.6621 - acc: 0.6500\n","Epoch 50/150\n"," - 0s - loss: 0.6534 - acc: 0.6500\n","Epoch 51/150\n"," - 0s - loss: 0.6452 - acc: 0.6500\n","Epoch 52/150\n"," - 0s - loss: 0.6366 - acc: 0.6500\n","Epoch 53/150\n"," - 0s - loss: 0.6285 - acc: 0.6500\n","Epoch 54/150\n"," - 0s - loss: 0.6205 - acc: 0.6500\n","Epoch 55/150\n"," - 0s - loss: 0.6130 - acc: 0.6500\n","Epoch 56/150\n"," - 0s - loss: 0.6058 - acc: 0.6500\n","Epoch 57/150\n"," - 0s - loss: 0.5990 - acc: 0.6500\n","Epoch 58/150\n"," - 0s - loss: 0.5920 - acc: 0.6500\n","Epoch 59/150\n"," - 0s - loss: 0.5852 - acc: 0.6700\n","Epoch 60/150\n"," - 0s - loss: 0.5790 - acc: 0.6700\n","Epoch 61/150\n"," - 0s - loss: 0.5727 - acc: 0.6900\n","Epoch 62/150\n"," - 0s - loss: 0.5665 - acc: 0.6900\n","Epoch 63/150\n"," - 0s - loss: 0.5605 - acc: 0.6900\n","Epoch 64/150\n"," - 0s - loss: 0.5544 - acc: 0.6900\n","Epoch 65/150\n"," - 0s - loss: 0.5490 - acc: 0.6900\n","Epoch 66/150\n"," - 0s - loss: 0.5436 - acc: 0.7000\n","Epoch 67/150\n"," - 0s - loss: 0.5385 - acc: 0.7000\n","Epoch 68/150\n"," - 0s - loss: 0.5334 - acc: 0.7000\n","Epoch 69/150\n"," - 0s - loss: 0.5287 - acc: 0.7100\n","Epoch 70/150\n"," - 0s - loss: 0.5243 - acc: 0.7200\n","Epoch 71/150\n"," - 0s - loss: 0.5198 - acc: 0.7300\n","Epoch 72/150\n"," - 0s - loss: 0.5150 - acc: 0.7300\n","Epoch 73/150\n"," - 0s - loss: 0.5108 - acc: 0.7400\n","Epoch 74/150\n"," - 0s - loss: 0.5066 - acc: 0.7400\n","Epoch 75/150\n"," - 0s - loss: 0.5022 - acc: 0.7600\n","Epoch 76/150\n"," - 0s - loss: 0.4986 - acc: 0.7500\n","Epoch 77/150\n"," - 0s - loss: 0.4945 - acc: 0.7400\n","Epoch 78/150\n"," - 0s - loss: 0.4908 - acc: 0.7500\n","Epoch 79/150\n"," - 0s - loss: 0.4867 - acc: 0.7700\n","Epoch 80/150\n"," - 0s - loss: 0.4830 - acc: 0.7700\n","Epoch 81/150\n"," - 0s - loss: 0.4794 - acc: 0.7900\n","Epoch 82/150\n"," - 0s - loss: 0.4761 - acc: 0.8500\n","Epoch 83/150\n"," - 0s - loss: 0.4726 - acc: 0.8500\n","Epoch 84/150\n"," - 0s - loss: 0.4693 - acc: 0.8500\n","Epoch 85/150\n"," - 0s - loss: 0.4660 - acc: 0.8500\n","Epoch 86/150\n"," - 0s - loss: 0.4628 - acc: 0.8500\n","Epoch 87/150\n"," - 0s - loss: 0.4599 - acc: 0.8200\n","Epoch 88/150\n"," - 0s - loss: 0.4575 - acc: 0.8000\n","Epoch 89/150\n"," - 0s - loss: 0.4546 - acc: 0.7800\n","Epoch 90/150\n"," - 0s - loss: 0.4521 - acc: 0.7800\n","Epoch 91/150\n"," - 0s - loss: 0.4490 - acc: 0.8100\n","Epoch 92/150\n"," - 0s - loss: 0.4460 - acc: 0.8300\n","Epoch 93/150\n"," - 0s - loss: 0.4433 - acc: 0.8500\n","Epoch 94/150\n"," - 0s - loss: 0.4407 - acc: 0.8500\n","Epoch 95/150\n"," - 0s - loss: 0.4380 - acc: 0.8500\n","Epoch 96/150\n"," - 0s - loss: 0.4352 - acc: 0.8600\n","Epoch 97/150\n"," - 0s - loss: 0.4326 - acc: 0.8600\n","Epoch 98/150\n"," - 0s - loss: 0.4302 - acc: 0.9000\n","Epoch 99/150\n"," - 0s - loss: 0.4270 - acc: 0.9100\n","Epoch 100/150\n"," - 0s - loss: 0.4245 - acc: 0.9200\n","Epoch 101/150\n"," - 0s - loss: 0.4223 - acc: 0.9200\n","Epoch 102/150\n"," - 0s - loss: 0.4195 - acc: 0.9200\n","Epoch 103/150\n"," - 0s - loss: 0.4171 - acc: 0.9200\n","Epoch 104/150\n"," - 0s - loss: 0.4147 - acc: 0.9200\n","Epoch 105/150\n"," - 0s - loss: 0.4124 - acc: 0.9000\n","Epoch 106/150\n"," - 0s - loss: 0.4102 - acc: 0.9000\n","Epoch 107/150\n"," - 0s - loss: 0.4077 - acc: 0.9200\n","Epoch 108/150\n"," - 0s - loss: 0.4055 - acc: 0.9200\n","Epoch 109/150\n"," - 0s - loss: 0.4028 - acc: 0.9200\n","Epoch 110/150\n"," - 0s - loss: 0.4006 - acc: 0.9200\n","Epoch 111/150\n"," - 0s - loss: 0.3985 - acc: 0.9200\n","Epoch 112/150\n"," - 0s - loss: 0.3965 - acc: 0.9200\n","Epoch 113/150\n"," - 0s - loss: 0.3945 - acc: 0.9200\n","Epoch 114/150\n"," - 0s - loss: 0.3930 - acc: 0.9200\n","Epoch 115/150\n"," - 0s - loss: 0.3914 - acc: 0.9000\n","Epoch 116/150\n"," - 0s - loss: 0.3891 - acc: 0.9100\n","Epoch 117/150\n"," - 0s - loss: 0.3860 - acc: 0.9200\n","Epoch 118/150\n"," - 0s - loss: 0.3844 - acc: 0.9200\n","Epoch 119/150\n"," - 0s - loss: 0.3821 - acc: 0.9500\n","Epoch 120/150\n"," - 0s - loss: 0.3801 - acc: 0.9500\n","Epoch 121/150\n"," - 0s - loss: 0.3780 - acc: 0.9500\n","Epoch 122/150\n"," - 0s - loss: 0.3757 - acc: 0.9500\n","Epoch 123/150\n"," - 0s - loss: 0.3738 - acc: 0.9500\n","Epoch 124/150\n"," - 0s - loss: 0.3720 - acc: 0.9500\n","Epoch 125/150\n"," - 0s - loss: 0.3693 - acc: 0.9500\n","Epoch 126/150\n"," - 0s - loss: 0.3662 - acc: 0.9500\n","Epoch 127/150\n"," - 0s - loss: 0.3646 - acc: 0.9300\n","Epoch 128/150\n"," - 0s - loss: 0.3652 - acc: 0.9200\n","Epoch 129/150\n"," - 0s - loss: 0.3640 - acc: 0.9100\n","Epoch 130/150\n"," - 0s - loss: 0.3627 - acc: 0.9100\n","Epoch 131/150\n"," - 0s - loss: 0.3604 - acc: 0.9100\n","Epoch 132/150\n"," - 0s - loss: 0.3572 - acc: 0.9300\n","Epoch 133/150\n"," - 0s - loss: 0.3544 - acc: 0.9300\n","Epoch 134/150\n"," - 0s - loss: 0.3521 - acc: 0.9300\n","Epoch 135/150\n"," - 0s - loss: 0.3501 - acc: 0.9500\n","Epoch 136/150\n"," - 0s - loss: 0.3482 - acc: 0.9500\n","Epoch 137/150\n"," - 0s - loss: 0.3465 - acc: 0.9500\n","Epoch 138/150\n"," - 0s - loss: 0.3461 - acc: 0.9500\n","Epoch 139/150\n"," - 0s - loss: 0.3435 - acc: 0.9300\n","Epoch 140/150\n"," - 0s - loss: 0.3407 - acc: 0.9500\n","Epoch 141/150\n"," - 0s - loss: 0.3384 - acc: 0.9500\n","Epoch 142/150\n"," - 0s - loss: 0.3366 - acc: 0.9500\n","Epoch 143/150\n"," - 0s - loss: 0.3347 - acc: 0.9500\n","Epoch 144/150\n"," - 0s - loss: 0.3336 - acc: 0.9400\n","Epoch 145/150\n"," - 0s - loss: 0.3324 - acc: 0.9300\n","Epoch 146/150\n"," - 0s - loss: 0.3311 - acc: 0.9300\n","Epoch 147/150\n"," - 0s - loss: 0.3300 - acc: 0.9300\n","Epoch 148/150\n"," - 0s - loss: 0.3283 - acc: 0.9300\n","Epoch 149/150\n"," - 0s - loss: 0.3256 - acc: 0.9400\n","Epoch 150/150\n"," - 0s - loss: 0.3224 - acc: 0.9500\n"],"name":"stdout"},{"output_type":"execute_result","data":{"text/plain":["<keras.callbacks.History at 0x1fefed36ac8>"]},"metadata":{"tags":[]},"execution_count":135}]},{"cell_type":"markdown","metadata":{"id":"vFGuIxEolhH8"},"source":["## Predicting New Unseen Data\n","\n","Let's see how we did by predicting on **new data**. Remember, our model has **never** seen the test data that we scaled previously! This process is the exact same process you would use on totally brand new data. For example , a brand new bank note that you just analyzed ."]},{"cell_type":"code","metadata":{"id":"0KGt8bZXlhH8"},"source":["scaled_X_test"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"collapsed":true,"id":"HnNN6XMflhH9"},"source":["# Spits out probabilities by default.\n","# model.predict(scaled_X_test)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"ncDQBlcHlhH9"},"source":["model.predict_classes(scaled_X_test)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"jFNEb0YllhH9"},"source":["# Evaluating Model Performance\n","\n","So how well did we do? How do we actually measure \"well\". Is 95% accuracy good enough? It all depends on the situation. Also we need to take into account things like recall and precision. Make sure to watch the video discussion on classification evaluation before running this code!"]},{"cell_type":"code","metadata":{"id":"L2C7fbOjlhH9","outputId":"4cadbe63-a37f-4d08-b9fb-5125290f3855"},"source":["model.metrics_names"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["['loss', 'acc']"]},"metadata":{"tags":[]},"execution_count":139}]},{"cell_type":"code","metadata":{"id":"hRg7tlCNlhH-","outputId":"7bd01ff8-9be3-470b-b286-34f7eadb2e56"},"source":["model.evaluate(x=scaled_X_test,y=y_test)"],"execution_count":null,"outputs":[{"output_type":"stream","text":["50/50 [==============================] - 0s 2ms/step\n"],"name":"stdout"},{"output_type":"execute_result","data":{"text/plain":["[0.2843634402751923, 0.96]"]},"metadata":{"tags":[]},"execution_count":140}]},{"cell_type":"code","metadata":{"collapsed":true,"id":"yyE_M4KhlhH-"},"source":["from sklearn.metrics import confusion_matrix,classification_report"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"collapsed":true,"id":"n2xpnbdBlhH-"},"source":["predictions = model.predict_classes(scaled_X_test)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"fxiDQLXMlhH-","outputId":"bf2b9dce-9588-4b26-b627-9bee5df5c997"},"source":["predictions"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["array([1, 0, 2, 1, 2, 0, 1, 2, 2, 1, 2, 0, 0, 0, 0, 1, 2, 1, 1, 2, 0, 2,\n","       0, 2, 2, 2, 2, 2, 0, 0, 0, 0, 1, 0, 0, 2, 1, 0, 0, 0, 2, 1, 1, 0,\n","       0, 1, 2, 2, 1, 2], dtype=int64)"]},"metadata":{"tags":[]},"execution_count":161}]},{"cell_type":"code","metadata":{"id":"9ho2eS0ulhH-","outputId":"05baa917-5340-44b0-d727-7c10f5dcc064"},"source":["y_test.argmax(axis=1)"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["array([1, 0, 2, 1, 1, 0, 1, 2, 1, 1, 2, 0, 0, 0, 0, 1, 2, 1, 1, 2, 0, 2,\n","       0, 2, 2, 2, 2, 2, 0, 0, 0, 0, 1, 0, 0, 2, 1, 0, 0, 0, 2, 1, 1, 0,\n","       0, 1, 2, 2, 1, 2], dtype=int64)"]},"metadata":{"tags":[]},"execution_count":163}]},{"cell_type":"code","metadata":{"id":"H4FDonVHlhH_","outputId":"9885061f-df56-4da6-b5ca-21aba176196e"},"source":["confusion_matrix(y_test.argmax(axis=1),predictions)"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["array([[19,  0,  0],\n","       [ 0, 13,  2],\n","       [ 0,  0, 16]], dtype=int64)"]},"metadata":{"tags":[]},"execution_count":164}]},{"cell_type":"code","metadata":{"id":"yiyUe9zUlhH_","outputId":"38bf6151-eebb-41b1-ca03-3c97e160213c"},"source":["print(classification_report(y_test.argmax(axis=1),predictions))"],"execution_count":null,"outputs":[{"output_type":"stream","text":["              precision    recall  f1-score   support\n","\n","           0       1.00      1.00      1.00        19\n","           1       1.00      0.87      0.93        15\n","           2       0.89      1.00      0.94        16\n","\n","   micro avg       0.96      0.96      0.96        50\n","   macro avg       0.96      0.96      0.96        50\n","weighted avg       0.96      0.96      0.96        50\n","\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"wBRHcIm8lhH_"},"source":["## Saving and Loading Models\n","\n","Now that we have a model trained, let's see how we can save and load it."]},{"cell_type":"code","metadata":{"collapsed":true,"id":"Xlu1mqfAlhIA"},"source":["model.save('myfirstmodel.h5')"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"collapsed":true,"id":"sLAHGUoblhIA"},"source":["from keras.models import load_model"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"collapsed":true,"id":"E-4z0Sw9lhIA"},"source":["newmodel = load_model('myfirstmodel.h5')"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"0bPobO55lhIA","outputId":"97a42d2e-7681-4036-c521-003a8373fd41"},"source":["newmodel.predict_classes(X_test)"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["array([2, 1, 2, 2, 2, 1, 2, 2, 2, 2, 2, 1, 1, 1, 1, 2, 2, 2, 2, 2, 1, 2,\n","       1, 2, 2, 2, 2, 2, 1, 1, 1, 1, 2, 1, 1, 2, 2, 1, 1, 1, 2, 2, 2, 1,\n","       1, 2, 2, 2, 2, 2], dtype=int64)"]},"metadata":{"tags":[]},"execution_count":169}]},{"cell_type":"markdown","metadata":{"id":"uHcq3PU7lhIA"},"source":["Great job! You now know how to preprocess data, train a neural network, and evaluate its classification performance!"]}]}