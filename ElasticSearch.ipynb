{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"ElasticSearch.ipynb","provenance":[],"collapsed_sections":["32BiW_NpwluV","cUdsK2d61cvs","ipHDjxqEuXxQ"],"toc_visible":true},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","metadata":{"id":"view-in-github"},"source":["<a href=\"https://colab.research.google.com/github/zanasaed/NoteBook/blob/main/ElasticSearch.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"]},{"cell_type":"markdown","metadata":{"id":"Qkqmnz531SiI"},"source":["#Links of library and tutorial "]},{"cell_type":"markdown","metadata":{"id":"ElDe7yDS8F2J"},"source":["    [FreeCourseLab.com] Udemy - Elasticsearch 6 and Elastic Stack - In Depth and Hands On!\n","\n","[sundog-education](https://sundog-education.com/elasticsearch/)\n","\n","\n","doc: [offical](https://www.elastic.co/guide/en/elasticsearch/reference/current/index.html)\n"]},{"cell_type":"markdown","metadata":{"id":"32BiW_NpwluV"},"source":["# How install "]},{"cell_type":"markdown","metadata":{"id":"UF_1XZJ7SNhg"},"source":["look main website for [instruction](https://www.elastic.co/guide/en/elasticsearch/reference/current/deb.html) \n","\n","list of error that maybe see in instalation :\n","\n","`gpg: no valid OpenPGP data found.`\n","[solution](https://stackoverflow.com/questions/21338721/gpg-no-valid-openpgp-data-found) \n","be sure you use VPN \n","\n","[Elasticsearch: Job for elasticsearch.service failed\n","](https://stackoverflow.com/questions/58656747/elasticsearch-job-for-elasticsearch-service-failed)\n","\n","\n","to see elastic status `sudo systemctl status elasticsearch`\n","\n","\n","to test elastic is runing\n","`curl 127.0.0.1:9200`\n"]},{"cell_type":"markdown","metadata":{"id":"LN1vB11jwpYx"},"source":[""]},{"cell_type":"markdown","metadata":{"id":"cUdsK2d61cvs"},"source":["# definition \n"]},{"cell_type":"markdown","metadata":{"id":"FiPiMq6nQmR2"},"source":["## Key Concepts\n","\n","**Node**\n","\n","It refers to a single running instance of Elasticsearch. Single physical and virtual server accommodates multiple nodes depending upon the capabilities of their physical resources like RAM, storage and processing power.\n","\n","---\n","\n","**Cluster**\n","\n","It is a collection of one or more nodes. Cluster provides collective indexing and search capabilities across all the nodes for entire data.\n","\n","---\n"," \n","**Index** (indeces) Like DataBase \n","\n","It is a collection of different type of documents and their properties. Index also uses the concept of shards to improve the performance. For example, a set of document contains data of a social networking application.\n","\n","An index powers search into all documents withn a collection of types. They contain inverted indices that let you search across everything within them at once.\n","\n","---\n","\n","**Document** Like Row \n","\n","It is a collection of fields in a specific manner defined in JSON format. Every document belongs to a type and resides inside an index. Every document is associated with a unique identifier called the UID.\n","\n","Documents are the things you're searching for.They can be more text - any structured JSON data works. Every document has a unique ID,and a type.\n","\n","---\n","\n","**types** Like Table \n","A type define the schema and mapping shared by documents that represent the same sort of things. ( A log entry, an encyclopedia article, etc.)\n","\n","\n","**Note** : Elasticsearch is moving toward eliminating the concept of type. in Elasticsearch 6, only one type is allowed per index.\n","\n","---\n","**Shard**\n","\n","Indexes are horizontally subdivided into shards. This means each shard contains all the properties of document but contains less number of JSON objects than index. The horizontal separation makes shard an independent node, which can be store in any node. Primary shard is the original horizontal part of an index and then these primary shards are replicated into replica shards.\n","\n","---\n","\n","\n","**Replicas**\n","\n","Elasticsearch allows a user to create replicas of their indexes and shards. Replication not only helps in increasing the availability of data in case of failure, but also improves the performance of searching by carrying out a parallel search operation in these replicas.\n","\n","\n"]},{"cell_type":"markdown","metadata":{"id":"wML3NITuS7ku"},"source":["# Mapping and Indexing Data\n"]},{"cell_type":"markdown","metadata":{"id":"DEOAnElbS-4o"},"source":["## Definition \n","a maping is a schema definition.\n","\n","elasticsearach has resonbale defaults, but somtimes you need to customize them.\n","\n","we can use maping for more things like:\n","\n","**field types** :\n","\n","text, keyboard, byte, short, integer, long, float, double, boolean, data\n","\n","\n","```\n","\"properties\" : {\n","    \"user_id\" : {\n","        \"type\" : \"long\"\n","    }\n","}\n","```\n","\n","\n","**field index** : \n","\n","do you want this field to be queryable? true/false \n","\n","\n","```\n","\"properties\" : {\n","    \"genre\" : {\n","        \"index\" : \"false\"\n","    }\n","}\n","```\n","\n","\n","**field analyzer** : \n","\n","define your tokenizer and token filter. standard / whitespace / simple / english etc. \n","\n","\n","```\n","\"properties\" : {\n","    \"dascription\" : {\n","        \"analyzer\" : \"english\"\n","    }\n","}\n","```\n","**more about analyzers**\n","\n","There are three things analyzer can do.\n","\n","* **character filters** : \n","\n","    analyzer can \n","    remove HTML encoding, convert \"&\" to \"and\" word\n","\n","    The idea here is that if you apply the same analyzer to both your search \n","    query and the information that's index, you can get rid of all these \n","    discrepancies\n","\n","\n","* **tokenizer** : \n","\n","    spilt strings on whitespace / punctuation / non-letters \n","\n","    specifiy how a sting gets broken up into search terms that get indexed. \n","\n","* **token filter** : \n"," \n","    lowercasing, stemming, synonyms, stopwords\n","\n","    if you want your search be in case sensetive you might want token filter to \n","    lowecase everything\n","\n","    you can also handel stemming. for example if you want the box and boxes and \n","    boxing and boxed to all match for a search for the term box, you can do the \n","    stemming to normalize all those diffrent variant of the given word to the \n","    same root stem\n","\n","    synonyms : if you want to swarch for the word big to also tuen up document \n","    that contain the word large, your token filter could normalize all those to\n","    a given synonym, or to make sure that those also work as well.\n","\n","    stop word : if you don't wnat to waste space storing thingd like (the, and,\n","    a , that ,etc..), you might want to apply a stop word filter as well to your\n","    searches, so you don't end up indexing all that stuff too.\n","\n","\n","**choices for analyzers**\n","\n","* **standard** : \n","\n","    spilt on word boundaries, remove punctuation, lowercases. good choice if\n","    language is unknown\n","\n","* **simple** : \n","\n","    split on anything that isn't a letter, and lowercases\n","\n","* **whitesoace** : \n","\n","    split on whitespace but doesn't lowercase\n","\n","* **language**(i.e. english) : \n","\n","    accounts for language-specific stopwords and stemming\n","\n","\n","**NOTE**: Sometimes you can even mix and match different languages in the same index. there are some tricks you can do to actualy run diffrent analyzer on the same text, and store those as diffrent fields. so i could, for example have an English vertion and a french version of the information in my index. that i can search separtely.\n"]},{"cell_type":"markdown","metadata":{"id":"mQEp_2kFjKdd"},"source":["## example\n","\n","we are using curl to send an HTTP request with a content type header that indicates JSON data.\n","\n"," a PUT action to our localhost on port 9200, affecting the movies index, and we're defining  an explicit mapping for the movie type that change the property of the year to be interpreted  as a date type.\n","\n","\n","```\n","curl -H \"Content-Type: application/json\" -XPUT 127.0.0.1:9200/movies -d'\n","{\n","    \"mappings\" : {\n","        \"movie\" : {\n","             \"properties\" : {\n","                \"year\" : { \n","                    \"type\": \"date\"\n","                    }\n","            }\n","        }\n","    }\n","}'\n","```\n","-H --> content type header \n","\n","-XPUT --> PUT portocol \n","\n","127.0.0.1 elasticsearch addresses in our system\n","\n",":9200 --> elastic port \n","\n","/movies --> movies indexs (database)\n","\n","\n","-d' --> tells curl that the following lines are gonna be sent as the body of the HTTP request. and body should be JSON data.\n","\n","**Note** : to press tab space in curl body  press ctrl+v then enter tab button \n","\n","Now if you want to retrieve that, and make sure it actually took, we can use a get action to get that mapping back, and take look at it.\n","```\n","curl -H \"Content-Type: application/json\" -XGET 127.0.0.1:9200/movies/_mapping/movie?pretty\n","```\n","_mapping/movie --> to retrive the mapping associated with that index,specifically for the movie type \n","\n","?pretty --> return response in a pretty formatted response."]},{"cell_type":"markdown","metadata":{"id":"vamOvseA-i6o"},"source":["## Hacking CURL (Don't Skip This!)"]},{"cell_type":"markdown","metadata":{"id":"gZ836ToN-lDT"},"source":["**Instructor**: One of the things that changed\n","in Elasticsearch six is that every request that goes to your Elasticsearch server needs to have a content header that specifies a json content-type.\n","And typing that in every time we issue a curl command to communicate with our server can get very tedious.\n","\n","So let me show you a little hack that let's you avoid doing that.\n","\n","we're going to create our own curl script that lives in bin directory, and this curl script is going to override the one built into the system.\n","\n","So it's gonna be a little bash script that just calls the real curl script under user bin curl, but automatically passes in that `-H \"Content-Type application/json\"` parameter for us, so we don't have to keep typing it over and over and over again. And that dollar sign at sign at the end of the line there within our script says go ahead and append all the other parameters that we had on the command line for that curl command.\n","\n","without this hack in place, you will need to add `-H \"Content-Type application/json\"` to every single curl command in this course.\n","\n","In the real world, you're not gonna be using curl for the most part anyway.\n","\n","You're gonna be communicating with your Elasticsearch server using some sort of coding library or some sort of larger framework. Generally speaking, you don't wanna be sneaking around behind the back of your system and making commands silently do things that they weren't designed to do originally. That can become very confusing if you were to try to issue a curl command that required a different content-type header.\n","\n","You'd be left wondering why is it not doing what I'm telling it to do if you didn't remember that you had this thing in place.\n","\n","<br>\n","<br>\n","Let's work on this together and get it in place. \n","\n","\n","```\n","From your home directory:\n","mkdir bin\n","cd bin \n","vi curl(Hit I fir insert mode)\n","\n","#!/bin/bash\n","/usr/bin/curl -H \"Content-Type: application/json\" \"$@\"\n","\n","Esc - :wq - enter \n","\n","chmod a+x curl #gives executable permissions to everybody\n","\n","\n","```\n","\n","the neat thing about Ubuntu is that any scripts that are in your home directory's bin directory will take precedence over other scripts on your system.\n","\n","So this how we're going to trick Ubuntu into using our version of curl instead of the default one.\n","\n","\n","\n","\n","\n","Basically it takes all the parameters to the curl command and sticks it on the end of the real curl command in our system, but automatically sticks in that dash h content-type application json header for us, so we never have to type it again.\n","\n","Now to make it an executable script,\n","\n","there's one more thing we need to do.\n","\n","chmod, for change mod, that gives executable permissions to everybody,\n","\n","to make sure that it's working properly, in home directory if we type in which curl, it will tell us the path that the curl script will be executed from.\n","\n"]},{"cell_type":"markdown","metadata":{"id":"VyHncvTwI5px"},"source":["##  Import a Single Document  via JSON  REST\n","\n","\n","```\n","curl -XPUT 1270.0.0.2:9200/movies/movie/109487 -d'\n","{\"genre\":[\"IMAX\",\"Sci-Fi\"],\n","\"title\" : \"Insterstellar\",\n","\"year\" : 2014\n","}'\n","```\n","\n","\n","##  Import a Many Documents  via JSON  REST\n","\n","\n","```\n","curl -XPUT 127.0.0.1:9200/_bulk -d'\n","{\"create\": {\"_index\" : \"movies\" , \"_type\" : \"Movie\" , \"_id\" : \"135569\"}}\n","{\"_id\": \"135569\", \"title\" : \"Star Trek Beyond\", \"year\" : 2016}\n","{\"create\": {\"_index\" : \"movies\" , \"_type\" : \"Movie\" , \"_id\" : \"109487\"}}\n","{\"_id\": \"109487\", \"title\" : \"Insterstellar\", \"year\" : 2014}\n","{\"create\": {\"_index\" : \"movies\" , \"_type\" : \"Movie\" , \"_id\" : \"58559\"}}\n","{\"_id\": \"58559\", \"title\" : \"Dark knight, the\", \"year\" : 2008}\n","\n","```\n","\n","we can also use json file that has this row and insert it in indeses \n","\n","```\n","curl -XPUT 127.0.0.1:9200/_bulk?pretty --data-binary @myfile.json \n","```\n","\n"]},{"cell_type":"markdown","metadata":{"id":"S47eCAXkKaNt"},"source":["##  Updating Data in Elasticsearch\n","\n","Every document has a _version field \n","\n","Elasticsearch documents are immutable.\n","\n","When you update an existing document :\n","\n","a new document is created whit a incremented _version the old document is marked for deletion\n","\n","\n","**Curl format**:\n","\n","```\n","curl -XPOST 127.0.0.1:9200/movies/movie/109487/_update -d ' \n","{\n","    \"doc\" : {\n","        \"title\": \"Interseteller\"\n","    }\n","}'\n","```\n"]},{"cell_type":"markdown","metadata":{"id":"w2OdOpHcNCtU"},"source":["## Deleting Data in Elasticsearch\n"," \n"," for deleting doc that you know the id  just use :\n","\n"," ` curl -XDELETE 127.0.0.1:9200/movies/movie/58559`"]},{"cell_type":"markdown","metadata":{"id":"h7ZQRxfocbGp"},"source":["## Dealing with Concurrency\n","\n","to pervent update data from multi user at the same time we can use \n","\n","Optimistic concurrency control in action \n","\n","1. set the version of doc in the update command \n","2. ues retry_on_confilict in update command \n","\n","``` \n","curl -XPOST 127.0.0.1:9200/movies/movie/id/_update?retry_on_conflict=5 -d '\n","{\n","    \"doc\" : {\n","        \"title\" : \"blublu blu\"\n","    }\n","}'\n","\n","```\n","retry_on_conflict=5 : Hoe many time retry before giveing up \n","\n","```\n","\n","curl -XPOST 127.0.0.1:9200/movies/movie/id/_update?version=3 -d '\n","{\n","    \"doc\" : {\n","        \"title\" : \"blublu blu\"\n","    }\n","}'\n","\n","\n","```\n","\n","version=3 : doc version of the last see that we know \n","for get doc version look at the doc by\n"," \n","`curl -XGET 127.0.0.1:9200/movies/movie/id?pretty`\n"]},{"cell_type":"markdown","metadata":{"id":"Qhkln3s-xScz"},"source":["# **Search**"]},{"cell_type":"markdown","metadata":{"id":"1J9TEk2kRY3O"},"source":["## **query lite** "]},{"cell_type":"markdown","metadata":{"id":"DbfdaEGPxXmf"},"source":["\n","for search simple and fast we can usae this format query in web browser \n","\n","\n","```\n","INDEX/TYPE/_search?q=title:str\n","movies/move/_search?q=+year:>2010+title:trek\n","```\n","\n","example\n","return all seller that kind it is seller_data\n","```\n","http://localhost:9200/digikala/seller/_search?q=kind:seller_data&pretty\n","```\n","return all movies that cotains star in the title \n","```\n","curl -X GET \"localhost:9200/movies/movie/_search?q=title:star&pretty\"\n","```\n","\n","returns all movie that have word trek in title and released after the year 2010\n","```\n","curl -X GET \"localhost:9200/movies/movie/_search?q=%2Byear%3A%3E2010%2Btitle%3Atrek&pretty\"\n","```\n","+year:> --> %2Byear%3A%3E\n","%2B it is the hexadecimal code for the ascii value of plus \"+\"\n","in Kibana : \n","\n","```\n","GET digikala/seller/_search?\n","```\n","\n","\n","`curl -X GET \"localhost:9200/digikala/seller/_search?q=kind:seller_data\"`\n"]},{"cell_type":"markdown","metadata":{"id":"qpodxsIly6-E"},"source":["The problem is when you're sending across urls you need to encode them very often and that can make things very confusing very quickly.\n","\n","\n","So you couldn't actually reliably send across that string as is as part of a rest request you would have to url rncoded it in order to securely transmit it over the Internet. And that means that any special characters need to be encoded into their hexadecimal equivalency or encoded out.\n","\n","<br>\n","\n","If you want **learn more** just search for **URI  search**.\n","this is formally caleed \"URL Search\". search for that on the Elasticsearch documentation. \n","\n","it's really quite powerful, but again is only appropriate for quick \"curl tests\".\n","\n","<br>\n","\n","Reasons  why you should not use this little shortcut in production for sure :\n","\n","*   cryptic and tough to debug \n","*   can be a security issue if exposed end users \n","*   fragile- one wrong character and you're hosed.\n","\n","\n"]},{"cell_type":"markdown","metadata":{"id":"9yyJEevHuMxv"},"source":["**Counting number of documents using Elasticsearch**\n","[stacklink](https://stackoverflow.com/questions/25739888/counting-number-of-documents-using-elasticsearch)  \n","query in terminal : \n","\n","```\n","curl http://localhost:9200/_cat/indices?v\n","```\n","\n","\n","\n","query in Kibana :\n","\n","\n","\n","```\n","GET /_cat/indices?v\n","```\n","\n"]},{"cell_type":"markdown","metadata":{"id":"tbf1gt7IWdtV"},"source":["## Using JSON Search "]},{"cell_type":"markdown","metadata":{"id":"3RCkZGOsWvhZ"},"source":["insted useing url search we using json search that it is better \n","\n","\n","\n","**get requests can have a body**\n","\n","Usually, when you talk about GET requests on HTTP requests you're talking about just retrieving a web page from a web server but you can actually send a body along with a GET request as well.\n","Things you can do within a query.You can have two different things.\n","1. filters\n","2. queries\n","\n","There are filters and then there are queries so queries are usually used for returning data in terms of relevance.\n","\n","When we're doing something like searching for the search term track you would use a query because you want to get back results in orders of relevance as to how relevant the term truck was to that given document.\n","\n","\n","However, if you have a binary operation where the answer is basically yes or no you want to use a filter instead.So filters are much more efficient than queries.\n","\n","Because not only are they faster but the results can be cached by elasticsearch or so that if you do another query using the same filter they can get the results even faster.\n"]},{"cell_type":"markdown","metadata":{"id":"lqK46ZbjWvcP"},"source":["### Example of boolean query with a filter \n","we have a query that contains a boolean expression where you must have trek in the title and you also must have the filter pass the condition of the year being greater than or equal to 2010.\n","```\n","curl http://localhost:9200/movies/movie/_search?pretty -d' \n","{\n","    \"query\":{\n","        \"bool\":{\n","            \"must\":{\"term\":{\"title\":\"trek\"}},\n","            \"filter\":{\"range\":{\"year\":{\"get\":2010}}}\n","        }\n","    }\n","}'\n","\n","```\n"]},{"cell_type":"markdown","metadata":{"id":"O8wgZ9EMfZvF"},"source":["### **some types of filetrs**\n"]},{"cell_type":"markdown","metadata":{"id":"-R3neEAibMmj"},"source":["\n","there are many different kinds of filters. Range is just one of them.\n","\n","\n","\n","\n","\n","*   **term**: filter by exact value \n","\n","    if you need to do filter by some exact value of a term\n","\n","    `{\"term\":{\"year\":2014}}`\n","*   **terms**: match if any exact values in a list match \n","\n","     if you want to match anything in a list of values\n","     \n","    `{\"terms\":{\"genre\":['Sci-Fi\", \"Sdventure\"]}}`\n","*  **range**: Find numbers or dates in a  given range(gt,gte,lt,lte)\n","\n","     if you want to just do a greater than or less than or greater than or     \n","     equal or less than or equal sort of operation \n","\n","    `{\"range\"{\"year\": {\"gte\": 2010}}}`\n","*  **exisits** : Find documents where a fild exists \n","\n","    you can just test if a given field exists at all within a given document.\n","\n","    `{\"exists\" : {\"fielsd\": \"tags\"}}\n","*   **missing**: Find documents where a field is missing \n","\n","    If you want to find documents where they're given field does not exist \n","\n","    `{\"missing\": {\"firld\": \"tags\"}}\n","*  **bool**: Combine filters with Boolean logic(must,must_not,should)\n","\n"," we looked at a must example that's basically the equivalent of. And there is also a must not which is the equivalent of the Boolean operation not in this also a should which is the boolean equivalent of or. So you can do complex boolean filters where you can have conditions that must be combined you know with and relationships or relationships or not relationships and whatever you can imagine using a bool filter.\n","\n","\n"]},{"cell_type":"markdown","metadata":{"id":"rCDPpZHafQ9i"},"source":["### **some types of queries**\n","\n"]},{"cell_type":"markdown","metadata":{"id":"5pENhKadfgGF"},"source":["* match_all: returns all documents and is the default. Normally uesd with a filter.\n","\n","    `{\"match_all\":{}}`\n","\n","* match: search analyzed result, such as full text search.\n","\n","    `{\"mathch\": {\"title\": \"star\"}}`\n","\n","* multi_match: run the same query on multiple fields.\n","\n","    `{multi_match\": {\"query\": \"star\", \"fields\": [\"title\", \"synopsis\"]}}`\n","\n","* bool: Works like a bool filter, but results are scored by relevance."]},{"cell_type":"markdown","metadata":{"id":"wpNVtp4pum_L"},"source":["## 3- Full-Text vs. Phrase Search"]},{"cell_type":"markdown","metadata":{"id":"wthKw3j5x8c-"},"source":["Sometimes you don't want to search for individual search terms like star or trek or wars you want to search for phrases that are search terms and together in a certain order like Star Trek or Star Wars.\n","\n","find all terms, in the right order.\n","```\n","curl -XGET 127.0.0.1:9200/movies/movie/_search?pretty -d'\n","{\n","    \"query\":{\n","        \"match_phrase\":{\n","            \"title\": \"star wars\"\n","        }\n","    }\n","}\n","```\n","\n","order matters, but you're OK with some words begin in between the terms:\n","```\n","curl -XGET 127.0.0.1:9200/movies/movie/_search?pretty -d'\n","{\n","    \"query\":{\n","        \"match_phrase\":{\n","            \"title\": {\"query\":\"star beyond\", \"slop\":1}\n","        }\n","    }\n","}\n","```\n","\n","some match result qoode be : \n","stars trek beyond ,  star wars beyond , beyond star  \n","\n","\n","the slop represent how far you're willing to let a term move to satistfy a phrase(in either direction)\n","\n","another example:\"quick brown fox\" would match \"quick fox\" with slop of 1."]},{"cell_type":"markdown","metadata":{"id":"x_yOxM3Iu9O-"},"source":["**Proximity queries**\n","\n","just use really high slop if you want get any documents that contain the words in your phrase, but want documents that have the words closer together scored higher.\n","\n","```\n","curl -XGET 127.0.0.1:9200/movies/movie/_search?pretty -d'\n","{\n","    \"query\":{\n","        \"match_phrase\":{\n","            \"title\": {\"query\":\"star beyond\", \"slop\":100}\n","        }\n","    }\n","}\n","\n","\n","```\n"]},{"cell_type":"markdown","metadata":{"id":"YBsW9byrDknk"},"source":["## 5-Pagination\n"]},{"cell_type":"markdown","metadata":{"id":"pAqNSLmZD9RY"},"source":["want to return results to the end user one page at a time.\n","\n","\n","**Pagination syntax**\n","``` \n","URL search: \n","curl -XGET '127.0.0.1:9200/movies/_search?size=2&from=2&pretty'\n","\n","\n","json body search \n","\n","curl -XGET 127.0.0.1:9200/movies/_search?pretty -d'\n","{\n","    \"from\": 2,\n","    \"size\": 2,\n","    \"query\": \n","        {\"match\": {\"genre\": \"Sci-Fi\"}\n","        }\n","}\n","\n","```\n","returned everything in the index for that type for the movie type with a size of two items per page starting from Item number two.\n","the item start from zero so item number two it is the third item in the result list.\n","\n","\n","\n","**beware** \n","\n","This is kind of a problem with search engines in general\n","\n","* deep pagination can kill performance.\n","* every result must be retrieved, collected, and stored.\n","* enforce an upper bound on how may result you'll return to users.\n","\n","\n","So if you think about it you can't really tell a elasticsearch,Just give me \n","back results.Number 10373 to 10383. That's not just going to have the \n","overhead of retrieving 10 results because under the hood it still has to \n","retrieve and collect and sort everything in order to figure out what result \n","number 10380 is, and easy to figure out all 10300 results before that as \n","well. In order to give you what those those results are for the range that \n","you requested. So you can really start to kill performance when you start \n","paginate ing deep into your results you should enforce an upper bound on how many results you'll return to your users. Otherwise some nasty person will abuse your system and bring your system to its knees and even Web sites like Google have upper bounds on how many results they return For this reason.\n","<!-- -->\n","``` \n","curl -XGET '127.0.0.1:9200/movies/_search?size=2&pretty'\n","<!-- return first two result the from attribute start from zero  -->\n","\n","json body search \n","\n","curl -XGET 127.0.0.1:9200/movies/_search?pretty -d'\n","{\n","    \"from\": 2,\n","    \"size\": 2,\n","    \"query\": \n","        {\"match\": {\"genre\": \"Sci-Fi\"}\n","        }\n","}\n","\n","```\n"]},{"cell_type":"markdown","metadata":{"id":"JKnKpADcuTPW"},"source":["## Index management \n","to see list of tables go to [this link ](http://localhost:5601/app/management/data/index_management/indices)"]},{"cell_type":"markdown","metadata":{"id":"ipHDjxqEuXxQ"},"source":["# Use Elastic in scrapy \n"]},{"cell_type":"markdown","metadata":{"id":"OQbChzrxubfp"},"source":["there is a repository for use Elastick in Scrapy [link to this repository](https://github.com/jayzeng/scrapy-elasticsearch)"]},{"cell_type":"markdown","metadata":{"id":"6C9E9PsLue8b"},"source":["# Kibana\n"]},{"cell_type":"markdown","metadata":{"id":"qwd_7CMJuf_c"},"source":["## List of query that used in kibana Dev tools "]},{"cell_type":"markdown","metadata":{"id":"SXtGabY1uiTz"},"source":["\n","\n","```\n","GET /_cat/indices?v\n","```\n","\n","\n","\n","**Rturn part of Type doc**\n","```\n","GET digikala/seller/_search?\n","```\n","\n","\n","\n","**Return last record Doc base seller_int_id variable**\n","\n","```\n","\n","GET digikala/seller/_search\n","{\n","  \"query\": {\n","    \"match_all\": {}\n","  },\n","  \"size\": 1,\n","  \"sort\": [\n","    {\n","      \"seller_int_id\": {\n","        \"order\": \"desc\"\n","      }\n","    }\n","  ]\n","}\n","```\n","\n"]},{"cell_type":"markdown","metadata":{"id":"SnhdQTVg1eB0"},"source":["\n","\n","ELASTICSEARCH_BUFFER_LENGTH  = 20\n","\n","```\n","2021-06-02 17:36:21 [scrapy.statscollectors] INFO: Dumping Scrapy stats:\n","{'downloader/request_bytes': 5700,\n"," 'downloader/request_count': 20,\n"," 'downloader/request_method_count/GET': 20,\n"," 'downloader/response_bytes': 1110467,\n"," 'downloader/response_count': 20,\n"," 'downloader/response_status_count/200': 20,\n"," 'elapsed_time_seconds': 22.774146,\n"," 'finish_reason': 'finished',\n"," 'finish_time': datetime.datetime(2021, 6, 2, 13, 6, 21, 817350),\n"," 'item_scraped_count': 20,\n"," 'log_count/DEBUG': 84,\n"," 'log_count/INFO': 12,\n"," 'log_count/WARNING': 1,\n"," 'response_received_count': 20,\n"," 'scheduler/dequeued': 20,\n"," 'scheduler/dequeued/memory': 20,\n"," 'scheduler/enqueued': 20,\n"," 'scheduler/enqueued/memory': 20,\n"," 'start_time': datetime.datetime(2021, 6, 2, 13, 5, 59, 43204)}\n","2021-06-02 17:36:21 [scrapy.core.engine] INFO: Spider closed (finished)\n","\n","```\n","ELASTICSEARCH_BUFFER_LENGTH  = 10\n","\n","```\n","\n","2021-06-02 17:44:28 [scrapy.statscollectors] INFO: Dumping Scrapy stats:\n","{'downloader/request_bytes': 28500,\n"," 'downloader/request_count': 100,\n"," 'downloader/request_method_count/GET': 100,\n"," 'downloader/response_bytes': 5533242,\n"," 'downloader/response_count': 100,\n"," 'downloader/response_status_count/200': 100,\n"," 'elapsed_time_seconds': 124.430781,\n"," 'finish_reason': 'finished',\n"," 'finish_time': datetime.datetime(2021, 6, 2, 13, 14, 28, 1013),\n"," 'item_scraped_count': 100,\n"," 'log_count/DEBUG': 407,\n"," 'log_count/INFO': 19,\n"," 'log_count/WARNING': 1,\n"," 'response_received_count': 100,\n"," 'scheduler/dequeued': 100,\n"," 'scheduler/dequeued/memory': 100,\n"," 'scheduler/enqueued': 100,\n"," 'scheduler/enqueued/memory': 100,\n"," 'start_time': datetime.datetime(2021, 6, 2, 13, 12, 23, 570232)}\n","2021-06-02 17:44:28 [scrapy.core.engine] INFO: Spider closed (finished)\n","\n","```\n","\n","ELASTICSEARCH_BUFFER_LENGTH  = 50\n","\n","```\n","2021-06-02 18:50:17 [scrapy.statscollectors] INFO: Dumping Scrapy stats:\n","{'downloader/request_bytes': 85500,\n"," 'downloader/request_count': 300,\n"," 'downloader/request_method_count/GET': 300,\n"," 'downloader/response_bytes': 16403178,\n"," 'downloader/response_count': 300,\n"," 'downloader/response_status_count/200': 299,\n"," 'downloader/response_status_count/404': 1,\n"," 'elapsed_time_seconds': 365.304469,\n"," 'finish_reason': 'finished',\n"," 'finish_time': datetime.datetime(2021, 6, 2, 14, 20, 17, 760053),\n"," 'item_scraped_count': 300,\n"," 'log_count/DEBUG': 1219,\n"," 'log_count/INFO': 35,\n"," 'log_count/WARNING': 1,\n"," 'response_received_count': 300,\n"," 'scheduler/dequeued': 300,\n"," 'scheduler/dequeued/memory': 300,\n"," 'scheduler/enqueued': 300,\n"," 'scheduler/enqueued/memory': 300,\n"," 'start_time': datetime.datetime(2021, 6, 2, 14, 14, 12, 455584)}\n","2021-06-02 18:50:17 [scrapy.core.engine] INFO: Spider closed (finished)\n","\n","```\n","\n","ELASTICSEARCH_BUFFER_LENGTH  = 100\n","\n","```\n","2021-06-02 20:20:35 [scrapy.statscollectors] INFO: Dumping Scrapy stats:\n","{'downloader/request_bytes': 285000,\n"," 'downloader/request_count': 1000,\n"," 'downloader/request_method_count/GET': 1000,\n"," 'downloader/response_bytes': 53706790,\n"," 'downloader/response_count': 1000,\n"," 'downloader/response_status_count/200': 998,\n"," 'downloader/response_status_count/404': 2,\n"," 'elapsed_time_seconds': 1213.163875,\n"," 'finish_reason': 'finished',\n"," 'finish_time': datetime.datetime(2021, 6, 2, 15, 50, 35, 383917),\n"," 'item_scraped_count': 1000,\n"," 'log_count/DEBUG': 4031,\n"," 'log_count/INFO': 81,\n"," 'log_count/WARNING': 1,\n"," 'response_received_count': 1000,\n"," 'scheduler/dequeued': 1000,\n"," 'scheduler/dequeued/memory': 1000,\n"," 'scheduler/enqueued': 1000,\n"," 'scheduler/enqueued/memory': 1000,\n"," 'start_time': datetime.datetime(2021, 6, 2, 15, 30, 22, 220042)}\n","2021-06-02 20:20:35 [scrapy.core.engine] INFO: Spider closed (finished)\n","\n","```"]}]}