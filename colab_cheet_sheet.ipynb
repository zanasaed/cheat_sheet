{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"colab_cheet_sheet.ipynb","provenance":[],"collapsed_sections":["GJBs_flRovLc","gi_dKSm9Efzp","K6Gmek31VKUd","kabVb2QFVOZF","OdaxUTq6bAnk"],"toc_visible":true},"kernelspec":{"name":"python3","display_name":"Python 3"},"accelerator":"GPU"},"cells":[{"cell_type":"markdown","metadata":{"id":"view-in-github"},"source":["<a href=\"https://colab.research.google.com/github/zanasaed/NoteBook/blob/main/colab_cheet_sheet.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"]},{"cell_type":"markdown","metadata":{"id":"LOyJIM6xicYa"},"source":["# Colab"]},{"cell_type":"markdown","metadata":{"id":"5fCEDCU_qrC0"},"source":["<img height=\"45px\" src=\"https://colab.research.google.com/img/colab_favicon.ico\" align=\"left\" hspace=\"10px\" vspace=\"0px\">\n","\n","<h1>Welcome to Colaboratory!</h1>\n","\n","Colaboratory is a free Jupyter notebook environment that requires no setup and runs entirely in the cloud.\n","\n","With Colaboratory you can write and execute code, save and share your analyses, and access powerful computing resources, all for free from your browser."]},{"cell_type":"code","metadata":{"cellView":"form","id":"xitplqMNk_Hc","colab":{"height":420},"outputId":"ed4f60d2-878d-4056-c438-352dac39a112"},"source":["#@title Introducing Colaboratory\n","#@markdown This 3-minute video gives an overview of the key features of Colaboratory:\n","from IPython.display import YouTubeVideo\n","YouTubeVideo('inN8seMm7UI', width=600, height=400)"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/html":["\n","        <iframe\n","            width=\"600\"\n","            height=400\"\n","            src=\"https://www.youtube.com/embed/inN8seMm7UI\"\n","            frameborder=\"0\"\n","            allowfullscreen\n","        ></iframe>\n","        "],"text/plain":["<IPython.lib.display.YouTubeVideo at 0x7f956e9dda50>"]},"metadata":{"tags":[]},"execution_count":0}]},{"cell_type":"markdown","metadata":{"id":"GJBs_flRovLc"},"source":["## Getting Started\n","\n","The document you are reading is a  [Jupyter notebook](https://jupyter.org/), hosted in Colaboratory. It is not a static page, but an interactive environment that lets you write and execute code in Python and other languages.\n","\n","For example, here is a **code cell** with a short Python script that computes a value, stores it in a variable, and prints the result:"]},{"cell_type":"code","metadata":{"id":"X1ghRLVeE29e"},"source":[""],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"gJr_9dXGpJ05","colab":{"height":35},"outputId":"5626194c-e802-4293-942d-2908885c3c1f"},"source":["seconds_in_a_day = 24 * 60 * 60\n","seconds_in_a_day"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["86400"]},"metadata":{"tags":[]},"execution_count":0}]},{"cell_type":"markdown","metadata":{"id":"2fhs6GZ4qFMx"},"source":["To execute the code in the above cell, select it with a click and then either press the ▷ button to the left of the code, or use the keyboard shortcut \"⌘/Ctrl+Enter\".\n","\n","All cells modify the same global state, so variables that you define by executing a cell can be used in other cells:"]},{"cell_type":"code","metadata":{"id":"-gE-Ez1qtyIA","colab":{"height":35},"outputId":"8d2e4259-4682-4e19-b683-7b9087f28820"},"source":["seconds_in_a_week = 7 * seconds_in_a_day\n","seconds_in_a_week"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["604800"]},"metadata":{"tags":[]},"execution_count":0}]},{"cell_type":"markdown","metadata":{"id":"lSrWNr3MuFUS"},"source":["For more information about working with Colaboratory notebooks, see [Overview of Colaboratory](/notebooks/basic_features_overview.ipynb).\n"]},{"cell_type":"markdown","metadata":{"id":"-Rh3-Vt9Nev9"},"source":["## More Resources\n","\n","Learn how to make the most of Python, Jupyter, Colaboratory, and related tools with these resources:\n","\n","### Working with Notebooks in Colaboratory\n","- [Overview of Colaboratory](/notebooks/basic_features_overview.ipynb)\n","- [Guide to Markdown](/notebooks/markdown_guide.ipynb)\n","- [Importing libraries and installing dependencies](/notebooks/snippets/importing_libraries.ipynb)\n","- [Saving and loading notebooks in GitHub](https://colab.research.google.com/github/googlecolab/colabtools/blob/master/notebooks/colab-github-demo.ipynb)\n","- [Interactive forms](/notebooks/forms.ipynb)\n","- [Interactive widgets](/notebooks/widgets.ipynb)\n","\n","### Working with Data\n","- [Loading data: Drive, Sheets, and Google Cloud Storage](/notebooks/io.ipynb) \n","- [Charts: visualizing data](/notebooks/charts.ipynb)\n","- [Getting started with BigQuery](/notebooks/bigquery.ipynb)\n","\n","### Machine Learning Crash Course\n","These are a few of the notebooks from Google's online Machine Learning course. See the [full course website](https://developers.google.com/machine-learning/crash-course/) for more.\n","- [Intro to Pandas](/notebooks/mlcc/intro_to_pandas.ipynb)\n","- [Tensorflow concepts](/notebooks/mlcc/tensorflow_programming_concepts.ipynb)\n","- [First steps with TensorFlow](/notebooks/mlcc/first_steps_with_tensor_flow.ipynb)\n","- [Intro to neural nets](/notebooks/mlcc/intro_to_neural_nets.ipynb)\n","- [Intro to sparse data and embeddings](/notebooks/mlcc/intro_to_sparse_data_and_embeddings.ipynb)\n","\n","### Using Accelerated Hardware\n","- [TensorFlow with GPUs](/notebooks/gpu.ipynb)\n","- [TensorFlow with TPUs](/notebooks/tpu.ipynb)"]},{"cell_type":"markdown","metadata":{"id":"P-H6Lw1vyNNd"},"source":["## Machine Learning Examples: Seedbank\n","\n","To see end-to-end examples of the interactive machine learning analyses that Colaboratory makes possible, check out the [Seedbank](https://research.google.com/seedbank/) project.\n","\n","A few featured examples:\n","\n","- [Neural Style Transfer](https://research.google.com/seedbank/seed/neural_style_transfer_with_tfkeras): Use deep learning to transfer style between images.\n","- [EZ NSynth](https://research.google.com/seedbank/seed/ez_nsynth): Synthesize audio with WaveNet auto-encoders.\n","- [Fashion MNIST with Keras and TPUs](https://research.google.com/seedbank/seed/fashion_mnist_with_keras_and_tpus): Classify fashion-related images with deep learning.\n","- [DeepDream](https://research.google.com/seedbank/seed/deepdream): Produce DeepDream images from your own photos.\n","- [Convolutional VAE](https://research.google.com/seedbank/seed/convolutional_vae): Create a generative model of handwritten digits."]},{"cell_type":"markdown","metadata":{"id":"gi_dKSm9Efzp"},"source":["# set up VSCode on Google Colab\n","for see more detail go to this [link](https://github.com/amitness/amitness.github.io/blob/63cdc0abc5e7394860da3c53f9d72a4cd3545874/_posts/2020-09-01-vscode-on-colab.md)"]},{"cell_type":"code","metadata":{"id":"JE8EUwwELUqX"},"source":["!pip install colabcode\n","from colabcode import ColabCode\n","ColabCode()\n"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"b36T_-aoVb_j"},"source":["# Natural Language Processing in TensorFlow\n"]},{"cell_type":"markdown","metadata":{"id":"K6Gmek31VKUd"},"source":["## **Week1**"]},{"cell_type":"markdown","metadata":{"id":"kabVb2QFVOZF"},"source":["### Using APIs\n"]},{"cell_type":"markdown","metadata":{"id":"6xA4J5jeUv_t"},"source":["\n","The fit on texts method of the tokenizer then takes in the data and encodes it.\n","\n","The tokenizer provides a word index property which returns a dictionary containing key value pairs, where the key is the word, and the value is the token for that word, which you can inspect by simply printing it out.\n"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"_mgO4Vk9Jp68","outputId":"651271d6-1ee5-4743-c815-9e62bf48043d"},"source":["import tensorflow as tf \n","from tensorflow import keras \n","from tensorflow.keras.preprocessing.text import Tokenizer\n","\n","sentences = [\n","             'I love my dog ',\n","             'I love my cat',\n","             'You love my dog!'\n","\n","]\n","\n","tokenizer = Tokenizer(num_words = 100 )\n","tokenizer.fit_on_texts(sentences)\n","word_index = tokenizer.word_index\n","print(word_index)"],"execution_count":null,"outputs":[{"output_type":"stream","text":["{'love': 1, 'my': 2, 'i': 3, 'dog': 4, 'cat': 5, 'you': 6}\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"vUprKRz_W_h4"},"source":["### Text to sequence\n","\n"]},{"cell_type":"markdown","metadata":{"id":"1JK_S2P6dKR5"},"source":["\n","If you train a neural network on a corpus of texts, and the text has a word index generated from it, then when you want to do inference with the train model, you'll have to encode the text that you want to infer on with the same word index, otherwise it would be meaningless.\n","\n","\n","One really handy thing about this that you'll use later is the fact that the text to sequences called can take any set of sentences, so it can encode them based on the word set that it learned from the one that was passed into fit on texts.\n","\n","So I really love my dog would still be encoded as 4, 2, 1, 3, which is 'I love my dog' with 'really' being lost as the word is not in the Word Index, and 'my dog loves my manatee' would get encoded to 1, 3, 1, which is just 'my dog my'."]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Z-hA3flvXCgj","outputId":"4b891587-12d8-4dfb-add8-d62934756643"},"source":["import tensorflow as tf \n","from tensorflow import keras \n","from tensorflow.keras.preprocessing.text import Tokenizer\n","\n","sentences = [\n","             'I love my dog ',\n","             'I love my cat',\n","             'You love my dog!',\n","             'Do you think my dog is amazing?'\n","\n","]\n","\n","tokenizer = Tokenizer(num_words = 100 )\n","tokenizer.fit_on_texts(sentences)\n","word_index = tokenizer.word_index\n","sequence = tokenizer.texts_to_sequences(sentences)\n","\n","print(word_index)\n","print(seqquence)"],"execution_count":null,"outputs":[{"output_type":"stream","text":["{'my': 1, 'love': 2, 'dog': 3, 'i': 4, 'you': 5, 'cat': 6, 'do': 7, 'think': 8, 'is': 9, 'amazing': 10}\n","[[4, 2, 1, 3], [4, 2, 1, 6], [5, 2, 1, 3], [7, 5, 8, 1, 3, 9, 10]]\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"kO2WtPmRdJjS"},"source":[""]},{"cell_type":"markdown","metadata":{"id":"UkcryrZtaKrb"},"source":["### Looking more at the Tokenizer\n"]},{"cell_type":"markdown","metadata":{"id":"tch-iDRIdPUJ"},"source":["So what do we learn from this? First of all, we really need a lot of training data to get a broad vocabulary or we could end up with sentences like, my dog my, like we just did.\n","\n","Secondly, in many cases, it's a good idea to instead of just ignoring unseen words, to put a special value in when an unseen word is encountered.\n"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"5ypNDVScaJ4S","outputId":"638a3749-25f7-41cc-d8bc-e8d997a5de74"},"source":["import tensorflow as tf \n","from tensorflow import keras \n","from tensorflow.keras.preprocessing.text import Tokenizer\n","\n","sentences = [\n","             'I love my dog ',\n","             'I love my cat',\n","             'You love my dog!',\n","             'Do you think my dog is amazing?'\n","\n","]\n","\n","tokenizer = Tokenizer(num_words = 100 , oov_token='<OOV>')\n","tokenizer.fit_on_texts(sentences)\n","word_index = tokenizer.word_index\n","sequence = tokenizer.texts_to_sequences(sentences)\n","\n","print(word_index)\n","print(seqquence)"],"execution_count":null,"outputs":[{"output_type":"stream","text":["{'<OOV>': 1, 'my': 2, 'love': 3, 'dog': 4, 'i': 5, 'you': 6, 'cat': 7, 'do': 8, 'think': 9, 'is': 10, 'amazing': 11}\n","[[4, 2, 1, 3], [4, 2, 1, 6], [5, 2, 1, 3], [7, 5, 8, 1, 3, 9, 10]]\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"OdaxUTq6bAnk"},"source":["### Padding\n","\n","\n","\n","\n","\n"]},{"cell_type":"markdown","metadata":{"id":"0o-SoBQ5dgo5"},"source":["So I've made a few changes to the code to handle padding.\n","\n","First, in order to use the padding functions you'll have to import pad sequences from tensorflow.carastoppreprocessing.sequence.\n","\n","\n","Then once the tokenizer has created the sequences, these sequences can be passed to pad sequences in order to have them padded like this.\n","\n","\n","You can now see that the list of sentences has been padded out into a matrix and that each row in the matrix has the same length.\n","\n","\n","Often you'll see examples where the padding is after the sentence and not before as you just saw.\n","\n","\n","If you, like me, are more comfortable with that, you can change the code to this, adding the parameter padding equals post.\n","\n","\n","You may have noticed that the matrix width was the same as the longest sentence.\n","\n","\n","If I have sentences longer than the maxlength, then I'll lose information but from where.Like with the padding the default is pre, which means that you will lose from the beginning of the sentence.\n","If you want to override this so that you lose from the end instead, you can do so with the truncating parameter like this.\n","\n"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"zsnAM5KtbejH","outputId":"17471fac-4edd-42f0-f76a-0e68bb883585"},"source":["import tensorflow as tf \n","from tensorflow import keras \n","from tensorflow.keras.preprocessing.text import Tokenizer\n","from tensorflow.keras.preprocessing.sequence import pad_sequences\n","sentences = [\n","             'I love my dog ',\n","             'I love my cat',\n","             'You love my dog!',\n","             'Do you think my dog is amazing?'\n","\n","]\n","\n","tokenizer = Tokenizer(num_words = 100 , oov_token='<OOV>')\n","tokenizer.fit_on_texts(sentences)\n","word_index = tokenizer.word_index\n","\n","sequence = tokenizer.texts_to_sequences(sentences)\n","\n","padded = pad_sequences(sequence , padding='post' , maxlen=5 , truncating='post' )\n","\n","print(\"***\\n\",word_index)\n","print(\"***\\n\",seqquence)\n","print(\"***\\n\",padded)"],"execution_count":null,"outputs":[{"output_type":"stream","text":["***\n"," {'<OOV>': 1, 'my': 2, 'love': 3, 'dog': 4, 'i': 5, 'you': 6, 'cat': 7, 'do': 8, 'think': 9, 'is': 10, 'amazing': 11}\n","***\n"," [[4, 2, 1, 3], [4, 2, 1, 6], [5, 2, 1, 3], [7, 5, 8, 1, 3, 9, 10]]\n","***\n"," [[5 3 2 4 0]\n"," [5 3 2 7 0]\n"," [6 3 2 4 0]\n"," [8 6 9 2 4]]\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"MrV3G5Fddi4o"},"source":["### Sarcasm, really?\n","\n","[link](https://www.coursera.org/learn/natural-language-processing-tensorflow/lecture/HAUby/sarcasm-really)\n","\n","### Working with the Tokenizer\n","[link ](https://www.coursera.org/learn/natural-language-processing-tensorflow/lecture/VEUJX/working-with-the-tokenizer)\n","\n","### Notebook for lesson 3\n","[link](https://www.coursera.org/learn/natural-language-processing-tensorflow/lecture/uskDE/notebook-for-lesson-3)\n"]},{"cell_type":"markdown","metadata":{"id":"k0taHhf5fBTQ"},"source":[""]},{"cell_type":"code","metadata":{"id":"iMBuTga4fB-o"},"source":[""],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"lQr6bX0CVjwl"},"source":["## Week2"]},{"cell_type":"markdown","metadata":{"id":"77P5df0ZQjww"},"source":["### Introduction\n","[link ](https://coursera.org/learn/natural-language-processing-tensorflow/lecture/1OiEp/introduction)\n","\n","### The IMBD dataset\n","[link](https://www.coursera.org/learn/natural-language-processing-tensorflow/lecture/PxiP4/the-imbd-dataset)\n","\n","### Looking into the details\n","[link](https://www.coursera.org/learn/natural-language-processing-tensorflow/lecture/0N8WC/looking-into-the-details)\n","\n","### How can we use vectors?\n","[link ](https://www.coursera.org/learn/natural-language-processing-tensorflow/lecture/HKvYY/how-can-we-use-vectors)\n"]},{"cell_type":"code","metadata":{"id":"RC0mCLGlRTg3"},"source":["import tensorflow_datasets as tfds\n","\n","imdb, info = tfds.load(\"imdb_reviews\", with_info= True, as_supervised=True)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"2J1k6ResVl7r"},"source":["import numpy as np\n"," \n","train_data, test_data = imdb['train'], imdb['test']\n","\n","training_sentences = []\n","training_labels = []\n","\n","testing_sentences = [] \n","testing_labels = [] \n","\n","for s,l in train_data :\n","    training_sentences.append(str(s.numpy()))\n","    training_labels.append(l.numpy())\n","\n","for s, l in test_data:\n","    testing_sentences.append(str(s.numpy()))\n","    testing_labels.append(l.numpy())\n","\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"9ElcLVX9QqMO"},"source":[""],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"k5MZ2GRkg1l9"},"source":["# Udemy - Complete Guide to TensorFlow for Deep Learning with Python 2018-11\n"]},{"cell_type":"markdown","metadata":{"id":"6vsQ8uTBg6cz"},"source":["[linkt to descrption of course ](https://colab.research.google.com/drive/1Qf9oRvqqlJRxMyzaQCSjKdEghf8dOAJ6)"]}]}