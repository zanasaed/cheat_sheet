{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"Udemy - NLP - Natural Language Processing with Python_part2.ipynb","provenance":[],"collapsed_sections":["kTAvmdTxdBOP","JVCDSzsydzNf","hzUD2xoGcVyt","mT_Z3m35cVyu","EVCeeHZKcVzD","950RbabscVzE"],"toc_visible":true,"authorship_tag":"ABX9TyPFnS+7qiGvNB9i5w2VC4tE"},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.8.10"}},"cells":[{"cell_type":"markdown","metadata":{"id":"AStKbe1ec0Zk"},"source":["# Feature-Extraction-from-Text\n","\n"]},{"cell_type":"markdown","metadata":{"id":"Gv4MVJCYcVyY"},"source":["This unit is divided into two sections:\n","* First, we'll find out what what is necessary to build an NLP system that can turn a body of text into a numerical array of *features*.\n","* Next we'll show how to perform these steps using real tools."]},{"cell_type":"markdown","metadata":{"id":"kTAvmdTxdBOP"},"source":["## Building a Natural Language Processor From Scratch\n"]},{"cell_type":"markdown","metadata":{"id":"UtWO8puVcVyZ"},"source":["In this section we'll use basic Python to build a rudimentary NLP system. We'll build a *corpus of documents* (two small text files), create a *vocabulary* from all the words in both documents, and then demonstrate a *Bag of Words* technique to extract features from each document.<br>\n","<div class=\"alert alert-info\" style=\"margin: 20px\">**This first section is for illustration only!**\n","<br>Don't bother memorizing the code - we'd never do this in real life.</div>"]},{"cell_type":"markdown","metadata":{"id":"qRX0ttNHcVyZ"},"source":["### Start with some documents:\n","For simplicity we won't use any punctuation."]},{"cell_type":"code","metadata":{"id":"fzLR8DDecVya","outputId":"d2259f4b-f42a-48a9-9014-ac37bc2e25d4"},"source":["%%writefile 1.txt\n","This is a story about cats\n","our feline pets\n","Cats are furry animals"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Overwriting 1.txt\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"F3w22gK_cVyc","outputId":"2b9f53d2-6fe0-4f5a-ada9-afda7bdba423"},"source":["%%writefile 2.txt\n","This story is about surfing\n","Catching waves is fun\n","Surfing is a popular water sport"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Overwriting 2.txt\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"uh1b0EzGcVye"},"source":["### Build a vocabulary\n"]},{"cell_type":"markdown","metadata":{"id":"wNgoQj2IdX35"},"source":["The goal here is to build a numerical array from all the words that appear in every document. Later we'll create instances (vectors) for each individual document."]},{"cell_type":"code","metadata":{"id":"Ok6GgIC2cVyf","outputId":"a0aca5f5-eade-4a2c-f409-f0c223327f09"},"source":["vocab = {}\n","i = 1\n","\n","with open('1.txt') as f:\n","    x = f.read().lower().split()\n","\n","for word in x:\n","    if word in vocab:\n","        continue\n","    else:\n","        vocab[word]=i\n","        i+=1\n","\n","print(vocab)"],"execution_count":null,"outputs":[{"output_type":"stream","text":["{'this': 1, 'is': 2, 'a': 3, 'story': 4, 'about': 5, 'cats': 6, 'our': 7, 'feline': 8, 'pets': 9, 'are': 10, 'furry': 11, 'animals': 12}\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"eBqNOukFcVyi","outputId":"bc4cbae6-e7f6-47f3-e31e-3c2d6559eca3"},"source":["with open('2.txt') as f:\n","    x = f.read().lower().split()\n","\n","for word in x:\n","    if word in vocab:\n","        continue\n","    else:\n","        vocab[word]=i\n","        i+=1\n","\n","print(vocab)"],"execution_count":null,"outputs":[{"output_type":"stream","text":["{'this': 1, 'is': 2, 'a': 3, 'story': 4, 'about': 5, 'cats': 6, 'our': 7, 'feline': 8, 'pets': 9, 'are': 10, 'furry': 11, 'animals': 12, 'surfing': 13, 'catching': 14, 'waves': 15, 'fun': 16, 'popular': 17, 'water': 18, 'sport': 19}\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"50-CHlJWcVyk"},"source":["Even though `2.txt` has 15 words, only 7 new words were added to the dictionary.\n","\n","### Feature Extraction\n","Now that we've encapsulated our \"entire language\" in a dictionary, let's perform *feature extraction* on each of our original documents:"]},{"cell_type":"code","metadata":{"id":"LtCF8APXcVyl","outputId":"129115ee-82df-46ae-b2cc-8d3bb1cbd96f"},"source":["# Create an empty vector with space for each word in the vocabulary:\n","one = ['1.txt']+[0]*len(vocab)\n","one"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["['1.txt', 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]"]},"metadata":{"tags":[]},"execution_count":5}]},{"cell_type":"code","metadata":{"id":"2FaPHahHcVym","outputId":"e376d817-788b-4587-a7e7-3e81d8842ad9"},"source":["# map the frequencies of each word in 1.txt to our vector:\n","with open('1.txt') as f:\n","    x = f.read().lower().split()\n","    \n","for word in x:\n","    one[vocab[word]]+=1\n","    \n","one"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["['1.txt', 1, 1, 1, 1, 1, 2, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0]"]},"metadata":{"tags":[]},"execution_count":6}]},{"cell_type":"markdown","metadata":{"id":"QF3EMu-xcVym"},"source":["<font color=green>We can see that most of the words in 1.txt appear only once, although \"cats\" appears twice.</font>"]},{"cell_type":"code","metadata":{"id":"8xVQryq3cVyn"},"source":["# Do the same for the second document:\n","two = ['2.txt']+[0]*len(vocab)\n","\n","with open('2.txt') as f:\n","    x = f.read().lower().split()\n","    \n","for word in x:\n","    two[vocab[word]]+=1"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"ZX5uhPEzcVyn","outputId":"de27ccb6-bad3-44f7-e3d4-b5ff04216077"},"source":["# Compare the two vectors:\n","print(f'{one}\\n{two}')"],"execution_count":null,"outputs":[{"output_type":"stream","text":["['1.txt', 1, 1, 1, 1, 1, 2, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0]\n","['2.txt', 1, 3, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 2, 1, 1, 1, 1, 1, 1]\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"N2U8GnjhcVyn"},"source":["By comparing the vectors we see that some words are common to both, some appear only in `1.txt`, others only in `2.txt`. Extending this logic to tens of thousands of documents, we would see the vocabulary dictionary grow to hundreds of thousands of words. Vectors would contain mostly zero values, making them *sparse matrices*."]},{"cell_type":"markdown","metadata":{"id":"fpRTF36XcVyo"},"source":["### Bag of Words and Tf-idf\n"]},{"cell_type":"markdown","metadata":{"id":"uXiaNZkDddbB"},"source":["In the above examples, each vector can be considered a *bag of words*. By itself these may not be helpful until we consider *term frequencies*, or how often individual words appear in documents. A simple way to calculate term frequencies is to divide the number of occurrences of a word by the total number of words in the document. In this way, the number of times a word appears in large documents can be compared to that of smaller documents.\n","\n","However, it may be hard to differentiate documents based on term frequency if a word shows up in a majority of documents. To handle this we also consider *inverse document frequency*, which is the total number of documents divided by the number of documents that contain the word. In practice we convert this value to a logarithmic scale, as described [here](https://en.wikipedia.org/wiki/Tf%E2%80%93idf#Inverse_document_frequency).\n","\n","Together these terms become [**tf-idf**](https://en.wikipedia.org/wiki/Tf%E2%80%93idf)."]},{"cell_type":"markdown","metadata":{"id":"X5B5qJx-cVyo"},"source":["### Stop Words and Word Stems\n"]},{"cell_type":"markdown","metadata":{"id":"Sigq6hAUdgvK"},"source":["Some words like \"the\" and \"and\" appear so frequently, and in so many documents, that we needn't bother counting them. Also, it may make sense to only record the root of a word, say `cat` in place of both `cat` and `cats`. This will shrink our vocab array and improve performance."]},{"cell_type":"markdown","metadata":{"id":"xKKW7npPcVyp"},"source":["### Tokenization and Tagging\n"]},{"cell_type":"markdown","metadata":{"id":"TNf39qwwdi9V"},"source":["When we created our vectors the first thing we did was split the incoming text on whitespace with `.split()`. This was a crude form of *tokenization* - that is, dividing a document into individual words. In this simple example we didn't worry about punctuation or different parts of speech. In the real world we rely on some fairly sophisticated *morphology* to parse text appropriately.\n","\n","Once the text is divided, we can go back and *tag* our tokens with information about parts of speech, grammatical dependencies, etc. This adds more dimensions to our data and enables a deeper understanding of the context of specific documents. For this reason, vectors become ***high dimensional sparse matrices***."]},{"cell_type":"markdown","metadata":{"id":"GhKte4fTcVyp"},"source":["<div class=\"alert alert-info\" style=\"margin: 20px\">**That's the end of the first section.**\n","<br>In the next section we'll use scikit-learn to perform a real-life analysis.</div>"]},{"cell_type":"markdown","metadata":{"id":"3K_XxuLzcVyq"},"source":["___\n","## Feature Extraction from Text\n"]},{"cell_type":"markdown","metadata":{"id":"7EV6Yd-5dx_h"},"source":["In the **Scikit-learn Primer** lecture we applied a simple SVC classification model to the SMSSpamCollection dataset. We tried to predict the ham/spam label based on message length and punctuation counts. In this section we'll actually look at the text of each message and try to perform a classification based on content. We'll take advantage of some of scikit-learn's [feature extraction](https://scikit-learn.org/stable/modules/feature_extraction.html#text-feature-extraction) tools.\n","\n"]},{"cell_type":"markdown","metadata":{"id":"JVCDSzsydzNf"},"source":["### Load a dataset"]},{"cell_type":"code","metadata":{"id":"zaTHQ0ofcVyr","outputId":"29f3608f-021e-4b74-e911-f753362b278f"},"source":["# Perform imports and load the dataset:\n","import numpy as np\n","import pandas as pd\n","\n","df = pd.read_csv('../TextFiles/smsspamcollection.tsv', sep='\\t')\n","df.head()"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/html":["<div>\n","<style>\n","    .dataframe thead tr:only-child th {\n","        text-align: right;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: left;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>label</th>\n","      <th>message</th>\n","      <th>length</th>\n","      <th>punct</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>ham</td>\n","      <td>Go until jurong point, crazy.. Available only ...</td>\n","      <td>111</td>\n","      <td>9</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>ham</td>\n","      <td>Ok lar... Joking wif u oni...</td>\n","      <td>29</td>\n","      <td>6</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>spam</td>\n","      <td>Free entry in 2 a wkly comp to win FA Cup fina...</td>\n","      <td>155</td>\n","      <td>6</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>ham</td>\n","      <td>U dun say so early hor... U c already then say...</td>\n","      <td>49</td>\n","      <td>6</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>ham</td>\n","      <td>Nah I don't think he goes to usf, he lives aro...</td>\n","      <td>61</td>\n","      <td>2</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>"],"text/plain":["  label                                            message  length  punct\n","0   ham  Go until jurong point, crazy.. Available only ...     111      9\n","1   ham                      Ok lar... Joking wif u oni...      29      6\n","2  spam  Free entry in 2 a wkly comp to win FA Cup fina...     155      6\n","3   ham  U dun say so early hor... U c already then say...      49      6\n","4   ham  Nah I don't think he goes to usf, he lives aro...      61      2"]},"metadata":{"tags":[]},"execution_count":9}]},{"cell_type":"markdown","metadata":{"id":"L6qSwjfOcVys"},"source":["### Check for missing values:\n","Always a good practice."]},{"cell_type":"code","metadata":{"id":"P4IV3zH9cVys","outputId":"1f5799ad-1f87-4423-be7e-138908a64429"},"source":["df.isnull().sum()"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["label      0\n","message    0\n","length     0\n","punct      0\n","dtype: int64"]},"metadata":{"tags":[]},"execution_count":10}]},{"cell_type":"markdown","metadata":{"id":"hzUD2xoGcVyt"},"source":["### Take a quick look at the *ham* and *spam* `label` column:"]},{"cell_type":"code","metadata":{"id":"2dxGB8P_cVyt","outputId":"0adc7071-16c7-40fb-98e4-abb7192fe277"},"source":["df['label'].value_counts()"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["ham     4825\n","spam     747\n","Name: label, dtype: int64"]},"metadata":{"tags":[]},"execution_count":11}]},{"cell_type":"markdown","metadata":{"id":"3jY9w4BrcVyu"},"source":["<font color=green>4825 out of 5572 messages, or 86.6%, are ham. This means that any text classification model we create has to perform **better than 86.6%** to beat random chance.</font>"]},{"cell_type":"markdown","metadata":{"id":"mT_Z3m35cVyu"},"source":["### Split the data into train & test sets:"]},{"cell_type":"code","metadata":{"id":"RJGylmDfcVyu"},"source":["from sklearn.model_selection import train_test_split\n","\n","X = df['message']  # this time we want to look at the text\n","y = df['label']\n","\n","X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.33, random_state=42)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"BnlfNiwPcVyv"},"source":["### Scikit-learn's CountVectorizer\n"]},{"cell_type":"markdown","metadata":{"id":"vtLq01ZSd_ar"},"source":["Text preprocessing, tokenizing and the ability to filter out stopwords are all included in [CountVectorizer](https://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.CountVectorizer.html), which builds a dictionary of features and transforms documents to feature vectors."]},{"cell_type":"code","metadata":{"id":"JQhU1oGzcVyv","outputId":"bc6d9042-8a46-438d-c548-7a4a209133f8"},"source":["from sklearn.feature_extraction.text import CountVectorizer\n","count_vect = CountVectorizer()\n","\n","X_train_counts = count_vect.fit_transform(X_train)\n","X_train_counts.shape"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["(3733, 7082)"]},"metadata":{"tags":[]},"execution_count":13}]},{"cell_type":"markdown","metadata":{"id":"_bNhChjVcVyw"},"source":["<font color=green>This shows that our training set is comprised of 3733 documents, and 7082 features.</font>"]},{"cell_type":"markdown","metadata":{"id":"CJOr1Q6WcVyx"},"source":["### Transform Counts to Frequencies with Tf-idf\n"]},{"cell_type":"markdown","metadata":{"id":"t9icSfhweRqj"},"source":["While counting words is helpful, longer documents will have higher average count values than shorter documents, even though they might talk about the same topics.\n","\n","To avoid this we can simply divide the number of occurrences of each word in a document by the total number of words in the document: these new features are called **tf** for Term Frequencies.\n","\n","Another refinement on top of **tf** is to downscale weights for words that occur in many documents in the corpus and are therefore less informative than those that occur only in a smaller portion of the corpus.\n","\n","This downscaling is called **tf–idf** for “Term Frequency times Inverse Document Frequency”.\n","\n","Both tf and tf–idf can be computed as follows using [TfidfTransformer](https://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.TfidfTransformer.html):"]},{"cell_type":"code","metadata":{"id":"-Kc-1YVMcVyy","outputId":"ba3d9531-a141-4354-af0c-340d13e7527a"},"source":["from sklearn.feature_extraction.text import TfidfTransformer\n","tfidf_transformer = TfidfTransformer()\n","\n","X_train_tfidf = tfidf_transformer.fit_transform(X_train_counts)\n","X_train_tfidf.shape"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["(3733, 7082)"]},"metadata":{"tags":[]},"execution_count":14}]},{"cell_type":"markdown","metadata":{"id":"xxHgjPDGcVy0"},"source":["Note: the `fit_transform()` method actually performs two operations: it fits an estimator to the data and then transforms our count-matrix to a tf-idf representation."]},{"cell_type":"markdown","metadata":{"id":"wWjppaEccVy3"},"source":["### Combine Steps with TfidVectorizer\n","In the future, we can combine the CountVectorizer and TfidTransformer steps into one using [TfidVectorizer](https://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.TfidfVectorizer.html):"]},{"cell_type":"code","metadata":{"id":"yzPUKmEscVy3","outputId":"e98db9a2-b462-441a-a452-d1f850b24a81"},"source":["from sklearn.feature_extraction.text import TfidfVectorizer\n","vectorizer = TfidfVectorizer()\n","\n","X_train_tfidf = vectorizer.fit_transform(X_train) # remember to use the original X_train set\n","X_train_tfidf.shape"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["(3733, 7082)"]},"metadata":{"tags":[]},"execution_count":15}]},{"cell_type":"markdown","metadata":{"id":"kuWlstbOcVy4"},"source":["### Train a Classifier\n","Here we'll introduce an SVM classifier that's similar to SVC, called [LinearSVC](https://scikit-learn.org/stable/modules/generated/sklearn.svm.LinearSVC.html). LinearSVC handles sparse input better, and scales well to large numbers of samples."]},{"cell_type":"code","metadata":{"id":"WpgkuKSJcVy5","outputId":"17ce46b8-14b1-4e10-8fc9-c7a8d86b098e"},"source":["from sklearn.svm import LinearSVC\n","clf = LinearSVC()\n","clf.fit(X_train_tfidf,y_train)"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["LinearSVC(C=1.0, class_weight=None, dual=True, fit_intercept=True,\n","     intercept_scaling=1, loss='squared_hinge', max_iter=1000,\n","     multi_class='ovr', penalty='l2', random_state=None, tol=0.0001,\n","     verbose=0)"]},"metadata":{"tags":[]},"execution_count":16}]},{"cell_type":"markdown","metadata":{"id":"JxvH-j94cVzD"},"source":["<font color=green>Earlier we named our SVC classifier **svc_model**. Here we're using the more generic name **clf** (for classifier).</font>"]},{"cell_type":"markdown","metadata":{"id":"EVCeeHZKcVzD"},"source":["### Build a Pipeline\n","Remember that only our training set has been vectorized into a full vocabulary. In order to perform an analysis on our test set we'll have to submit it to the same procedures. Fortunately scikit-learn offers a [**Pipeline**](https://scikit-learn.org/stable/modules/generated/sklearn.pipeline.Pipeline.html) class that behaves like a compound classifier."]},{"cell_type":"code","metadata":{"id":"0bIu6-o9cVzD","outputId":"650e2eb1-f084-4924-bce5-aae586438832"},"source":["from sklearn.pipeline import Pipeline\n","# from sklearn.feature_extraction.text import TfidfVectorizer\n","# from sklearn.svm import LinearSVC\n","\n","text_clf = Pipeline([('tfidf', TfidfVectorizer()),\n","                     ('clf', LinearSVC()),\n","])\n","\n","# Feed the training data through the pipeline\n","text_clf.fit(X_train, y_train)  "],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["Pipeline(memory=None,\n","     steps=[('tfidf', TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n","        dtype=<class 'numpy.float64'>, encoding='utf-8', input='content',\n","        lowercase=True, max_df=1.0, max_features=None, min_df=1,\n","        ngram_range=(1, 1), norm='l2', preprocessor=None, smooth_idf=True,...ax_iter=1000,\n","     multi_class='ovr', penalty='l2', random_state=None, tol=0.0001,\n","     verbose=0))])"]},"metadata":{"tags":[]},"execution_count":17}]},{"cell_type":"markdown","metadata":{"id":"950RbabscVzE"},"source":["### Test the classifier and display results"]},{"cell_type":"code","metadata":{"id":"QOVex-QGcVzE"},"source":["# Form a prediction set\n","predictions = text_clf.predict(X_test)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"3MDjtHoJcVzE","outputId":"8eb6843e-25f3-41fc-ed63-026e4d4757b9"},"source":["# Report the confusion matrix\n","from sklearn import metrics\n","print(metrics.confusion_matrix(y_test,predictions))"],"execution_count":null,"outputs":[{"output_type":"stream","text":["[[1586    7]\n"," [  12  234]]\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"RESyBFMDcVzF","outputId":"1d5950e3-024e-4737-aa05-7439b164e247"},"source":["# Print a classification report\n","print(metrics.classification_report(y_test,predictions))"],"execution_count":null,"outputs":[{"output_type":"stream","text":["              precision    recall  f1-score   support\n","\n","         ham       0.99      1.00      0.99      1593\n","        spam       0.97      0.95      0.96       246\n","\n","   micro avg       0.99      0.99      0.99      1839\n","   macro avg       0.98      0.97      0.98      1839\n","weighted avg       0.99      0.99      0.99      1839\n","\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"PtkFq74GcVzG","outputId":"7814c7d8-76b8-44f3-fe95-70e8f18e0b3f"},"source":["# Print the overall accuracy\n","print(metrics.accuracy_score(y_test,predictions))"],"execution_count":null,"outputs":[{"output_type":"stream","text":["0.989668297988\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"jTx14iOOcVzG"},"source":["Using the text of the messages, our model performed exceedingly well; it correctly predicted spam **98.97%** of the time!<br>\n","Now let's apply what we've learned to a text classification project involving positive and negative movie reviews.\n","\n","### Next up: Text Classification Project"]},{"cell_type":"markdown","metadata":{"id":"iQX1nqA7lIIP"},"source":["# Text Classification Project\n"]},{"cell_type":"markdown","metadata":{"id":"qPRk6L1RlVRI"},"source":["Now we're at the point where we should be able to:\n","* Read in a collection of documents - a *corpus*\n","* Transform text into numerical vector data using a pipeline\n","* Create a classifier\n","* Fit/train the classifier\n","* Test the classifier on new data\n","* Evaluate performance\n","\n","For this project we'll use the Cornell University Movie Review polarity dataset v2.0 obtained from http://www.cs.cornell.edu/people/pabo/movie-review-data/\n","\n","In this exercise we'll try to develop a classification model as we did for the SMSSpamCollection dataset - that is, we'll try to predict the Positive/Negative labels based on text content alone. In an upcoming section we'll apply *Sentiment Analysis* to train models that have a deeper understanding of each review."]},{"cell_type":"markdown","metadata":{"id":"FNp-L-1MlIIV"},"source":["## Perform imports and load the dataset\n","The dataset contains the text of 2000 movie reviews. 1000 are positive, 1000 are negative, and the text has been preprocessed as a tab-delimited file."]},{"cell_type":"code","metadata":{"id":"tyTk35kNlIIY","outputId":"6f48a9ff-aefa-4a8b-b4dc-96dd82e9ecf8"},"source":["import numpy as np\n","import pandas as pd\n","\n","df = pd.read_csv('../TextFiles/moviereviews.tsv', sep='\\t')\n","df.head()"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/html":["<div>\n","<style>\n","    .dataframe thead tr:only-child th {\n","        text-align: right;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: left;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>label</th>\n","      <th>review</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>neg</td>\n","      <td>how do films like mouse hunt get into theatres...</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>neg</td>\n","      <td>some talented actresses are blessed with a dem...</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>pos</td>\n","      <td>this has been an extraordinary year for austra...</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>pos</td>\n","      <td>according to hollywood movies made in last few...</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>neg</td>\n","      <td>my first press screening of 1998 and already i...</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>"],"text/plain":["  label                                             review\n","0   neg  how do films like mouse hunt get into theatres...\n","1   neg  some talented actresses are blessed with a dem...\n","2   pos  this has been an extraordinary year for austra...\n","3   pos  according to hollywood movies made in last few...\n","4   neg  my first press screening of 1998 and already i..."]},"metadata":{"tags":[]},"execution_count":1}]},{"cell_type":"code","metadata":{"id":"_miBx3BdlIIh","outputId":"aefc66eb-0bc5-4bdb-a673-dd2b2a8add45"},"source":["len(df)"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["2000"]},"metadata":{"tags":[]},"execution_count":2}]},{"cell_type":"markdown","metadata":{"id":"4JqphWVdlIIj"},"source":["### Take a look at a typical review. This one is labeled \"negative\":"]},{"cell_type":"code","metadata":{"id":"fzmI2moplIIk","outputId":"f6f38a5d-af44-4e15-9ea1-3630a8aa9ba4"},"source":["from IPython.display import Markdown, display\n","display(Markdown('> '+df['review'][0]))"],"execution_count":null,"outputs":[{"output_type":"display_data","data":{"text/markdown":"> how do films like mouse hunt get into theatres ? \r\nisn't there a law or something ? \r\nthis diabolical load of claptrap from steven speilberg's dreamworks studio is hollywood family fare at its deadly worst . \r\nmouse hunt takes the bare threads of a plot and tries to prop it up with overacting and flat-out stupid slapstick that makes comedies like jingle all the way look decent by comparison . \r\nwriter adam rifkin and director gore verbinski are the names chiefly responsible for this swill . \r\nthe plot , for what its worth , concerns two brothers ( nathan lane and an appalling lee evens ) who inherit a poorly run string factory and a seemingly worthless house from their eccentric father . \r\ndeciding to check out the long-abandoned house , they soon learn that it's worth a fortune and set about selling it in auction to the highest bidder . \r\nbut battling them at every turn is a very smart mouse , happy with his run-down little abode and wanting it to stay that way . \r\nthe story alternates between unfunny scenes of the brothers bickering over what to do with their inheritance and endless action sequences as the two take on their increasingly determined furry foe . \r\nwhatever promise the film starts with soon deteriorates into boring dialogue , terrible overacting , and increasingly uninspired slapstick that becomes all sound and fury , signifying nothing . \r\nthe script becomes so unspeakably bad that the best line poor lee evens can utter after another run in with the rodent is : \" i hate that mouse \" . \r\noh cringe ! \r\nthis is home alone all over again , and ten times worse . \r\none touching scene early on is worth mentioning . \r\nwe follow the mouse through a maze of walls and pipes until he arrives at his makeshift abode somewhere in a wall . \r\nhe jumps into a tiny bed , pulls up a makeshift sheet and snuggles up to sleep , seemingly happy and just wanting to be left alone . \r\nit's a magical little moment in an otherwise soulless film . \r\na message to speilberg : if you want dreamworks to be associated with some kind of artistic credibility , then either give all concerned in mouse hunt a swift kick up the arse or hire yourself some decent writers and directors . \r\nthis kind of rubbish will just not do at all . \r\n","text/plain":["<IPython.core.display.Markdown object>"]},"metadata":{"tags":[]}}]},{"cell_type":"markdown","metadata":{"id":"8l43TW7llIIl"},"source":["## Check for missing values:\n"]},{"cell_type":"markdown","metadata":{"id":"qJ6gkbHnlgaH"},"source":["We have intentionally included records with missing data. Some have NaN values, others have short strings composed of only spaces. This might happen if a reviewer declined to provide a comment with their review. We will show two ways using pandas to identify and remove records containing empty data.\n","* NaN records are efficiently handled with [.isnull()](https://pandas.pydata.org/pandas-docs/stable/generated/pandas.DataFrame.isnull.html) and [.dropna()](https://pandas.pydata.org/pandas-docs/stable/generated/pandas.DataFrame.dropna.html)\n","* Strings that contain only whitespace can be handled with [.isspace()](https://docs.python.org/3/library/stdtypes.html#str.isspace), [.itertuples()](https://pandas.pydata.org/pandas-docs/stable/generated/pandas.DataFrame.itertuples.html), and [.drop()](https://pandas.pydata.org/pandas-docs/stable/generated/pandas.DataFrame.drop.html)\n"]},{"cell_type":"markdown","metadata":{"id":"dF1sO_FBlywH"},"source":["\n","### Detect & remove NaN values:"]},{"cell_type":"code","metadata":{"id":"FoJA36KElIIq","outputId":"c28ae5a6-a330-4edd-b349-64d317894dae"},"source":["# Check for the existence of NaN values in a cell:\n","df.isnull().sum()"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["label      0\n","review    35\n","dtype: int64"]},"metadata":{"tags":[]},"execution_count":4}]},{"cell_type":"markdown","metadata":{"id":"ne7on5rslIIw"},"source":["35 records show **NaN** (this stands for \"not a number\" and is equivalent to *None*). These are easily removed using the `.dropna()` pandas function.\n","<div class=\"alert alert-info\" style=\"margin: 20px\">CAUTION: By setting inplace=True, we permanently affect the DataFrame currently in memory, and this can't be undone. However, it does *not* affect the original source data. If we needed to, we could always load the original DataFrame from scratch.</div>"]},{"cell_type":"code","metadata":{"id":"AB4YHOuGlIIy","outputId":"42fac52c-e8e1-4ad3-a9bc-b54b67395758"},"source":["df.dropna(inplace=True)\n","\n","len(df)"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["1965"]},"metadata":{"tags":[]},"execution_count":5}]},{"cell_type":"markdown","metadata":{"id":"lwTTmCKzlII2"},"source":["### Detect & remove empty strings\n","Technically, we're dealing with \"whitespace only\" strings. If the original .tsv file had contained empty strings, pandas **.read_csv()** would have assigned NaN values to those cells by default.\n","\n","In order to detect these strings we need to iterate over each row in the DataFrame. The **.itertuples()** pandas method is a good tool for this as it provides access to every field. For brevity we'll assign the names `i`, `lb` and `rv` to the `index`, `label` and `review` columns."]},{"cell_type":"code","metadata":{"id":"Kf3DQN8vlII4","outputId":"4a5ba3b0-654d-4353-9642-371130f9d93e"},"source":["blanks = []  # start with an empty list\n","\n","for i,lb,rv in df.itertuples():  # iterate over the DataFrame\n","    if type(rv)==str:            # avoid NaN values\n","        if rv.isspace():         # test 'review' for whitespace\n","            blanks.append(i)     # add matching index numbers to the list\n","        \n","print(len(blanks), 'blanks: ', blanks)"],"execution_count":null,"outputs":[{"output_type":"stream","text":["27 blanks:  [57, 71, 147, 151, 283, 307, 313, 323, 343, 351, 427, 501, 633, 675, 815, 851, 977, 1079, 1299, 1455, 1493, 1525, 1531, 1763, 1851, 1905, 1993]\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"al8wndGBlII5"},"source":["Next we'll pass our list of index numbers to the **.drop()** method, and set `inplace=True` to make the change permanent."]},{"cell_type":"code","metadata":{"id":"GcDA2bAqlII6","outputId":"1ca19740-7ee9-46e5-f10a-8652657fe3a0"},"source":["df.drop(blanks, inplace=True)\n","\n","len(df)"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["1938"]},"metadata":{"tags":[]},"execution_count":7}]},{"cell_type":"markdown","metadata":{"id":"pb73OMwNlII7"},"source":["Great! We dropped 62 records from the original 2000. Let's continue with the analysis."]},{"cell_type":"markdown","metadata":{"id":"tz_pW7Jula1v"},"source":[""]},{"cell_type":"markdown","metadata":{"id":"y1LAfF2OlII8"},"source":["## Take a quick look at the `label` column:"]},{"cell_type":"code","metadata":{"id":"7CTUwbellII9","outputId":"8146c242-22f8-42ab-a8c4-bb9dcd195470"},"source":["df['label'].value_counts()"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["neg    969\n","pos    969\n","Name: label, dtype: int64"]},"metadata":{"tags":[]},"execution_count":8}]},{"cell_type":"markdown","metadata":{"id":"uAXREdf9lII_"},"source":["## Split the data into train & test sets:"]},{"cell_type":"code","metadata":{"collapsed":true,"id":"SPRSLieQlIJA"},"source":["from sklearn.model_selection import train_test_split\n","\n","X = df['review']\n","y = df['label']\n","\n","X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.33, random_state=42)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"r4UIYF-7lIJA"},"source":["## Build pipelines to vectorize the data, then train and fit a model\n","Now that we have sets to train and test, we'll develop a selection of pipelines, each with a different model."]},{"cell_type":"code","metadata":{"collapsed":true,"id":"wUGErZAxlIJB"},"source":["from sklearn.pipeline import Pipeline\n","from sklearn.feature_extraction.text import TfidfVectorizer\n","from sklearn.naive_bayes import MultinomialNB\n","from sklearn.svm import LinearSVC\n","\n","# Naïve Bayes:\n","text_clf_nb = Pipeline([('tfidf', TfidfVectorizer()),\n","                     ('clf', MultinomialNB()),\n","])\n","\n","# Linear SVC:\n","text_clf_lsvc = Pipeline([('tfidf', TfidfVectorizer()),\n","                     ('clf', LinearSVC()),\n","])"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"sDzWHy53lIJF"},"source":["## Feed the training data through the first pipeline\n","We'll run naïve Bayes first"]},{"cell_type":"code","metadata":{"id":"ye_-48FblIJH","outputId":"defa09d9-199b-43ed-dac9-705629a4e969"},"source":["text_clf_nb.fit(X_train, y_train)"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["Pipeline(memory=None,\n","     steps=[('tfidf', TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n","        dtype=<class 'numpy.float64'>, encoding='utf-8', input='content',\n","        lowercase=True, max_df=1.0, max_features=None, min_df=1,\n","        ngram_range=(1, 1), norm='l2', preprocessor=None, smooth_idf=True,...rue,\n","        vocabulary=None)), ('clf', MultinomialNB(alpha=1.0, class_prior=None, fit_prior=True))])"]},"metadata":{"tags":[]},"execution_count":11}]},{"cell_type":"markdown","metadata":{"id":"46WQC9O5lIJL"},"source":["## Run predictions and analyze the results (naïve Bayes)"]},{"cell_type":"code","metadata":{"collapsed":true,"id":"K7O7vhDilIJM"},"source":["# Form a prediction set\n","predictions = text_clf_nb.predict(X_test)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"E6oW_TyqlIJM","outputId":"3eabd414-457a-4d09-ef62-6aada312d4ad"},"source":["# Report the confusion matrix\n","from sklearn import metrics\n","print(metrics.confusion_matrix(y_test,predictions))"],"execution_count":null,"outputs":[{"output_type":"stream","text":["[[287  21]\n"," [130 202]]\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"jZeRz-QclIJP","outputId":"16947420-b548-450a-808c-dccfde7df32d"},"source":["# Print a classification report\n","print(metrics.classification_report(y_test,predictions))"],"execution_count":null,"outputs":[{"output_type":"stream","text":["              precision    recall  f1-score   support\n","\n","         neg       0.69      0.93      0.79       308\n","         pos       0.91      0.61      0.73       332\n","\n","   micro avg       0.76      0.76      0.76       640\n","   macro avg       0.80      0.77      0.76       640\n","weighted avg       0.80      0.76      0.76       640\n","\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"r16LqPGIlIJQ","outputId":"6f8e7b8a-d771-4216-9db1-c1c666c273bc"},"source":["# Print the overall accuracy\n","print(metrics.accuracy_score(y_test,predictions))"],"execution_count":null,"outputs":[{"output_type":"stream","text":["0.7640625\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"P1yjQvcJlIJR"},"source":["Naïve Bayes gave us better-than-average results at 76.4% for classifying reviews as positive or negative based on text alone. Let's see if we can do better."]},{"cell_type":"markdown","metadata":{"id":"jMTKSBb2lIJT"},"source":["## Feed the training data through the second pipeline\n","Next we'll run Linear SVC"]},{"cell_type":"code","metadata":{"id":"8TPHBe4rlIJV","outputId":"fd56e09b-cf18-4424-d217-f90df62f7e8a"},"source":["text_clf_lsvc.fit(X_train, y_train)"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["Pipeline(memory=None,\n","     steps=[('tfidf', TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n","        dtype=<class 'numpy.float64'>, encoding='utf-8', input='content',\n","        lowercase=True, max_df=1.0, max_features=None, min_df=1,\n","        ngram_range=(1, 1), norm='l2', preprocessor=None, smooth_idf=True,...ax_iter=1000,\n","     multi_class='ovr', penalty='l2', random_state=None, tol=0.0001,\n","     verbose=0))])"]},"metadata":{"tags":[]},"execution_count":16}]},{"cell_type":"markdown","metadata":{"id":"pKbtaHfdlIJW"},"source":["## Run predictions and analyze the results (Linear SVC)"]},{"cell_type":"code","metadata":{"collapsed":true,"id":"JLezKKQxlIJX"},"source":["# Form a prediction set\n","predictions = text_clf_lsvc.predict(X_test)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"9Nx4qYDZlIJX","outputId":"9da7cbd9-0ead-42a1-9e2b-47f560ed5955"},"source":["# Report the confusion matrix\n","from sklearn import metrics\n","print(metrics.confusion_matrix(y_test,predictions))"],"execution_count":null,"outputs":[{"output_type":"stream","text":["[[259  49]\n"," [ 49 283]]\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"GP7oGjamlIJY","outputId":"59a96e23-67e8-44b0-a959-417111b73ca4"},"source":["# Print a classification report\n","print(metrics.classification_report(y_test,predictions))"],"execution_count":null,"outputs":[{"output_type":"stream","text":["              precision    recall  f1-score   support\n","\n","         neg       0.84      0.84      0.84       308\n","         pos       0.85      0.85      0.85       332\n","\n","   micro avg       0.85      0.85      0.85       640\n","   macro avg       0.85      0.85      0.85       640\n","weighted avg       0.85      0.85      0.85       640\n","\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"3bcMNJfplIJZ","outputId":"331851d0-e27c-4c23-8b10-b5efa6c9e768"},"source":["# Print the overall accuracy\n","print(metrics.accuracy_score(y_test,predictions))"],"execution_count":null,"outputs":[{"output_type":"stream","text":["0.846875\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"bfnuaa5GlIJa"},"source":["Not bad! Based on text alone we correctly classified reviews as positive or negative **84.7%** of the time. In an upcoming section we'll try to improve this score even further by performing *sentiment analysis* on the reviews."]},{"cell_type":"markdown","metadata":{"id":"VvlU1XXLlIJb"},"source":["## Advanced Topic - Adding Stopwords to CountVectorizer\n"]},{"cell_type":"markdown","metadata":{"id":"BNxeqTTFloef"},"source":["By default, **CountVectorizer** and **TfidfVectorizer** do *not* filter stopwords. However, they offer some optional settings, including passing in your own stopword list.\n","<div class=\"alert alert-info\" style=\"margin: 20px\">CAUTION: There are some [known issues](http://aclweb.org/anthology/W18-2502) using Scikit-learn's built-in stopwords list. Some words that are filtered may in fact aid in classification. In this section we'll pass in our own stopword list, so that we know exactly what's being filtered.</div>"]},{"cell_type":"markdown","metadata":{"id":"6YD9Xg4PlIJb"},"source":["The [CountVectorizer](https://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.CountVectorizer.html) class accepts the following arguments:\n","> *CountVectorizer(input='content', encoding='utf-8', decode_error='strict', strip_accents=None, lowercase=True, preprocessor=None, tokenizer=None, **stop_words=None**, token_pattern='(?u)\\b\\w\\w+\\b', ngram_range=(1, 1), analyzer='word', max_df=1.0, min_df=1, max_features=None, vocabulary=None, binary=False, dtype=<class 'numpy.int64'>)*\n","\n","[TfidVectorizer](https://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.TfidfVectorizer.html) supports the same arguments and more. Under *stop_words* we have the following options:\n","> stop_words : *string {'english'}, list, or None (default)*\n","\n","That is, we can run `TfidVectorizer(stop_words='english')` to accept scikit-learn's built-in list,<br>\n","or `TfidVectorizer(stop_words=[a, and, the])` to filter these three words. In practice we would assign our list to a variable and pass that in instead."]},{"cell_type":"markdown","metadata":{"id":"u-qVs7hclIJc"},"source":["Scikit-learn's built-in list contains 318 stopwords:\n","> <pre>from sklearn.feature_extraction import text\n","> print(text.ENGLISH_STOP_WORDS)</pre>\n","['a', 'about', 'above', 'across', 'after', 'afterwards', 'again', 'against', 'all', 'almost', 'alone', 'along', 'already', 'also', 'although', 'always', 'am', 'among', 'amongst', 'amoungst', 'amount', 'an', 'and', 'another', 'any', 'anyhow', 'anyone', 'anything', 'anyway', 'anywhere', 'are', 'around', 'as', 'at', 'back', 'be', 'became', 'because', 'become', 'becomes', 'becoming', 'been', 'before', 'beforehand', 'behind', 'being', 'below', 'beside', 'besides', 'between', 'beyond', 'bill', 'both', 'bottom', 'but', 'by', 'call', 'can', 'cannot', 'cant', 'co', 'con', 'could', 'couldnt', 'cry', 'de', 'describe', 'detail', 'do', 'done', 'down', 'due', 'during', 'each', 'eg', 'eight', 'either', 'eleven', 'else', 'elsewhere', 'empty', 'enough', 'etc', 'even', 'ever', 'every', 'everyone', 'everything', 'everywhere', 'except', 'few', 'fifteen', 'fifty', 'fill', 'find', 'fire', 'first', 'five', 'for', 'former', 'formerly', 'forty', 'found', 'four', 'from', 'front', 'full', 'further', 'get', 'give', 'go', 'had', 'has', 'hasnt', 'have', 'he', 'hence', 'her', 'here', 'hereafter', 'hereby', 'herein', 'hereupon', 'hers', 'herself', 'him', 'himself', 'his', 'how', 'however', 'hundred', 'i', 'ie', 'if', 'in', 'inc', 'indeed', 'interest', 'into', 'is', 'it', 'its', 'itself', 'keep', 'last', 'latter', 'latterly', 'least', 'less', 'ltd', 'made', 'many', 'may', 'me', 'meanwhile', 'might', 'mill', 'mine', 'more', 'moreover', 'most', 'mostly', 'move', 'much', 'must', 'my', 'myself', 'name', 'namely', 'neither', 'never', 'nevertheless', 'next', 'nine', 'no', 'nobody', 'none', 'noone', 'nor', 'not', 'nothing', 'now', 'nowhere', 'of', 'off', 'often', 'on', 'once', 'one', 'only', 'onto', 'or', 'other', 'others', 'otherwise', 'our', 'ours', 'ourselves', 'out', 'over', 'own', 'part', 'per', 'perhaps', 'please', 'put', 'rather', 're', 'same', 'see', 'seem', 'seemed', 'seeming', 'seems', 'serious', 'several', 'she', 'should', 'show', 'side', 'since', 'sincere', 'six', 'sixty', 'so', 'some', 'somehow', 'someone', 'something', 'sometime', 'sometimes', 'somewhere', 'still', 'such', 'system', 'take', 'ten', 'than', 'that', 'the', 'their', 'them', 'themselves', 'then', 'thence', 'there', 'thereafter', 'thereby', 'therefore', 'therein', 'thereupon', 'these', 'they', 'thick', 'thin', 'third', 'this', 'those', 'though', 'three', 'through', 'throughout', 'thru', 'thus', 'to', 'together', 'too', 'top', 'toward', 'towards', 'twelve', 'twenty', 'two', 'un', 'under', 'until', 'up', 'upon', 'us', 'very', 'via', 'was', 'we', 'well', 'were', 'what', 'whatever', 'when', 'whence', 'whenever', 'where', 'whereafter', 'whereas', 'whereby', 'wherein', 'whereupon', 'wherever', 'whether', 'which', 'while', 'whither', 'who', 'whoever', 'whole', 'whom', 'whose', 'why', 'will', 'with', 'within', 'without', 'would', 'yet', 'you', 'your', 'yours', 'yourself', 'yourselves']\n","\n","However, there are words in this list that may influence a classification of movie reviews. With this in mind, let's trim the list to just 60 words:"]},{"cell_type":"code","metadata":{"collapsed":true,"id":"Zki3i8IylIJe"},"source":["stopwords = ['a', 'about', 'an', 'and', 'are', 'as', 'at', 'be', 'been', 'but', 'by', 'can', \\\n","             'even', 'ever', 'for', 'from', 'get', 'had', 'has', 'have', 'he', 'her', 'hers', 'his', \\\n","             'how', 'i', 'if', 'in', 'into', 'is', 'it', 'its', 'just', 'me', 'my', 'of', 'on', 'or', \\\n","             'see', 'seen', 'she', 'so', 'than', 'that', 'the', 'their', 'there', 'they', 'this', \\\n","             'to', 'was', 'we', 'were', 'what', 'when', 'which', 'who', 'will', 'with', 'you']"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"ZtHVhcwklIJh"},"source":["Now let's repeat the process above and see if the removal of stopwords improves or impairs our score."]},{"cell_type":"code","metadata":{"collapsed":true,"id":"rt6ekRXPlIJi"},"source":["# YOU DO NOT NEED TO RUN THIS CELL UNLESS YOU HAVE\n","# RECENTLY OPENED THIS NOTEBOOK OR RESTARTED THE KERNEL:\n","\n","import numpy as np\n","import pandas as pd\n","\n","df = pd.read_csv('../TextFiles/moviereviews.tsv', sep='\\t')\n","df.dropna(inplace=True)\n","blanks = []\n","for i,lb,rv in df.itertuples():\n","    if type(rv)==str:\n","        if rv.isspace():\n","            blanks.append(i)\n","df.drop(blanks, inplace=True)\n","from sklearn.model_selection import train_test_split\n","X = df['review']\n","y = df['label']\n","X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.33, random_state=42)\n","\n","from sklearn.pipeline import Pipeline\n","from sklearn.feature_extraction.text import TfidfVectorizer\n","from sklearn.naive_bayes import MultinomialNB\n","from sklearn.svm import LinearSVC\n","from sklearn import metrics"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"BiKGYaUilIJj","outputId":"bf158f8d-8e13-4306-bbb8-47741704a3d7"},"source":["# RUN THIS CELL TO ADD STOPWORDS TO THE LINEAR SVC PIPELINE:\n","text_clf_lsvc2 = Pipeline([('tfidf', TfidfVectorizer(stop_words=stopwords)),\n","                     ('clf', LinearSVC()),\n","])\n","text_clf_lsvc2.fit(X_train, y_train)"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["Pipeline(memory=None,\n","     steps=[('tfidf', TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n","        dtype=<class 'numpy.float64'>, encoding='utf-8', input='content',\n","        lowercase=True, max_df=1.0, max_features=None, min_df=1,\n","        ngram_range=(1, 1), norm='l2', preprocessor=None, smooth_idf=True,...ax_iter=1000,\n","     multi_class='ovr', penalty='l2', random_state=None, tol=0.0001,\n","     verbose=0))])"]},"metadata":{"tags":[]},"execution_count":22}]},{"cell_type":"code","metadata":{"id":"cmAQSaRtlIJk","outputId":"903dd2eb-dc2b-4275-922d-2fe475a1a7c9"},"source":["predictions = text_clf_lsvc2.predict(X_test)\n","print(metrics.confusion_matrix(y_test,predictions))"],"execution_count":null,"outputs":[{"output_type":"stream","text":["[[256  52]\n"," [ 48 284]]\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"VeNp7wovlIJl","outputId":"ed416159-4bf3-4e22-eab0-ee8eab6aa116"},"source":["print(metrics.classification_report(y_test,predictions))"],"execution_count":null,"outputs":[{"output_type":"stream","text":["              precision    recall  f1-score   support\n","\n","         neg       0.84      0.83      0.84       308\n","         pos       0.85      0.86      0.85       332\n","\n","   micro avg       0.84      0.84      0.84       640\n","   macro avg       0.84      0.84      0.84       640\n","weighted avg       0.84      0.84      0.84       640\n","\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"iZJwo_DplIJn","outputId":"9f1807b3-b486-4da0-8225-2ecd3768ae7d"},"source":["print(metrics.accuracy_score(y_test,predictions))"],"execution_count":null,"outputs":[{"output_type":"stream","text":["0.84375\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"NZIFDrlklIJo"},"source":["Our score didn't change that much. We went from 84.7% without filtering stopwords to 84.4% after adding a stopword filter to our pipeline. Keep in mind that 2000 movie reviews is a relatively small dataset. The real gain from stripping stopwords is improved processing speed; depending on the size of the corpus, it might save hours."]},{"cell_type":"markdown","metadata":{"id":"s9lBYtm_lIJo"},"source":["## Feed new data into a trained model\n","Once we've developed a fairly accurate model, it's time to feed new data through it. In this last section we'll write our own review, and see how accurately our model assigns a \"positive\" or \"negative\" label to it."]},{"cell_type":"markdown","metadata":{"id":"rXdaVGVPlIJp"},"source":["### First, train the model"]},{"cell_type":"code","metadata":{"collapsed":true,"id":"NWfwMrpblIJp"},"source":["# YOU DO NOT NEED TO RUN THIS CELL UNLESS YOU HAVE\n","# RECENTLY OPENED THIS NOTEBOOK OR RESTARTED THE KERNEL:\n","\n","import numpy as np\n","import pandas as pd\n","\n","df = pd.read_csv('../TextFiles/moviereviews.tsv', sep='\\t')\n","df.dropna(inplace=True)\n","blanks = []\n","for i,lb,rv in df.itertuples():\n","    if type(rv)==str:\n","        if rv.isspace():\n","            blanks.append(i)\n","df.drop(blanks, inplace=True)\n","from sklearn.model_selection import train_test_split\n","X = df['review']\n","y = df['label']\n","X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.33, random_state=42)\n","\n","from sklearn.pipeline import Pipeline\n","from sklearn.feature_extraction.text import TfidfVectorizer\n","from sklearn.naive_bayes import MultinomialNB\n","from sklearn.svm import LinearSVC\n","from sklearn import metrics\n","\n","# Naïve Bayes Model:\n","text_clf_nb = Pipeline([('tfidf', TfidfVectorizer()),\n","                     ('clf', MultinomialNB()),\n","])\n","\n","# Linear SVC Model:\n","text_clf_lsvc = Pipeline([('tfidf', TfidfVectorizer()),\n","                     ('clf', LinearSVC()),\n","])\n","\n","# Train both models on the moviereviews.tsv training set:\n","text_clf_nb.fit(X_train, y_train)\n","text_clf_lsvc.fit(X_train, y_train)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"mq_RxWxTlIJr"},"source":["### Next, feed new data to the model's `predict()` method"]},{"cell_type":"code","metadata":{"collapsed":true,"id":"Mct2tIU-lIJs"},"source":["myreview = \"A movie I really wanted to love was terrible. \\\n","I'm sure the producers had the best intentions, but the execution was lacking.\""],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"collapsed":true,"id":"qPnDQ3kAlIJt"},"source":["# Use this space to write your own review. Experiment with different lengths and writing styles.\n","myreview = \n","\n","\n","\n","\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"h7OOS0LVlIJt","outputId":"d59ab254-8312-4b1f-b4a6-0090be825185"},"source":["print(text_clf_nb.predict([myreview]))  # be sure to put \"myreview\" inside square brackets"],"execution_count":null,"outputs":[{"output_type":"stream","text":["['neg']\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"uSLrvTiTlIJv","outputId":"783366f4-a112-4cda-a228-988c9e5f5a93"},"source":["print(text_clf_lsvc.predict([myreview]))"],"execution_count":null,"outputs":[{"output_type":"stream","text":["['neg']\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"KS2-9f2SlIJx"},"source":["Great! Now you should be able to build text classification pipelines in scikit-learn, apply a variety of algorithms like naïve Bayes and Linear SVC, handle stopwords, and test a fitted model on new data."]},{"cell_type":"markdown","metadata":{"id":"a9ZAIrKGlIJy"},"source":["## Up next: Text Classification Assessment"]},{"cell_type":"markdown","metadata":{"id":"LZQGalAGrAds"},"source":["# New Section"]},{"cell_type":"markdown","metadata":{"id":"mIyiTT8xrGdU"},"source":["# Semantics and Word Vectors\n","\n"]},{"cell_type":"markdown","metadata":{"id":"1X2CBZYyrOgc"},"source":["Sometimes called \"opinion mining\", [Wikipedia](https://en.wikipedia.org/wiki/Sentiment_analysis) defines ***sentiment analysis*** as\n","<div class=\"alert alert-info\" style=\"margin: 20px\">\"the use of natural language processing ... to systematically identify, extract, quantify, and study affective states and subjective information.<br>\n","Generally speaking, sentiment analysis aims to determine the attitude of a speaker, writer, or other subject with respect to some topic or the overall contextual polarity or emotional reaction to a document, interaction, or event.\"</div>\n","\n","Up to now we've used the occurrence of specific words and word patterns to perform test classifications. In this section we'll take machine learning even further, and try to extract intended meanings from complex phrases. Some simple examples include:\n","* Python is relatively easy to learn.\n","* That was the worst movie I've ever seen.\n","\n","However, things get harder with phrases like:\n","* I do not dislike green eggs and ham. (requires negation handling)\n","\n","The way this is done is through complex machine learning algorithms like [word2vec](https://en.wikipedia.org/wiki/Word2vec). The idea is to create numerical arrays, or *word embeddings* for every word in a large corpus. Each word is assigned its own vector in such a way that words that frequently appear together in the same context are given vectors that are close together. The result is a model that may not know that a \"lion\" is an animal, but does know that \"lion\" is closer in context to \"cat\" than \"dandelion\".\n","\n","It is important to note that *building* useful models takes a long time - hours or days to train a large corpus - and that for our purposes it is best to import an existing model rather than take the time to train our own."]},{"cell_type":"markdown","metadata":{"id":"kBmulFzCrGdV"},"source":["___\n","# Installing Larger spaCy Models\n"]},{"cell_type":"markdown","metadata":{"id":"LuPr8wybrSkM"},"source":["Up to now we've been using spaCy's smallest English language model, [**en_core_web_sm**](https://spacy.io/models/en#en_core_web_sm) (35MB), which provides vocabulary, syntax, and entities, but not vectors. To take advantage of built-in word vectors we'll need a larger library. We have a few options:\n","> [**en_core_web_md**](https://spacy.io/models/en#en_core_web_md) (116MB) Vectors: 685k keys, 20k unique vectors (300 dimensions)\n","> <br>or<br>\n","> [**en_core_web_lg**](https://spacy.io/models/en#en_core_web_lg) (812MB) Vectors: 685k keys, 685k unique vectors (300 dimensions)\n","\n","If you plan to rely heavily on word vectors, consider using spaCy's largest vector library containing over one million unique vectors:\n","> [**en_vectors_web_lg**](https://spacy.io/models/en#en_vectors_web_lg) (631MB) Vectors: 1.1m keys, 1.1m unique vectors (300 dimensions)\n","\n","For our purposes **en_core_web_md** should suffice.\n","\n","### From the command line (you must run this as admin or use sudo):\n","\n","> `activate spacyenv`&emsp;*if using a virtual environment*   \n","> \n","> `python -m spacy download en_core_web_md`  \n","> `python -m spacy download en_core_web_lg`&emsp;&emsp;&ensp;*optional library*  \n","> `python -m spacy download en_vectors_web_lg`&emsp;*optional library*  \n","\n","> ### If successful, you should see a message like: \n","> <tt><br>\n","> **Linking successful**<br>\n","> C:\\Anaconda3\\envs\\spacyenv\\lib\\site-packages\\en_core_web_md --><br>\n","> C:\\Anaconda3\\envs\\spacyenv\\lib\\site-packages\\spacy\\data\\en_core_web_md<br>\n","> <br>\n","> You can now load the model via spacy.load('en_core_web_md')</tt>\n","\n","<font color=green>Of course, we have a third option, and that is to train our own vectors from a large corpus of documents. Unfortunately this would take a prohibitively large amount of time and processing power.</font> "]},{"cell_type":"markdown","metadata":{"id":"Qp3ECJzorGdX"},"source":["___\n","# Word Vectors\n"]},{"cell_type":"markdown","metadata":{"id":"zFv3a7u0rlFK"},"source":["Word vectors - also called *word embeddings* - are mathematical descriptions of individual words such that words that appear frequently together in the language will have similar values. In this way we can mathematically derive *context*. As mentioned above, the word vector for \"lion\" will be closer in value to \"cat\" than to \"dandelion\"."]},{"cell_type":"markdown","metadata":{"id":"jLcl4z4BrGdY"},"source":["## Vector values\n"]},{"cell_type":"markdown","metadata":{"id":"1VFt-pG1rps6"},"source":["So what does a word vector look like? Since spaCy employs 300 dimensions, word vectors are stored as 300-item arrays.\n","\n","Note that we would see the same set of values with **en_core_web_md** and **en_core_web_lg**, as both were trained using the [word2vec](https://en.wikipedia.org/wiki/Word2vec) family of algorithms."]},{"cell_type":"code","metadata":{"collapsed":true,"id":"KevjPitdrGdY"},"source":["# Import spaCy and load the language library\n","import spacy\n","nlp = spacy.load('en_core_web_md')  # make sure to use a larger model!"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"scrolled":true,"id":"vH-4V3zNrGdZ","outputId":"6eaecd18-2340-44bc-e89e-46dca27cd53e"},"source":["nlp(u'lion').vector"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["array([  1.89630002e-01,  -4.03090000e-01,   3.53500009e-01,\n","        -4.79070008e-01,  -4.33109999e-01,   2.38570005e-01,\n","         2.69620001e-01,   6.43320009e-02,   3.07669997e-01,\n","         1.37119997e+00,  -3.75820011e-01,  -2.27129996e-01,\n","        -3.56570005e-01,  -2.53549993e-01,   1.75429992e-02,\n","         3.39619994e-01,   7.47229978e-02,   5.12260020e-01,\n","        -3.97590011e-01,   5.13330009e-03,  -3.09289992e-01,\n","         4.89110015e-02,  -1.86100006e-01,  -4.17019993e-01,\n","        -8.16389978e-01,  -1.69080004e-01,  -2.62459993e-01,\n","        -1.59830004e-02,   1.24789998e-01,  -3.72759998e-02,\n","        -5.71250021e-01,  -1.62959993e-01,   1.23760000e-01,\n","        -5.54639995e-02,   1.32440001e-01,   2.75190007e-02,\n","         1.25919998e-01,  -3.27219993e-01,  -4.91649985e-01,\n","        -3.55589986e-01,  -3.06300014e-01,   6.11849986e-02,\n","        -1.69320002e-01,  -6.24050014e-02,   6.57630026e-01,\n","        -2.79249996e-01,  -3.04499990e-03,  -2.23999992e-02,\n","        -2.80149996e-01,  -2.19750002e-01,  -4.31879997e-01,\n","         3.98639999e-02,  -2.21019998e-01,  -4.26930003e-02,\n","         5.27479984e-02,   2.87259996e-01,   1.23149998e-01,\n","        -2.86619999e-02,   7.82940015e-02,   4.67539996e-01,\n","        -2.45890006e-01,  -1.10639997e-01,   7.22500011e-02,\n","        -9.49800014e-02,  -2.75480002e-01,  -5.40970027e-01,\n","         1.28230006e-01,  -8.24080035e-02,   3.10350001e-01,\n","        -6.33940026e-02,  -7.37550020e-01,  -5.49920022e-01,\n","         9.99990031e-02,  -2.07580000e-01,  -3.96739990e-02,\n","         2.06640005e-01,  -9.75570008e-02,  -3.70920002e-01,\n","         2.79009998e-01,  -6.22179985e-01,  -1.02799997e-01,\n","         2.32710004e-01,   4.38380003e-01,   3.24449986e-02,\n","        -2.98660010e-01,  -7.36109987e-02,   7.15939999e-01,\n","         1.42409995e-01,   2.77700007e-01,  -3.98920000e-01,\n","         3.66559997e-02,   1.57590002e-01,   8.20140019e-02,\n","        -5.73430002e-01,   3.54570001e-01,   2.24910006e-01,\n","        -6.26990020e-01,  -8.81059989e-02,   2.43609995e-01,\n","         3.85329992e-01,  -1.40829995e-01,   1.76909998e-01,\n","         7.08969980e-02,   1.79509997e-01,  -4.59069997e-01,\n","        -8.21200013e-01,  -2.66309995e-02,   6.25490025e-02,\n","         4.24149990e-01,  -8.96300003e-02,  -2.46539995e-01,\n","         1.41560003e-01,   4.01870012e-01,  -4.12319988e-01,\n","         8.45159963e-02,  -1.06260002e-01,   7.31450021e-01,\n","         1.92169994e-01,   1.42399997e-01,   2.85109997e-01,\n","        -2.94539988e-01,  -2.19479993e-01,   9.04600024e-01,\n","        -1.90980002e-01,  -1.03400004e+00,  -1.57539994e-01,\n","        -1.19640000e-01,   4.98879999e-01,  -1.06239998e+00,\n","        -3.28200012e-01,  -1.12319998e-02,  -7.94820011e-01,\n","         3.72750014e-01,  -6.87099993e-03,  -2.57719994e-01,\n","        -4.70050007e-01,  -4.13870007e-01,  -6.40890002e-02,\n","        -2.80330002e-01,  -4.07779999e-02,  -2.48659992e+00,\n","         6.24939986e-03,  -1.02100000e-02,   1.27519995e-01,\n","         3.49649996e-01,  -1.25709996e-01,   3.15699995e-01,\n","         4.19259995e-01,   2.00560004e-01,  -5.59840024e-01,\n","        -2.28009999e-01,   1.20119996e-01,  -2.05180002e-03,\n","        -8.97639990e-02,  -8.03729966e-02,   1.19690001e-02,\n","        -2.69780010e-01,   3.48289996e-01,   7.36640021e-03,\n","        -1.11369997e-01,   6.34100020e-01,   3.84490013e-01,\n","        -6.22479975e-01,   4.11450006e-02,   2.59220004e-01,\n","         6.58110023e-01,  -4.95480001e-01,  -1.30300000e-01,\n","        -3.82789999e-01,   1.11560002e-01,  -4.30849999e-01,\n","         3.44729990e-01,   2.71090008e-02,  -2.51080006e-01,\n","        -2.80110002e-01,   2.16619998e-01,   3.26599985e-01,\n","         5.58950007e-02,   7.60769993e-02,  -5.24800010e-02,\n","         4.59280014e-02,  -2.52660006e-01,   5.28450012e-01,\n","        -1.31449997e-01,  -1.24530002e-01,   4.05559987e-01,\n","         3.18769991e-01,   2.44149994e-02,  -2.26199999e-01,\n","        -6.19599998e-01,  -4.08859998e-01,  -3.55339982e-02,\n","        -5.51229995e-03,   2.34380007e-01,   8.78539979e-01,\n","        -2.51610011e-01,   4.05999988e-01,  -4.42840010e-01,\n","         3.49339992e-01,  -5.64289987e-01,  -2.36760005e-01,\n","         6.21990025e-01,  -2.81749994e-01,   4.20240015e-01,\n","         1.00429997e-01,  -1.47200003e-01,   4.95929986e-01,\n","        -3.58500004e-01,  -1.39980003e-01,  -2.74940014e-01,\n","         2.38270000e-01,   5.72679996e-01,   7.90250003e-02,\n","         1.78720001e-02,  -2.18290001e-01,   5.50500005e-02,\n","        -5.41999996e-01,   1.67879999e-01,   3.90650004e-01,\n","         3.02089989e-01,   2.30399996e-01,  -3.93510014e-02,\n","        -2.10779995e-01,  -2.72240013e-01,   1.69070005e-01,\n","         5.48189998e-01,   9.48880017e-02,   7.97980011e-01,\n","        -6.61579967e-02,   1.98440000e-01,   2.03070000e-01,\n","         4.48080003e-02,  -1.02399997e-01,  -6.99089989e-02,\n","        -3.67560014e-02,   9.51590016e-02,  -2.78299987e-01,\n","        -1.05970003e-01,  -1.62760004e-01,  -1.82109997e-01,\n","        -3.18969995e-01,  -2.16330007e-01,   1.49939999e-01,\n","        -7.20570013e-02,   2.22639993e-01,  -4.55509990e-01,\n","         3.03409994e-01,   1.84310004e-01,   2.16810003e-01,\n","        -3.19400012e-01,   2.64259994e-01,   5.81059992e-01,\n","         5.46349995e-02,   6.32380009e-01,   4.31690007e-01,\n","         9.03429985e-02,   1.94940001e-01,   3.54829997e-01,\n","        -2.07059998e-02,  -7.31169999e-01,   1.29409999e-01,\n","         1.74180001e-01,  -1.50649995e-01,   5.33550009e-02,\n","         4.47940007e-02,  -1.65999994e-01,   2.20070004e-01,\n","        -5.39699972e-01,  -2.49679998e-01,  -2.64640003e-01,\n","        -5.55149972e-01,   5.82419991e-01,   2.22949997e-01,\n","         2.44330004e-01,   4.52749997e-01,   3.46929997e-01,\n","         1.22550003e-01,  -3.90589982e-02,  -3.27490002e-01,\n","        -2.78910011e-01,   1.37659997e-01,   3.83920014e-01,\n","         1.05430000e-03,  -1.02420002e-02,   4.92049992e-01,\n","        -1.79220006e-01,   4.12149988e-02,   1.35470003e-01,\n","        -2.05980003e-01,  -2.31940001e-01,  -7.77010024e-01,\n","        -3.82369995e-01,  -7.63830006e-01,   1.94179997e-01,\n","        -1.54410005e-01,   8.97400022e-01,   3.06259990e-01,\n","         4.03759986e-01,   2.17380002e-01,  -3.80499989e-01], dtype=float32)"]},"metadata":{"tags":[]},"execution_count":2}]},{"cell_type":"markdown","metadata":{"id":"2hh84xBkrGdb"},"source":["What's interesting is that Doc and Span objects themselves have vectors, derived from the averages of individual token vectors. <br>This makes it possible to compare similarities between whole documents."]},{"cell_type":"code","metadata":{"id":"lkw2VwzrrGdb","outputId":"776a51c7-0ae0-4963-e2a2-0a8777443654"},"source":["doc = nlp(u'The quick brown fox jumped over the lazy dogs.')\n","\n","doc.vector"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["array([ -1.96635887e-01,  -2.32740352e-03,  -5.36607020e-02,\n","        -6.10564947e-02,  -4.08843048e-02,   1.45266443e-01,\n","        -1.08268000e-01,  -6.27789786e-03,   1.48455709e-01,\n","         1.90697408e+00,  -2.57692993e-01,  -1.95818534e-03,\n","        -1.16141019e-02,  -1.62858292e-01,  -1.62938282e-01,\n","         1.18210977e-02,   5.12646027e-02,   1.00078702e+00,\n","        -2.01447997e-02,  -2.54611671e-01,  -1.28316596e-01,\n","        -1.97198763e-02,  -2.89733019e-02,  -1.94347113e-01,\n","         1.26644447e-01,  -8.69869068e-02,  -2.20812604e-01,\n","        -1.58452198e-01,   9.86308008e-02,  -1.79210991e-01,\n","        -1.55290633e-01,   1.95643142e-01,   2.66436003e-02,\n","        -1.64984968e-02,   1.18824698e-01,  -1.17830629e-03,\n","         4.99809943e-02,  -4.23077159e-02,  -3.86111848e-02,\n","        -7.47400150e-03,   1.23448208e-01,   9.60620027e-03,\n","        -3.32463719e-02,  -1.77848607e-01,   1.19390726e-01,\n","         1.87545009e-02,  -1.84173390e-01,   6.91781715e-02,\n","         1.28520593e-01,   1.48827005e-02,  -1.78013414e-01,\n","         1.10003807e-01,  -3.35464999e-02,  -1.52476998e-02,\n","        -9.41195935e-02,   1.58633105e-02,  -1.29811959e-02,\n","         1.40140295e-01,  -1.47720069e-01,  -3.81718054e-02,\n","         4.66808230e-02,   3.31423879e-02,   7.97965974e-02,\n","         1.60014004e-01,   8.90410226e-03,  -1.01237908e-01,\n","         7.39663988e-02,   2.47380026e-02,   4.26153988e-02,\n","         9.66729969e-02,   2.87616011e-02,   7.22841993e-02,\n","         1.76565602e-01,   7.55538046e-02,   1.10501610e-01,\n","        -1.02358103e-01,  -5.43345436e-02,  -4.12176028e-02,\n","         3.98623049e-02,  -2.98339734e-03,  -5.32988012e-02,\n","         1.90624595e-01,  -6.42587021e-02,  -1.76225007e-02,\n","         3.94165330e-02,  -1.14773512e-01,   4.25241649e-01,\n","         2.07243040e-01,   2.60730416e-01,   1.31226778e-01,\n","        -8.00508037e-02,   6.88939020e-02,   7.05293044e-02,\n","        -1.10744104e-01,   4.14580032e-02,   5.13269613e-03,\n","        -1.29179001e-01,  -5.84542975e-02,   9.13560018e-02,\n","        -1.75975591e-01,   9.52741057e-02,   1.37699964e-02,\n","        -1.30865201e-01,  -4.76420000e-02,   1.61670998e-01,\n","        -6.76959991e-01,   2.68619388e-01,  -7.94106945e-02,\n","         8.56394917e-02,  -5.94138019e-02,   7.44821057e-02,\n","        -1.67490095e-01,   1.97447598e-01,  -2.71580786e-01,\n","         1.51915969e-02,   1.12019002e-01,  -4.32585999e-02,\n","        -1.03554968e-02,   6.33272156e-02,   5.20200143e-03,\n","         4.94491048e-02,  -1.07016601e-01,  -6.45387918e-02,\n","        -1.76269561e-01,  -1.98135704e-01,   4.17800918e-02,\n","         1.23686995e-02,  -1.13280594e-01,  -4.03523073e-02,\n","        -4.21132054e-03,  -9.65667963e-02,  -7.12300017e-02,\n","        -2.19088510e-01,   6.41715974e-02,   1.11634992e-01,\n","        -7.12868944e-02,  -8.27060193e-02,   1.53889004e-02,\n","         6.84699565e-02,  -5.50561920e-02,  -1.84788990e+00,\n","        -4.75010052e-02,   1.31487206e-01,   1.03359401e-01,\n","         1.80857688e-01,  -8.03041980e-02,   2.27739997e-02,\n","         5.56868985e-02,   9.20986086e-02,   6.22248054e-02,\n","         4.86670025e-02,  -4.06427011e-02,   3.83703932e-02,\n","        -4.05869968e-02,  -2.26339817e-01,   3.69174965e-02,\n","        -1.30066186e-01,   1.27621710e-01,   2.76701003e-02,\n","        -1.39992401e-01,  -3.75526994e-02,  -8.11104029e-02,\n","        -1.78196102e-01,  -1.21652998e-01,  -5.88919744e-02,\n","        -1.06128812e-01,  -4.72390745e-03,  -1.14130601e-01,\n","        -7.60087445e-02,  -9.48704034e-02,   1.68780401e-01,\n","         3.82669941e-02,  -1.68303996e-01,  -1.30991384e-01,\n","        -2.46409744e-01,   1.42855030e-02,   1.23633012e-01,\n","         7.95699935e-03,  -3.22283022e-02,   3.75844017e-02,\n","        -4.48104031e-02,  -2.00578898e-01,  -2.86081016e-01,\n","        -1.83181003e-01,  -5.46899159e-04,   6.52990937e-02,\n","         2.34263036e-02,  -7.60660022e-02,   1.13897599e-01,\n","        -7.05380812e-02,   1.30277812e-01,   2.83973999e-02,\n","         1.73887815e-02,  -1.71358977e-02,   1.78455990e-02,\n","         1.86773703e-01,   1.83613986e-01,  -4.05438878e-02,\n","         1.28929759e-03,  -3.71900201e-03,  -1.97373003e-01,\n","         4.78463694e-02,  -2.21408010e-01,   2.68826094e-02,\n","         2.40951017e-01,   7.42616802e-02,   7.53984973e-02,\n","        -7.67349079e-02,  -5.37766796e-03,  -8.06540065e-03,\n","         1.88790001e-02,   8.31135064e-02,  -5.20760007e-02,\n","         1.29393607e-01,   4.09864075e-02,   7.31946975e-02,\n","        -1.64099425e-01,   1.17529690e-01,  -6.96440935e-02,\n","         1.91028208e-01,   1.01721905e-01,   6.34808987e-02,\n","        -8.29815865e-02,  -6.95784390e-03,  -1.69757873e-01,\n","        -2.02478573e-01,   3.65395918e-02,   1.32345587e-01,\n","         3.53013016e-02,   2.27603033e-01,  -1.52753398e-01,\n","         7.80210178e-03,   2.06879750e-02,  -8.63540452e-03,\n","         9.85722095e-02,  -2.91380938e-02,  -1.42988954e-02,\n","        -9.39018354e-02,   1.43968105e-01,   7.82396942e-02,\n","        -1.93540990e-01,  -9.36544985e-02,  -8.23533013e-02,\n","         4.40272018e-02,  -2.22195080e-03,  -1.29856914e-01,\n","        -1.53841600e-01,  -1.55329984e-02,  -2.55266696e-01,\n","         1.14425398e-01,  -1.03161987e-02,  -4.66439016e-02,\n","        -5.69390282e-02,   7.72780031e-02,   1.28908500e-01,\n","         1.61679000e-01,   1.50837511e-01,   6.18334934e-02,\n","        -9.06937942e-02,  -3.52137014e-02,   1.35956988e-01,\n","         7.52059072e-02,   5.73905036e-02,  -1.65402606e-01,\n","         1.68419987e-01,  -1.83722824e-01,   5.91069926e-03,\n","        -1.25354990e-01,   3.95771042e-02,   5.67352995e-02,\n","        -5.63519308e-03,   1.53597593e-01,  -6.84822723e-02,\n","        -1.40976995e-01,  -3.62732522e-02,  -2.61475928e-02,\n","         2.50091963e-02,   1.18994810e-01,  -2.66857035e-02,\n","         7.50442073e-02,   2.04583794e-01,   4.37736101e-02,\n","        -8.17096978e-02,   6.80228025e-02,   5.50465994e-02,\n","        -2.39979066e-02,   7.68290013e-02,  -5.76773956e-02,\n","         8.30340981e-02,   3.63199934e-02,  -1.65820405e-01,\n","         2.55408939e-02,  -5.30679002e-02,  -1.35961995e-01,\n","        -1.03501797e-01,   1.36406809e-01,   9.66293067e-02,\n","         7.33902007e-02,  -1.83055893e-01,  -2.73141060e-02], dtype=float32)"]},"metadata":{"tags":[]},"execution_count":3}]},{"cell_type":"markdown","metadata":{"id":"TgCuHyx_rnL0"},"source":[""]},{"cell_type":"markdown","metadata":{"id":"3o6LOilvrGdc"},"source":["## Identifying similar vectors\n","The best way to expose vector relationships is through the `.similarity()` method of Doc tokens."]},{"cell_type":"code","metadata":{"scrolled":true,"id":"Rfw2uC6jrGdc","outputId":"a830e8f8-8338-4eae-a3a1-2fb5d50a7039"},"source":["# Create a three-token Doc object:\n","tokens = nlp(u'lion cat pet')\n","\n","# Iterate through token combinations:\n","for token1 in tokens:\n","    for token2 in tokens:\n","        print(token1.text, token2.text, token1.similarity(token2))"],"execution_count":null,"outputs":[{"output_type":"stream","text":["lion lion 1.0\n","lion cat 0.526544\n","lion pet 0.399238\n","cat lion 0.526544\n","cat cat 1.0\n","cat pet 0.750546\n","pet lion 0.399238\n","pet cat 0.750546\n","pet pet 1.0\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"bqK6KjMirGdc"},"source":["<font color=green>Note that order doesn't matter. `token1.similarity(token2)` has the same value as `token2.similarity(token1)`.</font>\n","#### To view this as a table:"]},{"cell_type":"code","metadata":{"id":"NUaxkfyOrGdd","outputId":"e608b7db-5f52-4c7c-8c2c-9918739f42c3"},"source":["# For brevity, assign each token a name\n","a,b,c = tokens\n","\n","# Display as a Markdown table (this only works in Jupyter!)\n","from IPython.display import Markdown, display\n","display(Markdown(f'<table><tr><th></th><th>{a.text}</th><th>{b.text}</th><th>{c.text}</th></tr>\\\n","<tr><td>**{a.text}**</td><td>{a.similarity(a):{.4}}</td><td>{b.similarity(a):{.4}}</td><td>{c.similarity(a):{.4}}</td></tr>\\\n","<tr><td>**{b.text}**</td><td>{a.similarity(b):{.4}}</td><td>{b.similarity(b):{.4}}</td><td>{c.similarity(b):{.4}}</td></tr>\\\n","<tr><td>**{c.text}**</td><td>{a.similarity(c):{.4}}</td><td>{b.similarity(c):{.4}}</td><td>{c.similarity(c):{.4}}</td></tr>'))"],"execution_count":null,"outputs":[{"output_type":"display_data","data":{"text/markdown":"<table><tr><th></th><th>lion</th><th>cat</th><th>pet</th></tr><tr><td>**lion**</td><td>1.0</td><td>0.5265</td><td>0.3992</td></tr><tr><td>**cat**</td><td>0.5265</td><td>1.0</td><td>0.7505</td></tr><tr><td>**pet**</td><td>0.3992</td><td>0.7505</td><td>1.0</td></tr>","text/plain":["<IPython.core.display.Markdown object>"]},"metadata":{"tags":[]}}]},{"cell_type":"markdown","metadata":{"id":"7Ycbh_BsrGdd"},"source":["As expected, we see the strongest similarity between \"cat\" and \"pet\", the weakest between \"lion\" and \"pet\", and some similarity between \"lion\" and \"cat\". A word will have a perfect (1.0) similarity with itself.\n","\n","If you're curious, the similarity between \"lion\" and \"dandelion\" is very small:"]},{"cell_type":"code","metadata":{"id":"EFkQ5AvyrGdd","outputId":"9baa469c-b6cc-4c03-da09-8a7bab0c4da8"},"source":["nlp(u'lion').similarity(nlp(u'dandelion'))"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["0.18064451829601527"]},"metadata":{"tags":[]},"execution_count":6}]},{"cell_type":"markdown","metadata":{"id":"nwngmtdJrGde"},"source":["### Opposites are not necessarily different\n","Words that have opposite meaning, but that often appear in the same *context* may have similar vectors."]},{"cell_type":"code","metadata":{"id":"LYBE6DDQrGde","outputId":"420d74c6-cb98-4827-ee1c-748e579847ec"},"source":["# Create a three-token Doc object:\n","tokens = nlp(u'like love hate')\n","\n","# Iterate through token combinations:\n","for token1 in tokens:\n","    for token2 in tokens:\n","        print(token1.text, token2.text, token1.similarity(token2))"],"execution_count":null,"outputs":[{"output_type":"stream","text":["like like 1.0\n","like love 0.657904\n","like hate 0.657465\n","love like 0.657904\n","love love 1.0\n","love hate 0.63931\n","hate like 0.657465\n","hate love 0.63931\n","hate hate 1.0\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"PkxxwHenrGde"},"source":["## Vector norms\n"]},{"cell_type":"markdown","metadata":{"id":"M4n7k5fQrt_5"},"source":["It's sometimes helpful to aggregate 300 dimensions into a [Euclidian (L2) norm](https://en.wikipedia.org/wiki/Norm_%28mathematics%29#Euclidean_norm), computed as the square root of the sum-of-squared-vectors. This is accessible as the `.vector_norm` token attribute. Other helpful attributes include `.has_vector` and `.is_oov` or *out of vocabulary*.\n","\n","For example, our 685k vector library may not have the word \"[nargle](https://en.wikibooks.org/wiki/Muggles%27_Guide_to_Harry_Potter/Magic/Nargle)\". To test this:"]},{"cell_type":"code","metadata":{"id":"rvFW2goirGdf","outputId":"87a03550-99f4-4ad5-efa5-b07b104e954d"},"source":["tokens = nlp(u'dog cat nargle')\n","\n","for token in tokens:\n","    print(token.text, token.has_vector, token.vector_norm, token.is_oov)"],"execution_count":null,"outputs":[{"output_type":"stream","text":["dog True 7.03367 False\n","cat True 6.68082 False\n","nargle False 0.0 True\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"7lU-43iTrGdf"},"source":["Indeed we see that \"nargle\" does not have a vector, so the vector_norm value is zero, and it identifies as *out of vocabulary*."]},{"cell_type":"markdown","metadata":{"id":"bI-yN5VvrGdf"},"source":["## Vector arithmetic\n","Believe it or not, we can actually calculate new vectors by adding & subtracting related vectors. A famous example suggests\n","<pre>\"king\" - \"man\" + \"woman\" = \"queen\"</pre>\n","Let's try it out!"]},{"cell_type":"code","metadata":{"id":"qe_QYteorGdg","outputId":"d3da7418-8540-447d-9fba-1f3213e0f6a7"},"source":["from scipy import spatial\n","\n","cosine_similarity = lambda x, y: 1 - spatial.distance.cosine(x, y)\n","\n","king = nlp.vocab['king'].vector\n","man = nlp.vocab['man'].vector\n","woman = nlp.vocab['woman'].vector\n","\n","# Now we find the closest vector in the vocabulary to the result of \"man\" - \"woman\" + \"queen\"\n","new_vector = king - man + woman\n","computed_similarities = []\n","\n","for word in nlp.vocab:\n","    # Ignore words without vectors and mixed-case words:\n","    if word.has_vector:\n","        if word.is_lower:\n","            if word.is_alpha:\n","                similarity = cosine_similarity(new_vector, word.vector)\n","                computed_similarities.append((word, similarity))\n","\n","computed_similarities = sorted(computed_similarities, key=lambda item: -item[1])\n","\n","print([w[0].text for w in computed_similarities[:10]])"],"execution_count":null,"outputs":[{"output_type":"stream","text":["['king', 'queen', 'commoner', 'highness', 'prince', 'sultan', 'maharajas', 'princes', 'kumbia', 'kings']\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"QIpfGAu3rGdg"},"source":["So in this case, \"king\" was still closer than \"queen\" to our calculated vector, although \"queen\" did show up!"]},{"cell_type":"markdown","metadata":{"id":"j-VsNOx5u8cs"},"source":["# Sentiment Analysis\n"]},{"cell_type":"markdown","metadata":{"id":"oYEj96bSvByL"},"source":["Now that we've seen word vectors we can start to investigate sentiment analysis. The goal is to find commonalities between documents, with the understanding that similarly *combined* vectors should correspond to similar sentiments.\n","\n","While the scope of sentiment analysis is very broad, we will focus our work in two ways.\n","\n","### 1. Polarity classification\n","We won't try to determine if a sentence is objective or subjective, fact or opinion. Rather, we care only if the text expresses a *positive*, *negative* or *neutral* opinion.\n","### 2. Document level scope\n","We'll also try to aggregate all of the sentences in a document or paragraph, to arrive at an overall opinion.\n","### 3. Coarse analysis\n","We won't try to perform a fine-grained analysis that would determine the degree of positivity/negativity. That is, we're not trying to guess how many stars a reviewer awarded, just whether the review was positive or negative."]},{"cell_type":"markdown","metadata":{"id":"xOsWMC4pu8cu"},"source":["## Broad Steps:\n","* First, consider the text being analyzed. A model trained on paragraph-long movie reviews might not be effective on tweets. Make sure to use an appropriate model for the task at hand.\n","* Next, decide the type of analysis to perform. In the previous section on text classification we used a bag-of-words technique that considered only single tokens, or *unigrams*. Some rudimentary sentiment analysis models go one step further, and consider two-word combinations, or *bigrams*. In this section, we'd like to work with complete sentences, and for this we're going to import a trained NLTK lexicon called *VADER*."]},{"cell_type":"markdown","metadata":{"id":"P0ss44wXu8cu"},"source":["## NLTK's VADER module\n","VADER is an NLTK module that provides sentiment scores based on words used (\"completely\" boosts a score, while \"slightly\" reduces it), on capitalization & punctuation (\"GREAT!!!\" is stronger than \"great.\"), and negations (words like \"isn't\" and \"doesn't\" affect the outcome).\n","<br>To view the source code visit https://www.nltk.org/_modules/nltk/sentiment/vader.html"]},{"cell_type":"markdown","metadata":{"id":"EnE9m7kXu8cv"},"source":["**Download the VADER lexicon.** You only need to do this once."]},{"cell_type":"code","metadata":{"id":"HkUfBklCu8cv","outputId":"c3e8bc4f-722f-49dc-bb9b-be9168fe48b1"},"source":["import nltk\n","nltk.download('vader_lexicon')"],"execution_count":null,"outputs":[{"output_type":"stream","text":["[nltk_data] Downloading package vader_lexicon to\n","[nltk_data]     C:\\Users\\Mike\\AppData\\Roaming\\nltk_data...\n","[nltk_data]   Package vader_lexicon is already up-to-date!\n"],"name":"stderr"},{"output_type":"execute_result","data":{"text/plain":["True"]},"metadata":{"tags":[]},"execution_count":1}]},{"cell_type":"markdown","metadata":{"id":"uF5K3YUtu8cx"},"source":["<div class=\"alert alert-danger\">NOTE: At the time of this writing there's a <a href='https://github.com/nltk/nltk/issues/2053'>known issue</a> with SentimentIntensityAnalyzer that raises a harmless warning on loading<br>\n","<tt><font color=black>&emsp;UserWarning: The twython library has not been installed.<br>&emsp;Some functionality from the twitter package will not be available.</tt>\n","\n","This is due to be fixed in an upcoming NLTK release. For now, if you want to avoid it you can (optionally) install the NLTK twitter library with<br>\n","<tt><font color=black>&emsp;conda install nltk[twitter]</tt><br>or<br>\n","<tt><font color=black>&emsp;pip3 install -U nltk[twitter]</tt></div>"]},{"cell_type":"code","metadata":{"id":"9176cRefu8cx"},"source":["from nltk.sentiment.vader import SentimentIntensityAnalyzer\n","\n","sid = SentimentIntensityAnalyzer()"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"8Yks3n8ou8cy"},"source":["VADER's `SentimentIntensityAnalyzer()` takes in a string and returns a dictionary of scores in each of four categories:\n","* negative\n","* neutral\n","* positive\n","* compound *(computed by normalizing the scores above)*"]},{"cell_type":"code","metadata":{"id":"1vV12T_-u8cz","outputId":"16aca208-624a-40cb-defe-664581d67436"},"source":["a = 'This was a good movie.'\n","sid.polarity_scores(a)"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["{'neg': 0.0, 'neu': 0.508, 'pos': 0.492, 'compound': 0.4404}"]},"metadata":{"tags":[]},"execution_count":3}]},{"cell_type":"code","metadata":{"id":"dlLOiYEVu8cz","outputId":"32a44ad8-cc09-4ab0-8e29-0c99d936773b"},"source":["a = 'This was the best, most awesome movie EVER MADE!!!'\n","sid.polarity_scores(a)"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["{'neg': 0.0, 'neu': 0.425, 'pos': 0.575, 'compound': 0.8877}"]},"metadata":{"tags":[]},"execution_count":4}]},{"cell_type":"code","metadata":{"id":"KxtXOHeCu8c0","outputId":"d92a9bc1-0f16-43b0-8713-ad2f9799550a"},"source":["a = 'This was the worst film to ever disgrace the screen.'\n","sid.polarity_scores(a)"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["{'neg': 0.477, 'neu': 0.523, 'pos': 0.0, 'compound': -0.8074}"]},"metadata":{"tags":[]},"execution_count":5}]},{"cell_type":"markdown","metadata":{"id":"7HX32mGQu8c0"},"source":["## Use VADER to analyze Amazon Reviews\n","For this exercise we're going to apply `SentimentIntensityAnalyzer` to a dataset of 10,000 Amazon reviews. Like our movie reviews datasets, these are labeled as either \"pos\" or \"neg\". At the end we'll determine the accuracy of our sentiment analysis with VADER."]},{"cell_type":"code","metadata":{"id":"LFJDfOgbu8c0","outputId":"f94d0726-06cd-44cb-eedc-49601ec639f9"},"source":["import numpy as np\n","import pandas as pd\n","\n","df = pd.read_csv('../TextFiles/amazonreviews.tsv', sep='\\t')\n","df.head()"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>label</th>\n","      <th>review</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>pos</td>\n","      <td>Stuning even for the non-gamer: This sound tra...</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>pos</td>\n","      <td>The best soundtrack ever to anything.: I'm rea...</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>pos</td>\n","      <td>Amazing!: This soundtrack is my favorite music...</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>pos</td>\n","      <td>Excellent Soundtrack: I truly like this soundt...</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>pos</td>\n","      <td>Remember, Pull Your Jaw Off The Floor After He...</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>"],"text/plain":["  label                                             review\n","0   pos  Stuning even for the non-gamer: This sound tra...\n","1   pos  The best soundtrack ever to anything.: I'm rea...\n","2   pos  Amazing!: This soundtrack is my favorite music...\n","3   pos  Excellent Soundtrack: I truly like this soundt...\n","4   pos  Remember, Pull Your Jaw Off The Floor After He..."]},"metadata":{"tags":[]},"execution_count":6}]},{"cell_type":"code","metadata":{"id":"rXeN8atFu8c1","outputId":"93dd45b3-149d-4bc1-c0c7-10d5bd5f61a0"},"source":["df['label'].value_counts()"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["neg    5097\n","pos    4903\n","Name: label, dtype: int64"]},"metadata":{"tags":[]},"execution_count":7}]},{"cell_type":"markdown","metadata":{"id":"dUEpKNSlu8c1"},"source":["### Clean the data (optional):\n","Recall that our moviereviews.tsv file contained empty records. Let's check to see if any exist in amazonreviews.tsv."]},{"cell_type":"code","metadata":{"id":"3KqmMANCu8c2"},"source":["# REMOVE NaN VALUES AND EMPTY STRINGS:\n","df.dropna(inplace=True)\n","\n","blanks = []  # start with an empty list\n","\n","for i,lb,rv in df.itertuples():  # iterate over the DataFrame\n","    if type(rv)==str:            # avoid NaN values\n","        if rv.isspace():         # test 'review' for whitespace\n","            blanks.append(i)     # add matching index numbers to the list\n","\n","df.drop(blanks, inplace=True)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"hvuwhi-zu8c2","outputId":"a7411310-b758-4b5a-915c-515c6f65f650"},"source":["df['label'].value_counts()"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["neg    5097\n","pos    4903\n","Name: label, dtype: int64"]},"metadata":{"tags":[]},"execution_count":9}]},{"cell_type":"markdown","metadata":{"id":"RtzL6CN_u8c2"},"source":["In this case there were no empty records. Good!"]},{"cell_type":"markdown","metadata":{"id":"ECt8jYhIu8c3"},"source":["## Let's run the first review through VADER"]},{"cell_type":"code","metadata":{"id":"Uq9EXWm0u8c3","outputId":"95545544-bc57-4ecf-8c33-8d9bd5622769"},"source":["sid.polarity_scores(df.loc[0]['review'])"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["{'neg': 0.088, 'neu': 0.669, 'pos': 0.243, 'compound': 0.9454}"]},"metadata":{"tags":[]},"execution_count":10}]},{"cell_type":"code","metadata":{"id":"vWniPUvXu8c4","outputId":"5df7f658-cd07-41d2-f811-e8ec70cb3b8a"},"source":["df.loc[0]['label']"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["'pos'"]},"metadata":{"tags":[]},"execution_count":11}]},{"cell_type":"markdown","metadata":{"id":"L1r-X6CQu8c5"},"source":["Great! Our first review was labeled \"positive\", and earned a positive compound score."]},{"cell_type":"markdown","metadata":{"id":"BxoMS7Mku8c5"},"source":["## Adding Scores and Labels to the DataFrame\n","In this next section we'll add columns to the original DataFrame to store polarity_score dictionaries, extracted compound scores, and new \"pos/neg\" labels derived from the compound score. We'll use this last column to perform an accuracy test."]},{"cell_type":"code","metadata":{"id":"9vyShgCKu8c6","outputId":"02ecff70-9fc7-43c4-8fa1-907b29264752"},"source":["df['scores'] = df['review'].apply(lambda review: sid.polarity_scores(review))\n","\n","df.head()"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>label</th>\n","      <th>review</th>\n","      <th>scores</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>pos</td>\n","      <td>Stuning even for the non-gamer: This sound tra...</td>\n","      <td>{'neg': 0.088, 'neu': 0.669, 'pos': 0.243, 'co...</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>pos</td>\n","      <td>The best soundtrack ever to anything.: I'm rea...</td>\n","      <td>{'neg': 0.018, 'neu': 0.837, 'pos': 0.145, 'co...</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>pos</td>\n","      <td>Amazing!: This soundtrack is my favorite music...</td>\n","      <td>{'neg': 0.04, 'neu': 0.692, 'pos': 0.268, 'com...</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>pos</td>\n","      <td>Excellent Soundtrack: I truly like this soundt...</td>\n","      <td>{'neg': 0.09, 'neu': 0.615, 'pos': 0.295, 'com...</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>pos</td>\n","      <td>Remember, Pull Your Jaw Off The Floor After He...</td>\n","      <td>{'neg': 0.0, 'neu': 0.746, 'pos': 0.254, 'comp...</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>"],"text/plain":["  label                                             review  \\\n","0   pos  Stuning even for the non-gamer: This sound tra...   \n","1   pos  The best soundtrack ever to anything.: I'm rea...   \n","2   pos  Amazing!: This soundtrack is my favorite music...   \n","3   pos  Excellent Soundtrack: I truly like this soundt...   \n","4   pos  Remember, Pull Your Jaw Off The Floor After He...   \n","\n","                                              scores  \n","0  {'neg': 0.088, 'neu': 0.669, 'pos': 0.243, 'co...  \n","1  {'neg': 0.018, 'neu': 0.837, 'pos': 0.145, 'co...  \n","2  {'neg': 0.04, 'neu': 0.692, 'pos': 0.268, 'com...  \n","3  {'neg': 0.09, 'neu': 0.615, 'pos': 0.295, 'com...  \n","4  {'neg': 0.0, 'neu': 0.746, 'pos': 0.254, 'comp...  "]},"metadata":{"tags":[]},"execution_count":12}]},{"cell_type":"code","metadata":{"id":"iX1unJq2u8c6","outputId":"fafb9ab1-aa21-40f4-e527-aab27dad30bb"},"source":["df['compound']  = df['scores'].apply(lambda score_dict: score_dict['compound'])\n","\n","df.head()"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>label</th>\n","      <th>review</th>\n","      <th>scores</th>\n","      <th>compound</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>pos</td>\n","      <td>Stuning even for the non-gamer: This sound tra...</td>\n","      <td>{'neg': 0.088, 'neu': 0.669, 'pos': 0.243, 'co...</td>\n","      <td>0.9454</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>pos</td>\n","      <td>The best soundtrack ever to anything.: I'm rea...</td>\n","      <td>{'neg': 0.018, 'neu': 0.837, 'pos': 0.145, 'co...</td>\n","      <td>0.8957</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>pos</td>\n","      <td>Amazing!: This soundtrack is my favorite music...</td>\n","      <td>{'neg': 0.04, 'neu': 0.692, 'pos': 0.268, 'com...</td>\n","      <td>0.9858</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>pos</td>\n","      <td>Excellent Soundtrack: I truly like this soundt...</td>\n","      <td>{'neg': 0.09, 'neu': 0.615, 'pos': 0.295, 'com...</td>\n","      <td>0.9814</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>pos</td>\n","      <td>Remember, Pull Your Jaw Off The Floor After He...</td>\n","      <td>{'neg': 0.0, 'neu': 0.746, 'pos': 0.254, 'comp...</td>\n","      <td>0.9781</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>"],"text/plain":["  label                                             review  \\\n","0   pos  Stuning even for the non-gamer: This sound tra...   \n","1   pos  The best soundtrack ever to anything.: I'm rea...   \n","2   pos  Amazing!: This soundtrack is my favorite music...   \n","3   pos  Excellent Soundtrack: I truly like this soundt...   \n","4   pos  Remember, Pull Your Jaw Off The Floor After He...   \n","\n","                                              scores  compound  \n","0  {'neg': 0.088, 'neu': 0.669, 'pos': 0.243, 'co...    0.9454  \n","1  {'neg': 0.018, 'neu': 0.837, 'pos': 0.145, 'co...    0.8957  \n","2  {'neg': 0.04, 'neu': 0.692, 'pos': 0.268, 'com...    0.9858  \n","3  {'neg': 0.09, 'neu': 0.615, 'pos': 0.295, 'com...    0.9814  \n","4  {'neg': 0.0, 'neu': 0.746, 'pos': 0.254, 'comp...    0.9781  "]},"metadata":{"tags":[]},"execution_count":13}]},{"cell_type":"code","metadata":{"id":"dsezS-yZu8c6","outputId":"0c4a093e-fdaa-4f6f-ce22-5b88eb6d02fe"},"source":["df['comp_score'] = df['compound'].apply(lambda c: 'pos' if c >=0 else 'neg')\n","\n","df.head()"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>label</th>\n","      <th>review</th>\n","      <th>scores</th>\n","      <th>compound</th>\n","      <th>comp_score</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>pos</td>\n","      <td>Stuning even for the non-gamer: This sound tra...</td>\n","      <td>{'neg': 0.088, 'neu': 0.669, 'pos': 0.243, 'co...</td>\n","      <td>0.9454</td>\n","      <td>pos</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>pos</td>\n","      <td>The best soundtrack ever to anything.: I'm rea...</td>\n","      <td>{'neg': 0.018, 'neu': 0.837, 'pos': 0.145, 'co...</td>\n","      <td>0.8957</td>\n","      <td>pos</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>pos</td>\n","      <td>Amazing!: This soundtrack is my favorite music...</td>\n","      <td>{'neg': 0.04, 'neu': 0.692, 'pos': 0.268, 'com...</td>\n","      <td>0.9858</td>\n","      <td>pos</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>pos</td>\n","      <td>Excellent Soundtrack: I truly like this soundt...</td>\n","      <td>{'neg': 0.09, 'neu': 0.615, 'pos': 0.295, 'com...</td>\n","      <td>0.9814</td>\n","      <td>pos</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>pos</td>\n","      <td>Remember, Pull Your Jaw Off The Floor After He...</td>\n","      <td>{'neg': 0.0, 'neu': 0.746, 'pos': 0.254, 'comp...</td>\n","      <td>0.9781</td>\n","      <td>pos</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>"],"text/plain":["  label                                             review  \\\n","0   pos  Stuning even for the non-gamer: This sound tra...   \n","1   pos  The best soundtrack ever to anything.: I'm rea...   \n","2   pos  Amazing!: This soundtrack is my favorite music...   \n","3   pos  Excellent Soundtrack: I truly like this soundt...   \n","4   pos  Remember, Pull Your Jaw Off The Floor After He...   \n","\n","                                              scores  compound comp_score  \n","0  {'neg': 0.088, 'neu': 0.669, 'pos': 0.243, 'co...    0.9454        pos  \n","1  {'neg': 0.018, 'neu': 0.837, 'pos': 0.145, 'co...    0.8957        pos  \n","2  {'neg': 0.04, 'neu': 0.692, 'pos': 0.268, 'com...    0.9858        pos  \n","3  {'neg': 0.09, 'neu': 0.615, 'pos': 0.295, 'com...    0.9814        pos  \n","4  {'neg': 0.0, 'neu': 0.746, 'pos': 0.254, 'comp...    0.9781        pos  "]},"metadata":{"tags":[]},"execution_count":14}]},{"cell_type":"markdown","metadata":{"id":"dg90MjzXu8c7"},"source":["## Report on Accuracy\n","Finally, we'll use scikit-learn to determine how close VADER came to our original 10,000 labels."]},{"cell_type":"code","metadata":{"id":"qFHSp2MXu8c7"},"source":["from sklearn.metrics import accuracy_score,classification_report,confusion_matrix"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"DNrbiJ3tu8c7","outputId":"5c454dfd-3296-493f-84f4-eea1ec74efef"},"source":["accuracy_score(df['label'],df['comp_score'])"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["0.7091"]},"metadata":{"tags":[]},"execution_count":16}]},{"cell_type":"code","metadata":{"id":"d_ZNrCkfu8c8","outputId":"42bb54c5-7e8a-4e35-ab62-2e37edccf1a4"},"source":["print(classification_report(df['label'],df['comp_score']))"],"execution_count":null,"outputs":[{"output_type":"stream","text":["              precision    recall  f1-score   support\n","\n","         neg       0.86      0.51      0.64      5097\n","         pos       0.64      0.91      0.75      4903\n","\n","   micro avg       0.71      0.71      0.71     10000\n","   macro avg       0.75      0.71      0.70     10000\n","weighted avg       0.75      0.71      0.70     10000\n","\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"Wt46tMvKu8c8","outputId":"fa2c4dff-3cdf-46f8-bb03-f60037cadebe"},"source":["print(confusion_matrix(df['label'],df['comp_score']))"],"execution_count":null,"outputs":[{"output_type":"stream","text":["[[2623 2474]\n"," [ 435 4468]]\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"yDmNv7cuu8c9"},"source":["This tells us that VADER correctly identified an Amazon review as \"positive\" or \"negative\" roughly 71% of the time.\n"]},{"cell_type":"markdown","metadata":{"id":"Scp9WEZ2hJN8"},"source":["# Latent Dirichlet Allocation"]},{"cell_type":"markdown","metadata":{"id":"5R_E99Q4hJN9"},"source":["## Data\n","\n","We will be using articles from NPR (National Public Radio), obtained from their website [www.npr.org](http://www.npr.org)"]},{"cell_type":"code","metadata":{"collapsed":true,"id":"iioN0E1BhJN-"},"source":["import pandas as pd"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"collapsed":true,"id":"G-mxc3QChJN-"},"source":["npr = pd.read_csv('npr.csv')"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"82xJj--HhJN_","outputId":"f55d42c0-fa46-4582-c383-4e56649abe13"},"source":["npr.head()"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>Article</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>In the Washington of 2016, even when the polic...</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>Donald Trump has used Twitter  —   his prefe...</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>Donald Trump is unabashedly praising Russian...</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>Updated at 2:50 p. m. ET, Russian President Vl...</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>From photography, illustration and video, to d...</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>"],"text/plain":["                                             Article\n","0  In the Washington of 2016, even when the polic...\n","1    Donald Trump has used Twitter  —   his prefe...\n","2    Donald Trump is unabashedly praising Russian...\n","3  Updated at 2:50 p. m. ET, Russian President Vl...\n","4  From photography, illustration and video, to d..."]},"metadata":{"tags":[]},"execution_count":3}]},{"cell_type":"markdown","metadata":{"id":"GQY1UOnlhJOD"},"source":["Notice how we don't have the topic of the articles! Let's use LDA to attempt to figure out clusters of the articles."]},{"cell_type":"markdown","metadata":{"id":"pErRTSgVhJOD"},"source":["## Preprocessing"]},{"cell_type":"code","metadata":{"collapsed":true,"id":"HikHMRe6hJOE"},"source":["from sklearn.feature_extraction.text import CountVectorizer"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"K4qzwg8IhJOF"},"source":["**`max_df`**` : float in range [0.0, 1.0] or int, default=1.0`<br>\n","When building the vocabulary ignore terms that have a document frequency strictly higher than the given threshold (corpus-specific stop words). If float, the parameter represents a proportion of documents, integer absolute counts. This parameter is ignored if vocabulary is not None.\n","\n","**`min_df`**` : float in range [0.0, 1.0] or int, default=1`<br>\n","When building the vocabulary ignore terms that have a document frequency strictly lower than the given threshold. This value is also called cut-off in the literature. If float, the parameter represents a proportion of documents, integer absolute counts. This parameter is ignored if vocabulary is not None."]},{"cell_type":"code","metadata":{"collapsed":true,"id":"i3eaqE9ohJOG"},"source":["cv = CountVectorizer(max_df=0.95, min_df=2, stop_words='english')"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"collapsed":true,"id":"MXMX19FahJOI"},"source":["dtm = cv.fit_transform(npr['Article'])"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"I9Gdi7wUhJOJ","outputId":"750d432e-2258-4550-aa6b-4aead95df761"},"source":["dtm"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["<11992x54777 sparse matrix of type '<class 'numpy.int64'>'\n","\twith 3033388 stored elements in Compressed Sparse Row format>"]},"metadata":{"tags":[]},"execution_count":7}]},{"cell_type":"markdown","metadata":{"id":"-fF5nkWNhJOJ"},"source":["## LDA"]},{"cell_type":"code","metadata":{"collapsed":true,"id":"6Oll-PtyhJOJ"},"source":["from sklearn.decomposition import LatentDirichletAllocation"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"collapsed":true,"id":"D0l3iPXWhJOK"},"source":["LDA = LatentDirichletAllocation(n_components=7,random_state=42)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"fglMrcFXhJOK","outputId":"11fed3ff-f5aa-4cef-ec74-12f27e7a93cf"},"source":["# This can take awhile, we're dealing with a large amount of documents!\n","LDA.fit(dtm)"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["LatentDirichletAllocation(batch_size=128, doc_topic_prior=None,\n","             evaluate_every=-1, learning_decay=0.7,\n","             learning_method='batch', learning_offset=10.0,\n","             max_doc_update_iter=100, max_iter=10, mean_change_tol=0.001,\n","             n_components=7, n_jobs=None, n_topics=None, perp_tol=0.1,\n","             random_state=42, topic_word_prior=None,\n","             total_samples=1000000.0, verbose=0)"]},"metadata":{"tags":[]},"execution_count":40}]},{"cell_type":"markdown","metadata":{"id":"8nJ05S8lhJOL"},"source":["## Showing Stored Words"]},{"cell_type":"code","metadata":{"id":"8awKGrSYhJOL","outputId":"db93ec52-ba92-4468-8831-eb867ef9f2f9"},"source":["len(cv.get_feature_names())"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["54777"]},"metadata":{"tags":[]},"execution_count":41}]},{"cell_type":"code","metadata":{"collapsed":true,"id":"UXl2Q137hJOL"},"source":["import random"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"wXIAoYkvhJOM","outputId":"b0f44b1b-439a-4213-9c62-12f9d9c357f6"},"source":["for i in range(10):\n","    random_word_id = random.randint(0,54776)\n","    print(cv.get_feature_names()[random_word_id])"],"execution_count":null,"outputs":[{"output_type":"stream","text":["cred\n","fairly\n","occupational\n","temer\n","tamil\n","closest\n","condone\n","breathes\n","tendrils\n","pivot\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"1bOiJTE0hJOM","outputId":"686daa19-b664-4af4-9525-5e129d2c3586"},"source":["for i in range(10):\n","    random_word_id = random.randint(0,54776)\n","    print(cv.get_feature_names()[random_word_id])"],"execution_count":null,"outputs":[{"output_type":"stream","text":["foremothers\n","mocoa\n","ellroy\n","liron\n","ally\n","discouraged\n","utterance\n","provo\n","videgaray\n","archivist\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"collapsed":true,"id":"7azS2uFxhJON"},"source":["### Showing Top Words Per Topic"]},{"cell_type":"code","metadata":{"id":"5ozOFcSOhJON","outputId":"8d34c932-d46e-43c1-f94e-043656e5d9fd"},"source":["len(LDA.components_)"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["7"]},"metadata":{"tags":[]},"execution_count":46}]},{"cell_type":"code","metadata":{"id":"vbG2IGzShJON","outputId":"71cbc5a1-adc3-4dfb-c5af-ddc371d980f7"},"source":["LDA.components_"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["array([[8.64332806e+00, 2.38014333e+03, 1.42900522e-01, ...,\n","        1.43006821e-01, 1.42902042e-01, 1.42861626e-01],\n","       [2.76191749e+01, 5.36394437e+02, 1.42857148e-01, ...,\n","        1.42861973e-01, 1.42857147e-01, 1.42906875e-01],\n","       [7.22783888e+00, 8.24033986e+02, 1.42857148e-01, ...,\n","        6.14236247e+00, 2.14061364e+00, 1.42923753e-01],\n","       ...,\n","       [3.11488651e+00, 3.50409655e+02, 1.42857147e-01, ...,\n","        1.42859912e-01, 1.42857146e-01, 1.42866614e-01],\n","       [4.61486388e+01, 5.14408600e+01, 3.14281373e+00, ...,\n","        1.43107628e-01, 1.43902481e-01, 2.14271779e+00],\n","       [4.93991422e-01, 4.18841042e+02, 1.42857151e-01, ...,\n","        1.42857146e-01, 1.43760101e-01, 1.42866201e-01]])"]},"metadata":{"tags":[]},"execution_count":47}]},{"cell_type":"code","metadata":{"id":"n5eidXidhJOO","outputId":"0ffd2e6c-c999-4ef5-adaa-09fd21073196"},"source":["len(LDA.components_[0])"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["54777"]},"metadata":{"tags":[]},"execution_count":48}]},{"cell_type":"code","metadata":{"collapsed":true,"id":"Ahr4FrgAhJOO"},"source":["single_topic = LDA.components_[0]"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"5_sKJqzvhJOO","outputId":"4967c20d-421e-4182-e55b-4e1f7c3568e2"},"source":["# Returns the indices that would sort this array.\n","single_topic.argsort()"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["array([ 2475, 18302, 35285, ..., 22673, 42561, 42993], dtype=int64)"]},"metadata":{"tags":[]},"execution_count":50}]},{"cell_type":"code","metadata":{"id":"Gz3VbKGahJOP","outputId":"82e5f34f-6762-4614-b586-ecfb0e6ccaca"},"source":["# Word least representative of this topic\n","single_topic[18302]"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["0.14285714309286987"]},"metadata":{"tags":[]},"execution_count":51}]},{"cell_type":"code","metadata":{"id":"b8pAk418hJOP","outputId":"65d68456-554e-4d64-c208-89b6d332184f"},"source":["# Word most representative of this topic\n","single_topic[42993]"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["6247.245510521082"]},"metadata":{"tags":[]},"execution_count":52}]},{"cell_type":"code","metadata":{"id":"lWHyRsYrhJOP","outputId":"c04e0f72-a069-41da-c3c9-ebe7411d68c0"},"source":["# Top 10 words for this topic:\n","single_topic.argsort()[-10:]"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["array([33390, 36310, 21228, 10425, 31464,  8149, 36283, 22673, 42561,\n","       42993], dtype=int64)"]},"metadata":{"tags":[]},"execution_count":53}]},{"cell_type":"code","metadata":{"collapsed":true,"id":"3xWWKjAWhJOP"},"source":["top_word_indices = single_topic.argsort()[-10:]"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"sHB7Gu_FhJOQ","outputId":"3e8342c7-a04b-4796-e704-a9428e77d762"},"source":["for index in top_word_indices:\n","    print(cv.get_feature_names()[index])"],"execution_count":null,"outputs":[{"output_type":"stream","text":["new\n","percent\n","government\n","company\n","million\n","care\n","people\n","health\n","said\n","says\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"Zruzn-RzhJOQ"},"source":["These look like business articles perhaps... Let's confirm by using .transform() on our vectorized articles to attach a label number. But first, let's view all the 10 topics found."]},{"cell_type":"code","metadata":{"id":"dE70RqXqhJOQ","outputId":"3fbb1678-7570-4fb5-e0b3-160164356832"},"source":["for index,topic in enumerate(LDA.components_):\n","    print(f'THE TOP 15 WORDS FOR TOPIC #{index}')\n","    print([cv.get_feature_names()[i] for i in topic.argsort()[-15:]])\n","    print('\\n')"],"execution_count":null,"outputs":[{"output_type":"stream","text":["THE TOP 15 WORDS FOR TOPIC #0\n","['companies', 'money', 'year', 'federal', '000', 'new', 'percent', 'government', 'company', 'million', 'care', 'people', 'health', 'said', 'says']\n","\n","\n","THE TOP 15 WORDS FOR TOPIC #1\n","['military', 'house', 'security', 'russia', 'government', 'npr', 'reports', 'says', 'news', 'people', 'told', 'police', 'president', 'trump', 'said']\n","\n","\n","THE TOP 15 WORDS FOR TOPIC #2\n","['way', 'world', 'family', 'home', 'day', 'time', 'water', 'city', 'new', 'years', 'food', 'just', 'people', 'like', 'says']\n","\n","\n","THE TOP 15 WORDS FOR TOPIC #3\n","['time', 'new', 'don', 'years', 'medical', 'disease', 'patients', 'just', 'children', 'study', 'like', 'women', 'health', 'people', 'says']\n","\n","\n","THE TOP 15 WORDS FOR TOPIC #4\n","['voters', 'vote', 'election', 'party', 'new', 'obama', 'court', 'republican', 'campaign', 'people', 'state', 'president', 'clinton', 'said', 'trump']\n","\n","\n","THE TOP 15 WORDS FOR TOPIC #5\n","['years', 'going', 've', 'life', 'don', 'new', 'way', 'music', 'really', 'time', 'know', 'think', 'people', 'just', 'like']\n","\n","\n","THE TOP 15 WORDS FOR TOPIC #6\n","['student', 'years', 'data', 'science', 'university', 'people', 'time', 'schools', 'just', 'education', 'new', 'like', 'students', 'school', 'says']\n","\n","\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"9AT_CbnPhJOR"},"source":["### Attaching Discovered Topic Labels to Original Articles"]},{"cell_type":"code","metadata":{"id":"EgCherCEhJOR","outputId":"ffc75e48-c07f-4101-8866-af52f73bb2ac"},"source":["dtm"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["<11992x54777 sparse matrix of type '<class 'numpy.int64'>'\n","\twith 3033388 stored elements in Compressed Sparse Row format>"]},"metadata":{"tags":[]},"execution_count":57}]},{"cell_type":"code","metadata":{"id":"9UFc5URqhJOR","outputId":"aedb6ac9-1103-4363-c140-390fdd3f6451"},"source":["dtm.shape"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["(11992, 54777)"]},"metadata":{"tags":[]},"execution_count":58}]},{"cell_type":"code","metadata":{"id":"6YIYUuqshJOS","outputId":"47f69cb4-a506-4bcb-b40c-600f91b6f74b"},"source":["len(npr)"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["11992"]},"metadata":{"tags":[]},"execution_count":59}]},{"cell_type":"code","metadata":{"collapsed":true,"id":"CmB_Ch5VhJOS"},"source":["topic_results = LDA.transform(dtm)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"xZlYNaTWhJOS","outputId":"cd11db3d-2b9e-44c5-d5dc-f728a7a6d8f5"},"source":["topic_results.shape"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["(11992, 7)"]},"metadata":{"tags":[]},"execution_count":61}]},{"cell_type":"code","metadata":{"id":"FFr3qPCshJOS","outputId":"8b60d501-a907-492d-b6c4-c0243f29435d"},"source":["topic_results[0]"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["array([1.61040465e-02, 6.83341493e-01, 2.25376318e-04, 2.25369288e-04,\n","       2.99652737e-01, 2.25479379e-04, 2.25497980e-04])"]},"metadata":{"tags":[]},"execution_count":62}]},{"cell_type":"code","metadata":{"id":"sJt826FDhJOT","outputId":"2ef9187b-8c64-4fb5-805e-b27e8b03d81c"},"source":["topic_results[0].round(2)"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["array([0.02, 0.68, 0.  , 0.  , 0.3 , 0.  , 0.  ])"]},"metadata":{"tags":[]},"execution_count":63}]},{"cell_type":"code","metadata":{"id":"_rytzCyChJOT","outputId":"6d063a7d-94d7-495e-d408-580502cc81e7"},"source":["topic_results[0].argmax()"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["1"]},"metadata":{"tags":[]},"execution_count":64}]},{"cell_type":"markdown","metadata":{"id":"KX795e8AhJOT"},"source":["This means that our model thinks that the first article belongs to topic #1."]},{"cell_type":"markdown","metadata":{"id":"HmK80vuvhJOU"},"source":["### Combining with Original Data"]},{"cell_type":"code","metadata":{"id":"JOI2zA3ShJOU","outputId":"5d0cc1b7-ae3d-4557-bdb4-8e136e8e92ab"},"source":["npr.head()"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>Article</th>\n","      <th>Topic</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>In the Washington of 2016, even when the polic...</td>\n","      <td>1</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>Donald Trump has used Twitter  —   his prefe...</td>\n","      <td>1</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>Donald Trump is unabashedly praising Russian...</td>\n","      <td>1</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>Updated at 2:50 p. m. ET, Russian President Vl...</td>\n","      <td>1</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>From photography, illustration and video, to d...</td>\n","      <td>6</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>"],"text/plain":["                                             Article  Topic\n","0  In the Washington of 2016, even when the polic...      1\n","1    Donald Trump has used Twitter  —   his prefe...      1\n","2    Donald Trump is unabashedly praising Russian...      1\n","3  Updated at 2:50 p. m. ET, Russian President Vl...      1\n","4  From photography, illustration and video, to d...      6"]},"metadata":{"tags":[]},"execution_count":65}]},{"cell_type":"code","metadata":{"id":"6wmNtEDrhJOU","outputId":"ac9e4398-b89c-49bb-f484-383e2fe4d0b5"},"source":["topic_results.argmax(axis=1)"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["array([1, 1, 1, ..., 3, 4, 0], dtype=int64)"]},"metadata":{"tags":[]},"execution_count":66}]},{"cell_type":"code","metadata":{"collapsed":true,"id":"Lfl8FTO3hJOU"},"source":["npr['Topic'] = topic_results.argmax(axis=1)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"gD8Bjh3ihJOV","outputId":"7a4f3b0f-f6fb-474a-dc14-ab3de9af9723"},"source":["npr.head(10)"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>Article</th>\n","      <th>Topic</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>In the Washington of 2016, even when the polic...</td>\n","      <td>1</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>Donald Trump has used Twitter  —   his prefe...</td>\n","      <td>1</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>Donald Trump is unabashedly praising Russian...</td>\n","      <td>1</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>Updated at 2:50 p. m. ET, Russian President Vl...</td>\n","      <td>1</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>From photography, illustration and video, to d...</td>\n","      <td>2</td>\n","    </tr>\n","    <tr>\n","      <th>5</th>\n","      <td>I did not want to join yoga class. I hated tho...</td>\n","      <td>3</td>\n","    </tr>\n","    <tr>\n","      <th>6</th>\n","      <td>With a   who has publicly supported the debunk...</td>\n","      <td>3</td>\n","    </tr>\n","    <tr>\n","      <th>7</th>\n","      <td>I was standing by the airport exit, debating w...</td>\n","      <td>2</td>\n","    </tr>\n","    <tr>\n","      <th>8</th>\n","      <td>If movies were trying to be more realistic, pe...</td>\n","      <td>3</td>\n","    </tr>\n","    <tr>\n","      <th>9</th>\n","      <td>Eighteen years ago, on New Year’s Eve, David F...</td>\n","      <td>2</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>"],"text/plain":["                                             Article  Topic\n","0  In the Washington of 2016, even when the polic...      1\n","1    Donald Trump has used Twitter  —   his prefe...      1\n","2    Donald Trump is unabashedly praising Russian...      1\n","3  Updated at 2:50 p. m. ET, Russian President Vl...      1\n","4  From photography, illustration and video, to d...      2\n","5  I did not want to join yoga class. I hated tho...      3\n","6  With a   who has publicly supported the debunk...      3\n","7  I was standing by the airport exit, debating w...      2\n","8  If movies were trying to be more realistic, pe...      3\n","9  Eighteen years ago, on New Year’s Eve, David F...      2"]},"metadata":{"tags":[]},"execution_count":68}]},{"cell_type":"markdown","metadata":{"id":"hI4IN6prhJOV"},"source":["## Great work!"]}]}